{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_rnn_sampling.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "dbH0Vobe4PRs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow\n",
        "import keras\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pdaVfrTU433h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "nuql15A944-e",
        "colab_type": "code",
        "outputId": "e892b32c-135e-434b-e5f4-bd63197de01c",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dc625898-5847-4fdd-a359-e08401379061\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-dc625898-5847-4fdd-a359-e08401379061\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gmaD2W795Fwm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "45c2e65f-d006-46d3-a1af-58821e8509e6"
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['train_E6oV3lV.csv']))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-861210732f95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_E6oV3lV.csv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'train_E6oV3lV.csv'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7c7Moym94PR3",
        "colab_type": "code",
        "outputId": "c252efe5-fda1-4c64-cbc4-6194ebab731b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train_E6oV3lV.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-07c771820603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_E6oV3lV.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: File b'train_E6oV3lV.csv' does not exist"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "H2dI7p3v41m8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IcLP8yXG4PSA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cb6n7kmf4PSJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.groupby('label').count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UClgYXuI4PST",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "count_class_0, count_class_1 = df['label'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FqH0RnT54PSY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_class_0 =  df.query('label==0')\n",
        "df_class_1 =  df.query('label==1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7CwyCN6l4PSe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_class_0_under = df_class_0.sample(count_class_1)\n",
        "df_under = pd.concat([df_class_0_under, df_class_1],ignore_index=True ,axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QNc_CDPG4PSo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_under.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5BQlslFk4PSz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_under.groupby('label').count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZhPU_plX4PS9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = df_under['tweet']\n",
        "Y = df_under['label']\n",
        "Y_org = Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "79a5bKnV4PTC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X[34]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eds5ksHJ4PTL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "max_features = 10000\n",
        "tokenizer = Tokenizer(num_words=max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', split=' ', lower=True, char_level=False, oov_token=None)\n",
        "tokenizer.fit_on_texts(X.values)\n",
        "X = tokenizer.texts_to_sequences(X.values)\n",
        "\n",
        "# add padding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X = pad_sequences(X, maxlen=400)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FW_k09Wg4PTS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "giwlGZ1U4PTV",
        "colab_type": "code",
        "outputId": "7bc33603-61e1-46a6-ff89-45510da26b3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "max(df['tweet'], key=len)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' @user lmfao pathetic #soit   #growup #funny #noonethere #iknowwhoitis ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98±ð\\x9f\\x98±ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98±ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82â\\x80¦'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "cgOz7bZo4PTg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit(X).transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Gj4xPlJ4PTj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components = 95)\n",
        "pca.fit(X)\n",
        "X = pca.transform(X);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QETVpTg04PTm",
        "colab_type": "code",
        "outputId": "13daf7f5-6b6a-4ccd-efbe-ffa7d1b2c0d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
        "plt.xlabel(\"No. of components\")\n",
        "plt.ylabel(\"cummulative explained Variance\");"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEGCAYAAACHGfl5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "f_OZL-_s4PTt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2, random_state= 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UlTIX2GQ4PTw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrIIlpF14PT0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gA4171ow4PT3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier = Sequential()\n",
        "classifier.add(LSTM(units=40, activation='relu',return_sequences= True, input_shape=(None, 95)))\n",
        "classifier.add(Dropout(rate=0.3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CZMP0EYY4PT6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier.add(LSTM(units=20, return_sequences= True,activation='relu'))\n",
        "classifier.add(Dropout(rate=0.2))\n",
        "classifier.add(LSTM(units=20,activation='relu'))\n",
        "classifier.add(Dropout(rate=0.2))\n",
        "classifier.add(Dense(units = 2, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DxjIi1fm4PT8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier.compile(optimizer='rmsprop',metrics=['accuracy'],loss='binary_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lgcBrRwk4PT_",
        "colab_type": "code",
        "outputId": "9066b69b-f175-46d5-fab6-59842141b285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "y_e = OneHotEncoder()\n",
        "Y_train_org = Y_train\n",
        "Y_test_org = Y_test\n",
        "Y_train = Y_train.reshape(-1, 1)\n",
        "Y_test = Y_test.reshape(-1, 1)\n",
        "y_e.fit(Y_train)\n",
        "Y_train = y_e.transform(Y_train)\n",
        "Y_test = y_e.transform(Y_test)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "sGo4HJi34PUD",
        "colab_type": "code",
        "outputId": "b93181da-e631-45a3-b950-d85d4d06c989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6834
        }
      },
      "cell_type": "code",
      "source": [
        "X_train_lstm = np.reshape(X_train, (X_train.shape[0],1, X_train.shape[1]))\n",
        "X_test_lstm = np.reshape(X_test, (X_test.shape[0],1 ,X_test.shape[1]))\n",
        "import tensorflow as tf\n",
        "with tf.device('/gpu:0'):\n",
        "    checker = classifier.fit(X_train_lstm, Y_train, batch_size=32, epochs=200, validation_data = (X_test_lstm, Y_test))\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3587 samples, validate on 897 samples\n",
            "Epoch 1/200\n",
            "3587/3587 [==============================] - 4s 1ms/step - loss: 0.6923 - acc: 0.5562 - val_loss: 0.6902 - val_acc: 0.6533\n",
            "Epoch 2/200\n",
            "3587/3587 [==============================] - 1s 222us/step - loss: 0.6826 - acc: 0.6500 - val_loss: 0.6815 - val_acc: 0.6722\n",
            "Epoch 3/200\n",
            "3587/3587 [==============================] - 1s 224us/step - loss: 0.6751 - acc: 0.6682 - val_loss: 0.6752 - val_acc: 0.6800\n",
            "Epoch 4/200\n",
            "3587/3587 [==============================] - 1s 234us/step - loss: 0.6555 - acc: 0.6769 - val_loss: 0.6636 - val_acc: 0.6795\n",
            "Epoch 5/200\n",
            "3587/3587 [==============================] - 1s 232us/step - loss: 0.6383 - acc: 0.6833 - val_loss: 0.6480 - val_acc: 0.6884\n",
            "Epoch 6/200\n",
            "3587/3587 [==============================] - 1s 228us/step - loss: 0.6275 - acc: 0.6853 - val_loss: 0.6264 - val_acc: 0.7023\n",
            "Epoch 7/200\n",
            "3587/3587 [==============================] - 1s 234us/step - loss: 0.6126 - acc: 0.6931 - val_loss: 0.6150 - val_acc: 0.7023\n",
            "Epoch 8/200\n",
            "3587/3587 [==============================] - 1s 225us/step - loss: 0.6028 - acc: 0.7046 - val_loss: 0.6098 - val_acc: 0.7101\n",
            "Epoch 9/200\n",
            "3587/3587 [==============================] - 1s 232us/step - loss: 0.5992 - acc: 0.7018 - val_loss: 0.5989 - val_acc: 0.7124\n",
            "Epoch 10/200\n",
            "3587/3587 [==============================] - 1s 234us/step - loss: 0.5955 - acc: 0.7035 - val_loss: 0.6024 - val_acc: 0.7168\n",
            "Epoch 11/200\n",
            "3587/3587 [==============================] - 1s 221us/step - loss: 0.5921 - acc: 0.7112 - val_loss: 0.6099 - val_acc: 0.7096\n",
            "Epoch 12/200\n",
            "3587/3587 [==============================] - 1s 224us/step - loss: 0.5890 - acc: 0.7113 - val_loss: 0.5986 - val_acc: 0.7174\n",
            "Epoch 13/200\n",
            "3587/3587 [==============================] - 1s 225us/step - loss: 0.5897 - acc: 0.7084 - val_loss: 0.5994 - val_acc: 0.7146\n",
            "Epoch 14/200\n",
            "3587/3587 [==============================] - 1s 229us/step - loss: 0.5840 - acc: 0.7205 - val_loss: 0.6050 - val_acc: 0.7113\n",
            "Epoch 15/200\n",
            "3587/3587 [==============================] - 1s 224us/step - loss: 0.5832 - acc: 0.7113 - val_loss: 0.6141 - val_acc: 0.7090\n",
            "Epoch 16/200\n",
            "3587/3587 [==============================] - 1s 223us/step - loss: 0.5810 - acc: 0.7226 - val_loss: 0.6023 - val_acc: 0.7079\n",
            "Epoch 17/200\n",
            "3587/3587 [==============================] - 1s 229us/step - loss: 0.5779 - acc: 0.7244 - val_loss: 0.5973 - val_acc: 0.7168\n",
            "Epoch 18/200\n",
            "3587/3587 [==============================] - 1s 219us/step - loss: 0.5794 - acc: 0.7233 - val_loss: 0.5941 - val_acc: 0.7168\n",
            "Epoch 19/200\n",
            "3587/3587 [==============================] - 1s 219us/step - loss: 0.5754 - acc: 0.7223 - val_loss: 0.5979 - val_acc: 0.7152\n",
            "Epoch 20/200\n",
            "3587/3587 [==============================] - 1s 211us/step - loss: 0.5714 - acc: 0.7293 - val_loss: 0.6020 - val_acc: 0.7124\n",
            "Epoch 21/200\n",
            "3587/3587 [==============================] - 1s 213us/step - loss: 0.5675 - acc: 0.7310 - val_loss: 0.5910 - val_acc: 0.7096\n",
            "Epoch 22/200\n",
            "3587/3587 [==============================] - 1s 222us/step - loss: 0.5696 - acc: 0.7279 - val_loss: 0.5965 - val_acc: 0.7107\n",
            "Epoch 23/200\n",
            "3587/3587 [==============================] - 1s 221us/step - loss: 0.5619 - acc: 0.7321 - val_loss: 0.6044 - val_acc: 0.7152\n",
            "Epoch 24/200\n",
            "3587/3587 [==============================] - 1s 225us/step - loss: 0.5564 - acc: 0.7340 - val_loss: 0.5957 - val_acc: 0.7163\n",
            "Epoch 25/200\n",
            "3587/3587 [==============================] - 1s 218us/step - loss: 0.5536 - acc: 0.7392 - val_loss: 0.5944 - val_acc: 0.7146\n",
            "Epoch 26/200\n",
            "3587/3587 [==============================] - 1s 226us/step - loss: 0.5548 - acc: 0.7359 - val_loss: 0.5938 - val_acc: 0.7124\n",
            "Epoch 27/200\n",
            "3587/3587 [==============================] - 1s 222us/step - loss: 0.5535 - acc: 0.7414 - val_loss: 0.5984 - val_acc: 0.7118\n",
            "Epoch 28/200\n",
            "3587/3587 [==============================] - 1s 218us/step - loss: 0.5542 - acc: 0.7409 - val_loss: 0.6016 - val_acc: 0.7135\n",
            "Epoch 29/200\n",
            "3587/3587 [==============================] - 1s 226us/step - loss: 0.5565 - acc: 0.7406 - val_loss: 0.5999 - val_acc: 0.7224\n",
            "Epoch 30/200\n",
            "3587/3587 [==============================] - 1s 218us/step - loss: 0.5484 - acc: 0.7444 - val_loss: 0.6088 - val_acc: 0.7135\n",
            "Epoch 31/200\n",
            "3587/3587 [==============================] - 1s 211us/step - loss: 0.5484 - acc: 0.7469 - val_loss: 0.6240 - val_acc: 0.7124\n",
            "Epoch 32/200\n",
            "3587/3587 [==============================] - 1s 207us/step - loss: 0.5443 - acc: 0.7519 - val_loss: 0.6059 - val_acc: 0.7179\n",
            "Epoch 33/200\n",
            "3587/3587 [==============================] - 1s 219us/step - loss: 0.5447 - acc: 0.7416 - val_loss: 0.6067 - val_acc: 0.7113\n",
            "Epoch 34/200\n",
            "3587/3587 [==============================] - 1s 225us/step - loss: 0.5389 - acc: 0.7519 - val_loss: 0.6248 - val_acc: 0.7157\n",
            "Epoch 35/200\n",
            "3587/3587 [==============================] - 1s 222us/step - loss: 0.5478 - acc: 0.7470 - val_loss: 0.6310 - val_acc: 0.7101\n",
            "Epoch 36/200\n",
            "3587/3587 [==============================] - 1s 231us/step - loss: 0.5375 - acc: 0.7448 - val_loss: 0.6169 - val_acc: 0.7174\n",
            "Epoch 37/200\n",
            "3587/3587 [==============================] - 1s 230us/step - loss: 0.5430 - acc: 0.7449 - val_loss: 0.6015 - val_acc: 0.7202\n",
            "Epoch 38/200\n",
            "3587/3587 [==============================] - 1s 224us/step - loss: 0.5365 - acc: 0.7523 - val_loss: 0.6079 - val_acc: 0.7196\n",
            "Epoch 39/200\n",
            "3587/3587 [==============================] - 1s 232us/step - loss: 0.5405 - acc: 0.7552 - val_loss: 0.6108 - val_acc: 0.7152\n",
            "Epoch 40/200\n",
            "3587/3587 [==============================] - 1s 224us/step - loss: 0.5371 - acc: 0.7583 - val_loss: 0.6167 - val_acc: 0.7146\n",
            "Epoch 41/200\n",
            "3587/3587 [==============================] - 1s 212us/step - loss: 0.5304 - acc: 0.7533 - val_loss: 0.6193 - val_acc: 0.7152\n",
            "Epoch 42/200\n",
            "3587/3587 [==============================] - 1s 212us/step - loss: 0.5295 - acc: 0.7541 - val_loss: 0.6183 - val_acc: 0.7146\n",
            "Epoch 43/200\n",
            "3587/3587 [==============================] - 1s 225us/step - loss: 0.5347 - acc: 0.7543 - val_loss: 0.6152 - val_acc: 0.7241\n",
            "Epoch 44/200\n",
            "3587/3587 [==============================] - 1s 226us/step - loss: 0.5345 - acc: 0.7497 - val_loss: 0.6048 - val_acc: 0.7174\n",
            "Epoch 45/200\n",
            "3587/3587 [==============================] - 1s 229us/step - loss: 0.5257 - acc: 0.7589 - val_loss: 0.6016 - val_acc: 0.7168\n",
            "Epoch 46/200\n",
            "3587/3587 [==============================] - 1s 220us/step - loss: 0.5239 - acc: 0.7604 - val_loss: 0.6263 - val_acc: 0.7196\n",
            "Epoch 47/200\n",
            "3587/3587 [==============================] - 1s 232us/step - loss: 0.5272 - acc: 0.7600 - val_loss: 0.6218 - val_acc: 0.7219\n",
            "Epoch 48/200\n",
            "3587/3587 [==============================] - 1s 233us/step - loss: 0.5297 - acc: 0.7605 - val_loss: 0.6175 - val_acc: 0.7297\n",
            "Epoch 49/200\n",
            "3587/3587 [==============================] - 1s 228us/step - loss: 0.5262 - acc: 0.7501 - val_loss: 0.6268 - val_acc: 0.7269\n",
            "Epoch 50/200\n",
            "3587/3587 [==============================] - 1s 226us/step - loss: 0.5291 - acc: 0.7639 - val_loss: 0.6030 - val_acc: 0.7179\n",
            "Epoch 51/200\n",
            "3587/3587 [==============================] - 1s 228us/step - loss: 0.5248 - acc: 0.7605 - val_loss: 0.6226 - val_acc: 0.7252\n",
            "Epoch 52/200\n",
            "3587/3587 [==============================] - 1s 219us/step - loss: 0.5184 - acc: 0.7632 - val_loss: 0.6142 - val_acc: 0.7219\n",
            "Epoch 53/200\n",
            "3587/3587 [==============================] - 1s 222us/step - loss: 0.5120 - acc: 0.7632 - val_loss: 0.6511 - val_acc: 0.7219\n",
            "Epoch 54/200\n",
            "3587/3587 [==============================] - 1s 232us/step - loss: 0.5246 - acc: 0.7607 - val_loss: 0.6091 - val_acc: 0.7152\n",
            "Epoch 55/200\n",
            "3587/3587 [==============================] - 1s 233us/step - loss: 0.5065 - acc: 0.7650 - val_loss: 0.6402 - val_acc: 0.7252\n",
            "Epoch 56/200\n",
            "3587/3587 [==============================] - 1s 220us/step - loss: 0.5117 - acc: 0.7685 - val_loss: 0.6475 - val_acc: 0.7213\n",
            "Epoch 57/200\n",
            "3587/3587 [==============================] - 1s 232us/step - loss: 0.5175 - acc: 0.7626 - val_loss: 0.6010 - val_acc: 0.7246\n",
            "Epoch 58/200\n",
            "3587/3587 [==============================] - 1s 223us/step - loss: 0.5060 - acc: 0.7675 - val_loss: 0.5940 - val_acc: 0.7246\n",
            "Epoch 59/200\n",
            "3587/3587 [==============================] - 1s 218us/step - loss: 0.5150 - acc: 0.7731 - val_loss: 0.6128 - val_acc: 0.7280\n",
            "Epoch 60/200\n",
            "3587/3587 [==============================] - 1s 216us/step - loss: 0.5077 - acc: 0.7672 - val_loss: 0.6053 - val_acc: 0.7324\n",
            "Epoch 61/200\n",
            "3587/3587 [==============================] - 1s 216us/step - loss: 0.5065 - acc: 0.7729 - val_loss: 0.5988 - val_acc: 0.7285\n",
            "Epoch 62/200\n",
            "3587/3587 [==============================] - 1s 233us/step - loss: 0.5048 - acc: 0.7753 - val_loss: 0.6069 - val_acc: 0.7302\n",
            "Epoch 63/200\n",
            "3587/3587 [==============================] - 1s 241us/step - loss: 0.5036 - acc: 0.7775 - val_loss: 0.5794 - val_acc: 0.7302\n",
            "Epoch 64/200\n",
            "3587/3587 [==============================] - 1s 248us/step - loss: 0.5130 - acc: 0.7643 - val_loss: 0.6155 - val_acc: 0.7336\n",
            "Epoch 65/200\n",
            "3587/3587 [==============================] - 1s 253us/step - loss: 0.5008 - acc: 0.7745 - val_loss: 0.6143 - val_acc: 0.7324\n",
            "Epoch 66/200\n",
            "3587/3587 [==============================] - 1s 249us/step - loss: 0.5005 - acc: 0.7707 - val_loss: 0.6377 - val_acc: 0.7280\n",
            "Epoch 67/200\n",
            "3587/3587 [==============================] - 1s 245us/step - loss: 0.4920 - acc: 0.7803 - val_loss: 0.6242 - val_acc: 0.7235\n",
            "Epoch 68/200\n",
            "3587/3587 [==============================] - 1s 235us/step - loss: 0.4991 - acc: 0.7792 - val_loss: 0.6368 - val_acc: 0.7246\n",
            "Epoch 69/200\n",
            "3587/3587 [==============================] - 1s 219us/step - loss: 0.4981 - acc: 0.7767 - val_loss: 0.6217 - val_acc: 0.7274\n",
            "Epoch 70/200\n",
            "3587/3587 [==============================] - 1s 223us/step - loss: 0.5026 - acc: 0.7777 - val_loss: 0.6085 - val_acc: 0.7324\n",
            "Epoch 71/200\n",
            "3587/3587 [==============================] - 1s 225us/step - loss: 0.5028 - acc: 0.7743 - val_loss: 0.6133 - val_acc: 0.7274\n",
            "Epoch 72/200\n",
            "3587/3587 [==============================] - 1s 208us/step - loss: 0.4893 - acc: 0.7828 - val_loss: 0.6269 - val_acc: 0.7313\n",
            "Epoch 73/200\n",
            "3587/3587 [==============================] - 1s 211us/step - loss: 0.4862 - acc: 0.7835 - val_loss: 0.6425 - val_acc: 0.7285\n",
            "Epoch 74/200\n",
            "3587/3587 [==============================] - 1s 226us/step - loss: 0.4867 - acc: 0.7862 - val_loss: 0.6330 - val_acc: 0.7341\n",
            "Epoch 75/200\n",
            "3587/3587 [==============================] - 1s 233us/step - loss: 0.4974 - acc: 0.7827 - val_loss: 0.6203 - val_acc: 0.7302\n",
            "Epoch 76/200\n",
            "3587/3587 [==============================] - 1s 218us/step - loss: 0.4966 - acc: 0.7763 - val_loss: 0.6094 - val_acc: 0.7297\n",
            "Epoch 77/200\n",
            "3587/3587 [==============================] - 1s 231us/step - loss: 0.4963 - acc: 0.7753 - val_loss: 0.6303 - val_acc: 0.7336\n",
            "Epoch 78/200\n",
            "3587/3587 [==============================] - 1s 231us/step - loss: 0.4882 - acc: 0.7881 - val_loss: 0.6182 - val_acc: 0.7330\n",
            "Epoch 79/200\n",
            "3587/3587 [==============================] - 1s 226us/step - loss: 0.4831 - acc: 0.7799 - val_loss: 0.6345 - val_acc: 0.7347\n",
            "Epoch 80/200\n",
            "3587/3587 [==============================] - 1s 229us/step - loss: 0.4851 - acc: 0.7824 - val_loss: 0.6158 - val_acc: 0.7386\n",
            "Epoch 81/200\n",
            "3587/3587 [==============================] - 1s 219us/step - loss: 0.4903 - acc: 0.7812 - val_loss: 0.6357 - val_acc: 0.7380\n",
            "Epoch 82/200\n",
            "3587/3587 [==============================] - 1s 226us/step - loss: 0.4907 - acc: 0.7803 - val_loss: 0.6160 - val_acc: 0.7391\n",
            "Epoch 83/200\n",
            "3587/3587 [==============================] - 1s 229us/step - loss: 0.4786 - acc: 0.7849 - val_loss: 0.6597 - val_acc: 0.7391\n",
            "Epoch 84/200\n",
            "3587/3587 [==============================] - 1s 231us/step - loss: 0.4852 - acc: 0.7838 - val_loss: 0.6353 - val_acc: 0.7408\n",
            "Epoch 85/200\n",
            "3587/3587 [==============================] - 1s 233us/step - loss: 0.4824 - acc: 0.7860 - val_loss: 0.5972 - val_acc: 0.7492\n",
            "Epoch 86/200\n",
            "3587/3587 [==============================] - 1s 227us/step - loss: 0.4759 - acc: 0.7832 - val_loss: 0.6501 - val_acc: 0.7347\n",
            "Epoch 87/200\n",
            "3587/3587 [==============================] - 1s 223us/step - loss: 0.4847 - acc: 0.7837 - val_loss: 0.6200 - val_acc: 0.7441\n",
            "Epoch 88/200\n",
            "3587/3587 [==============================] - 1s 231us/step - loss: 0.4939 - acc: 0.7830 - val_loss: 0.6057 - val_acc: 0.7453\n",
            "Epoch 89/200\n",
            "3587/3587 [==============================] - 1s 232us/step - loss: 0.4845 - acc: 0.7856 - val_loss: 0.6177 - val_acc: 0.7486\n",
            "Epoch 90/200\n",
            "3587/3587 [==============================] - 1s 231us/step - loss: 0.4739 - acc: 0.7869 - val_loss: 0.6412 - val_acc: 0.7391\n",
            "Epoch 91/200\n",
            "3587/3587 [==============================] - 1s 231us/step - loss: 0.4760 - acc: 0.7869 - val_loss: 0.6539 - val_acc: 0.7402\n",
            "Epoch 92/200\n",
            "3587/3587 [==============================] - 1s 229us/step - loss: 0.4832 - acc: 0.7827 - val_loss: 0.6764 - val_acc: 0.7497\n",
            "Epoch 93/200\n",
            "3587/3587 [==============================] - 1s 221us/step - loss: 0.4750 - acc: 0.7877 - val_loss: 0.6842 - val_acc: 0.7369\n",
            "Epoch 94/200\n",
            "3587/3587 [==============================] - 1s 224us/step - loss: 0.4754 - acc: 0.7877 - val_loss: 0.6703 - val_acc: 0.7430\n",
            "Epoch 95/200\n",
            "3587/3587 [==============================] - 1s 230us/step - loss: 0.4647 - acc: 0.7897 - val_loss: 0.6693 - val_acc: 0.7380\n",
            "Epoch 96/200\n",
            "3587/3587 [==============================] - 1s 226us/step - loss: 0.4756 - acc: 0.7855 - val_loss: 0.6748 - val_acc: 0.7436\n",
            "Epoch 97/200\n",
            "3587/3587 [==============================] - 1s 216us/step - loss: 0.4633 - acc: 0.7894 - val_loss: 0.6568 - val_acc: 0.7402\n",
            "Epoch 98/200\n",
            "3587/3587 [==============================] - 1s 222us/step - loss: 0.4603 - acc: 0.7983 - val_loss: 0.7022 - val_acc: 0.7369\n",
            "Epoch 99/200\n",
            "3587/3587 [==============================] - 1s 222us/step - loss: 0.4795 - acc: 0.7894 - val_loss: 0.6718 - val_acc: 0.7436\n",
            "Epoch 100/200\n",
            "3587/3587 [==============================] - 1s 225us/step - loss: 0.4714 - acc: 0.7915 - val_loss: 0.7068 - val_acc: 0.7375\n",
            "Epoch 101/200\n",
            "3587/3587 [==============================] - 1s 229us/step - loss: 0.4631 - acc: 0.7972 - val_loss: 0.7171 - val_acc: 0.7436\n",
            "Epoch 102/200\n",
            "3587/3587 [==============================] - 1s 228us/step - loss: 0.4641 - acc: 0.7920 - val_loss: 0.6951 - val_acc: 0.7425\n",
            "Epoch 103/200\n",
            "3587/3587 [==============================] - 1s 234us/step - loss: 0.4649 - acc: 0.7858 - val_loss: 0.6653 - val_acc: 0.7425\n",
            "Epoch 104/200\n",
            "3587/3587 [==============================] - 1s 240us/step - loss: 0.4572 - acc: 0.7945 - val_loss: 0.6721 - val_acc: 0.7419\n",
            "Epoch 105/200\n",
            "3587/3587 [==============================] - 1s 231us/step - loss: 0.4551 - acc: 0.7996 - val_loss: 0.6834 - val_acc: 0.7419\n",
            "Epoch 106/200\n",
            "3587/3587 [==============================] - 1s 222us/step - loss: 0.4679 - acc: 0.7873 - val_loss: 0.7250 - val_acc: 0.7358\n",
            "Epoch 107/200\n",
            "3587/3587 [==============================] - 1s 216us/step - loss: 0.4720 - acc: 0.7909 - val_loss: 0.6665 - val_acc: 0.7380\n",
            "Epoch 108/200\n",
            "3587/3587 [==============================] - 1s 215us/step - loss: 0.4676 - acc: 0.7945 - val_loss: 0.6467 - val_acc: 0.7397\n",
            "Epoch 109/200\n",
            "3587/3587 [==============================] - 1s 214us/step - loss: 0.4532 - acc: 0.8030 - val_loss: 0.6928 - val_acc: 0.7369\n",
            "Epoch 110/200\n",
            "3587/3587 [==============================] - 1s 207us/step - loss: 0.4548 - acc: 0.8023 - val_loss: 0.7006 - val_acc: 0.7380\n",
            "Epoch 111/200\n",
            "3587/3587 [==============================] - 1s 218us/step - loss: 0.4561 - acc: 0.7966 - val_loss: 0.6770 - val_acc: 0.7363\n",
            "Epoch 112/200\n",
            "3587/3587 [==============================] - 1s 224us/step - loss: 0.4523 - acc: 0.8025 - val_loss: 0.6776 - val_acc: 0.7347\n",
            "Epoch 113/200\n",
            "3587/3587 [==============================] - 1s 223us/step - loss: 0.4594 - acc: 0.7940 - val_loss: 0.6812 - val_acc: 0.7425\n",
            "Epoch 114/200\n",
            "3587/3587 [==============================] - 1s 215us/step - loss: 0.4445 - acc: 0.8001 - val_loss: 0.7590 - val_acc: 0.7336\n",
            "Epoch 115/200\n",
            "3587/3587 [==============================] - 1s 218us/step - loss: 0.4608 - acc: 0.7922 - val_loss: 0.6809 - val_acc: 0.7369\n",
            "Epoch 116/200\n",
            "3587/3587 [==============================] - 1s 219us/step - loss: 0.4550 - acc: 0.7994 - val_loss: 0.7140 - val_acc: 0.7436\n",
            "Epoch 117/200\n",
            "3587/3587 [==============================] - 1s 221us/step - loss: 0.4534 - acc: 0.7958 - val_loss: 0.6767 - val_acc: 0.7414\n",
            "Epoch 118/200\n",
            "3587/3587 [==============================] - 1s 215us/step - loss: 0.4589 - acc: 0.8028 - val_loss: 0.6715 - val_acc: 0.7425\n",
            "Epoch 119/200\n",
            "3587/3587 [==============================] - 1s 228us/step - loss: 0.4565 - acc: 0.7966 - val_loss: 0.6428 - val_acc: 0.7419\n",
            "Epoch 120/200\n",
            "3587/3587 [==============================] - 1s 225us/step - loss: 0.4634 - acc: 0.7943 - val_loss: 0.6549 - val_acc: 0.7402\n",
            "Epoch 121/200\n",
            "3587/3587 [==============================] - 1s 218us/step - loss: 0.4517 - acc: 0.8033 - val_loss: 0.6718 - val_acc: 0.7402\n",
            "Epoch 122/200\n",
            "3587/3587 [==============================] - 1s 226us/step - loss: 0.4553 - acc: 0.8005 - val_loss: 0.6706 - val_acc: 0.7419\n",
            "Epoch 123/200\n",
            "3587/3587 [==============================] - 1s 220us/step - loss: 0.4461 - acc: 0.7976 - val_loss: 0.6786 - val_acc: 0.7469\n",
            "Epoch 124/200\n",
            "3587/3587 [==============================] - 1s 227us/step - loss: 0.4393 - acc: 0.8026 - val_loss: 0.6619 - val_acc: 0.7469\n",
            "Epoch 125/200\n",
            "3587/3587 [==============================] - 1s 226us/step - loss: 0.4616 - acc: 0.7916 - val_loss: 0.7040 - val_acc: 0.7375\n",
            "Epoch 126/200\n",
            "3587/3587 [==============================] - 1s 232us/step - loss: 0.4544 - acc: 0.8014 - val_loss: 0.6665 - val_acc: 0.7430\n",
            "Epoch 127/200\n",
            "3587/3587 [==============================] - 1s 221us/step - loss: 0.4544 - acc: 0.8004 - val_loss: 0.6885 - val_acc: 0.7475\n",
            "Epoch 128/200\n",
            "3587/3587 [==============================] - 1s 227us/step - loss: 0.4378 - acc: 0.8032 - val_loss: 0.7118 - val_acc: 0.7559\n",
            "Epoch 129/200\n",
            "3587/3587 [==============================] - 1s 218us/step - loss: 0.4501 - acc: 0.8029 - val_loss: 0.6373 - val_acc: 0.7570\n",
            "Epoch 130/200\n",
            "3587/3587 [==============================] - 1s 229us/step - loss: 0.4450 - acc: 0.8001 - val_loss: 0.6542 - val_acc: 0.7514\n",
            "Epoch 131/200\n",
            "3587/3587 [==============================] - 1s 237us/step - loss: 0.4389 - acc: 0.8023 - val_loss: 0.7198 - val_acc: 0.7458\n",
            "Epoch 132/200\n",
            "3587/3587 [==============================] - 1s 232us/step - loss: 0.4497 - acc: 0.7994 - val_loss: 0.6751 - val_acc: 0.7458\n",
            "Epoch 133/200\n",
            "3587/3587 [==============================] - 1s 227us/step - loss: 0.4433 - acc: 0.8074 - val_loss: 0.6924 - val_acc: 0.7464\n",
            "Epoch 134/200\n",
            "3587/3587 [==============================] - 1s 227us/step - loss: 0.4535 - acc: 0.8054 - val_loss: 0.6708 - val_acc: 0.7469\n",
            "Epoch 135/200\n",
            "3587/3587 [==============================] - 1s 221us/step - loss: 0.4306 - acc: 0.8131 - val_loss: 0.7203 - val_acc: 0.7559\n",
            "Epoch 136/200\n",
            "3587/3587 [==============================] - 1s 219us/step - loss: 0.4424 - acc: 0.8074 - val_loss: 0.7062 - val_acc: 0.7542\n",
            "Epoch 137/200\n",
            "3587/3587 [==============================] - 1s 216us/step - loss: 0.4387 - acc: 0.8067 - val_loss: 0.6808 - val_acc: 0.7531\n",
            "Epoch 138/200\n",
            "3587/3587 [==============================] - 1s 229us/step - loss: 0.4380 - acc: 0.8044 - val_loss: 0.7090 - val_acc: 0.7480\n",
            "Epoch 139/200\n",
            "3587/3587 [==============================] - 1s 223us/step - loss: 0.4576 - acc: 0.7986 - val_loss: 0.6888 - val_acc: 0.7447\n",
            "Epoch 140/200\n",
            "3587/3587 [==============================] - 1s 217us/step - loss: 0.4417 - acc: 0.8037 - val_loss: 0.7448 - val_acc: 0.7436\n",
            "Epoch 141/200\n",
            "3587/3587 [==============================] - 1s 229us/step - loss: 0.4441 - acc: 0.8018 - val_loss: 0.7307 - val_acc: 0.7391\n",
            "Epoch 142/200\n",
            "3587/3587 [==============================] - 1s 228us/step - loss: 0.4368 - acc: 0.8093 - val_loss: 0.6928 - val_acc: 0.7419\n",
            "Epoch 143/200\n",
            "3587/3587 [==============================] - 1s 222us/step - loss: 0.4449 - acc: 0.7943 - val_loss: 0.6897 - val_acc: 0.7469\n",
            "Epoch 144/200\n",
            "3587/3587 [==============================] - 1s 223us/step - loss: 0.4404 - acc: 0.8068 - val_loss: 0.7284 - val_acc: 0.7486\n",
            "Epoch 145/200\n",
            "3587/3587 [==============================] - 1s 234us/step - loss: 0.4358 - acc: 0.8118 - val_loss: 0.6831 - val_acc: 0.7525\n",
            "Epoch 146/200\n",
            "3587/3587 [==============================] - 1s 229us/step - loss: 0.4447 - acc: 0.8011 - val_loss: 0.7145 - val_acc: 0.7453\n",
            "Epoch 147/200\n",
            "3587/3587 [==============================] - 1s 226us/step - loss: 0.4431 - acc: 0.8053 - val_loss: 0.7025 - val_acc: 0.7503\n",
            "Epoch 148/200\n",
            "3587/3587 [==============================] - 1s 228us/step - loss: 0.4371 - acc: 0.8055 - val_loss: 0.7216 - val_acc: 0.7425\n",
            "Epoch 149/200\n",
            "3587/3587 [==============================] - 1s 225us/step - loss: 0.4278 - acc: 0.8074 - val_loss: 0.7567 - val_acc: 0.7408\n",
            "Epoch 150/200\n",
            "3587/3587 [==============================] - 1s 234us/step - loss: 0.4416 - acc: 0.8001 - val_loss: 0.7693 - val_acc: 0.7430\n",
            "Epoch 151/200\n",
            "3587/3587 [==============================] - 1s 231us/step - loss: 0.4407 - acc: 0.8026 - val_loss: 0.7278 - val_acc: 0.7402\n",
            "Epoch 152/200\n",
            "3587/3587 [==============================] - 1s 240us/step - loss: 0.4340 - acc: 0.8051 - val_loss: 0.7559 - val_acc: 0.7425\n",
            "Epoch 153/200\n",
            "3587/3587 [==============================] - 1s 258us/step - loss: 0.4433 - acc: 0.8074 - val_loss: 0.7385 - val_acc: 0.7402\n",
            "Epoch 154/200\n",
            "3587/3587 [==============================] - 1s 247us/step - loss: 0.4310 - acc: 0.8099 - val_loss: 0.7444 - val_acc: 0.7447\n",
            "Epoch 155/200\n",
            "3587/3587 [==============================] - 1s 247us/step - loss: 0.4402 - acc: 0.8012 - val_loss: 0.7570 - val_acc: 0.7492\n",
            "Epoch 156/200\n",
            "3587/3587 [==============================] - 1s 230us/step - loss: 0.4277 - acc: 0.8107 - val_loss: 0.7570 - val_acc: 0.7425\n",
            "Epoch 157/200\n",
            "3587/3587 [==============================] - 1s 245us/step - loss: 0.4333 - acc: 0.8101 - val_loss: 0.7131 - val_acc: 0.7408\n",
            "Epoch 158/200\n",
            "3587/3587 [==============================] - 1s 259us/step - loss: 0.4314 - acc: 0.8018 - val_loss: 0.8002 - val_acc: 0.7402\n",
            "Epoch 159/200\n",
            "3587/3587 [==============================] - 1s 252us/step - loss: 0.4402 - acc: 0.8053 - val_loss: 0.7897 - val_acc: 0.7425\n",
            "Epoch 160/200\n",
            "3587/3587 [==============================] - 1s 252us/step - loss: 0.4278 - acc: 0.8068 - val_loss: 0.7339 - val_acc: 0.7469\n",
            "Epoch 161/200\n",
            "3587/3587 [==============================] - 1s 250us/step - loss: 0.4411 - acc: 0.8032 - val_loss: 0.7060 - val_acc: 0.7475\n",
            "Epoch 162/200\n",
            "3587/3587 [==============================] - 1s 250us/step - loss: 0.4245 - acc: 0.8096 - val_loss: 0.7534 - val_acc: 0.7492\n",
            "Epoch 163/200\n",
            "3587/3587 [==============================] - 1s 249us/step - loss: 0.4268 - acc: 0.8115 - val_loss: 0.7246 - val_acc: 0.7436\n",
            "Epoch 164/200\n",
            "3587/3587 [==============================] - 1s 236us/step - loss: 0.4244 - acc: 0.8111 - val_loss: 0.7431 - val_acc: 0.7458\n",
            "Epoch 165/200\n",
            "3587/3587 [==============================] - 1s 227us/step - loss: 0.4230 - acc: 0.8182 - val_loss: 0.7122 - val_acc: 0.7469\n",
            "Epoch 166/200\n",
            "3587/3587 [==============================] - 1s 217us/step - loss: 0.4245 - acc: 0.8121 - val_loss: 0.7140 - val_acc: 0.7514\n",
            "Epoch 167/200\n",
            "3587/3587 [==============================] - 1s 231us/step - loss: 0.4238 - acc: 0.8136 - val_loss: 0.7676 - val_acc: 0.7447\n",
            "Epoch 168/200\n",
            "3587/3587 [==============================] - 1s 228us/step - loss: 0.4154 - acc: 0.8143 - val_loss: 0.7816 - val_acc: 0.7447\n",
            "Epoch 169/200\n",
            "3587/3587 [==============================] - 1s 220us/step - loss: 0.4369 - acc: 0.8046 - val_loss: 0.7084 - val_acc: 0.7497\n",
            "Epoch 170/200\n",
            "3587/3587 [==============================] - 1s 224us/step - loss: 0.4319 - acc: 0.8022 - val_loss: 0.7281 - val_acc: 0.7503\n",
            "Epoch 171/200\n",
            "3587/3587 [==============================] - 1s 210us/step - loss: 0.4260 - acc: 0.8125 - val_loss: 0.7599 - val_acc: 0.7436\n",
            "Epoch 172/200\n",
            "3587/3587 [==============================] - 1s 214us/step - loss: 0.4259 - acc: 0.8110 - val_loss: 0.7429 - val_acc: 0.7520\n",
            "Epoch 173/200\n",
            "3587/3587 [==============================] - 1s 226us/step - loss: 0.4249 - acc: 0.8157 - val_loss: 0.7413 - val_acc: 0.7536\n",
            "Epoch 174/200\n",
            "3587/3587 [==============================] - 1s 232us/step - loss: 0.4460 - acc: 0.8075 - val_loss: 0.7052 - val_acc: 0.7480\n",
            "Epoch 175/200\n",
            "3587/3587 [==============================] - 1s 224us/step - loss: 0.4288 - acc: 0.8067 - val_loss: 0.6734 - val_acc: 0.7531\n",
            "Epoch 176/200\n",
            "3587/3587 [==============================] - 1s 220us/step - loss: 0.4217 - acc: 0.8097 - val_loss: 0.7306 - val_acc: 0.7492\n",
            "Epoch 177/200\n",
            "3587/3587 [==============================] - 1s 215us/step - loss: 0.4158 - acc: 0.8224 - val_loss: 0.7529 - val_acc: 0.7447\n",
            "Epoch 178/200\n",
            "3587/3587 [==============================] - 1s 218us/step - loss: 0.4245 - acc: 0.8166 - val_loss: 0.7556 - val_acc: 0.7464\n",
            "Epoch 179/200\n",
            "3587/3587 [==============================] - 1s 220us/step - loss: 0.4284 - acc: 0.8100 - val_loss: 0.7248 - val_acc: 0.7458\n",
            "Epoch 180/200\n",
            "3587/3587 [==============================] - 1s 220us/step - loss: 0.4135 - acc: 0.8173 - val_loss: 0.7911 - val_acc: 0.7542\n",
            "Epoch 181/200\n",
            "3587/3587 [==============================] - 1s 228us/step - loss: 0.4260 - acc: 0.8128 - val_loss: 0.7883 - val_acc: 0.7570\n",
            "Epoch 182/200\n",
            "3587/3587 [==============================] - 1s 208us/step - loss: 0.4149 - acc: 0.8160 - val_loss: 0.7503 - val_acc: 0.7592\n",
            "Epoch 183/200\n",
            "3587/3587 [==============================] - 1s 214us/step - loss: 0.4338 - acc: 0.8057 - val_loss: 0.7123 - val_acc: 0.7536\n",
            "Epoch 184/200\n",
            "3587/3587 [==============================] - 1s 213us/step - loss: 0.4219 - acc: 0.8135 - val_loss: 0.7343 - val_acc: 0.7525\n",
            "Epoch 185/200\n",
            "3587/3587 [==============================] - 1s 226us/step - loss: 0.4291 - acc: 0.8124 - val_loss: 0.7410 - val_acc: 0.7447\n",
            "Epoch 186/200\n",
            "3587/3587 [==============================] - 1s 227us/step - loss: 0.4311 - acc: 0.8071 - val_loss: 0.7228 - val_acc: 0.7458\n",
            "Epoch 187/200\n",
            "3587/3587 [==============================] - 1s 228us/step - loss: 0.4080 - acc: 0.8221 - val_loss: 0.7427 - val_acc: 0.7480\n",
            "Epoch 188/200\n",
            "3587/3587 [==============================] - 1s 229us/step - loss: 0.4194 - acc: 0.8154 - val_loss: 0.7421 - val_acc: 0.7514\n",
            "Epoch 189/200\n",
            "3587/3587 [==============================] - 1s 221us/step - loss: 0.4174 - acc: 0.8154 - val_loss: 0.7080 - val_acc: 0.7559\n",
            "Epoch 190/200\n",
            "3587/3587 [==============================] - 1s 216us/step - loss: 0.4129 - acc: 0.8138 - val_loss: 0.7472 - val_acc: 0.7503\n",
            "Epoch 191/200\n",
            "3587/3587 [==============================] - 1s 209us/step - loss: 0.4279 - acc: 0.8107 - val_loss: 0.6746 - val_acc: 0.7570\n",
            "Epoch 192/200\n",
            "3587/3587 [==============================] - 1s 216us/step - loss: 0.4191 - acc: 0.8160 - val_loss: 0.6762 - val_acc: 0.7547\n",
            "Epoch 193/200\n",
            "3587/3587 [==============================] - 1s 219us/step - loss: 0.4176 - acc: 0.8231 - val_loss: 0.7244 - val_acc: 0.7514\n",
            "Epoch 194/200\n",
            "3587/3587 [==============================] - 1s 225us/step - loss: 0.4221 - acc: 0.8135 - val_loss: 0.7087 - val_acc: 0.7514\n",
            "Epoch 195/200\n",
            "3587/3587 [==============================] - 1s 225us/step - loss: 0.4217 - acc: 0.8099 - val_loss: 0.7348 - val_acc: 0.7503\n",
            "Epoch 196/200\n",
            "3587/3587 [==============================] - 1s 223us/step - loss: 0.4068 - acc: 0.8191 - val_loss: 0.7507 - val_acc: 0.7547\n",
            "Epoch 197/200\n",
            "3587/3587 [==============================] - 1s 221us/step - loss: 0.4198 - acc: 0.8110 - val_loss: 0.7043 - val_acc: 0.7503\n",
            "Epoch 198/200\n",
            "3587/3587 [==============================] - 1s 224us/step - loss: 0.4215 - acc: 0.8115 - val_loss: 0.6881 - val_acc: 0.7525\n",
            "Epoch 199/200\n",
            "3587/3587 [==============================] - 1s 227us/step - loss: 0.4109 - acc: 0.8152 - val_loss: 0.6945 - val_acc: 0.7480\n",
            "Epoch 200/200\n",
            "3587/3587 [==============================] - 1s 213us/step - loss: 0.4036 - acc: 0.8235 - val_loss: 0.7483 - val_acc: 0.7447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UuRIH0Mu4PUH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y_pred_test_label = classifier.predict(X_test_lstm)\n",
        "y_pred_test=np.argmax(Y_pred_test_label,axis =1)\n",
        "y_pred_test\n",
        "Y_pred_train_label = classifier.predict(X_train_lstm)\n",
        "y_pred_train = np.argmax(Y_pred_train_label,axis=1)\n",
        "Y_test_true = Y_test_org.astype(np.int)\n",
        "Y_train_true = Y_train_org.astype(np.int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7h6-ufM24PUM",
        "colab_type": "code",
        "outputId": "f6776684-2683-45c3-88e8-9bf3c8c6ae68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_test_true,y_pred_test))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.70      0.73       443\n",
            "           1       0.73      0.79      0.76       454\n",
            "\n",
            "   micro avg       0.74      0.74      0.74       897\n",
            "   macro avg       0.75      0.74      0.74       897\n",
            "weighted avg       0.75      0.74      0.74       897\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-mqtFfYq4PUP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Hw1b3Jf4PUU",
        "colab_type": "code",
        "outputId": "978923d2-7734-4360-dc87-64a5fb4c6640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "### for softmax function\n",
        "print(\"TRAIN:  \\n\",confusion_matrix(y_pred_train,Y_train_true))\n",
        "print(\"\\nTest:  \\n\",confusion_matrix(y_pred_test,Y_test_true))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN:  \n",
            " [[1395  160]\n",
            " [ 404 1628]]\n",
            "\n",
            "Test:  \n",
            " [[310  96]\n",
            " [133 358]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6sbPDlpW4PUZ",
        "colab_type": "code",
        "outputId": "233b2a2a-e422-4c64-d1cb-f6f9ab8bfec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(Y_train_true,y_pred_train))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.78      0.83      1799\n",
            "           1       0.80      0.91      0.85      1788\n",
            "\n",
            "   micro avg       0.84      0.84      0.84      3587\n",
            "   macro avg       0.85      0.84      0.84      3587\n",
            "weighted avg       0.85      0.84      0.84      3587\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1ClHiT1K4PUj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier.save(\"rnn_classifier.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tfxebo8U4PUo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "new_model = load_model(\"rnn_classifier.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j5LQTgpn4PUv",
        "colab_type": "code",
        "outputId": "bcc51caa-9265-4786-eaac-3db93c8984c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "new_model.summary()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_4 (LSTM)                (None, None, 40)          21760     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, None, 40)          0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, None, 20)          4880      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, None, 20)          0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 20)                3280      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 42        \n",
            "=================================================================\n",
            "Total params: 29,962\n",
            "Trainable params: 29,962\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2WhxPkOH4PU4",
        "colab_type": "code",
        "outputId": "7c7388a9-95db-4957-f170-e9c25fb56101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2839
        }
      },
      "cell_type": "code",
      "source": [
        "new_model.get_weights()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 1.26522658e-02, -1.45466877e-02,  5.32281734e-02, ...,\n",
              "          1.94309264e-01,  1.73231259e-01,  1.97624599e-04],\n",
              "        [ 1.72596708e-01,  3.61912549e-01, -1.56815454e-01, ...,\n",
              "         -1.52456895e-01, -2.23304018e-01, -8.94366950e-02],\n",
              "        [-3.51972759e-01,  1.42514795e-01, -6.63460866e-02, ...,\n",
              "         -2.25700513e-01,  1.13745153e-01, -4.46361512e-01],\n",
              "        ...,\n",
              "        [-1.17162585e-01,  1.50364965e-01,  1.16100907e-01, ...,\n",
              "          1.03792459e-01, -3.79721448e-02,  1.18878633e-01],\n",
              "        [-1.50815308e-01, -3.23526040e-02,  4.69220579e-02, ...,\n",
              "         -3.83230522e-02,  5.05429506e-03,  1.05341494e-01],\n",
              "        [-7.68706948e-02,  1.54663324e-02, -5.09368256e-02, ...,\n",
              "          1.02868140e-01,  7.49372542e-02,  1.30157769e-01]], dtype=float32),\n",
              " array([[-0.20377603, -0.01456034,  0.19023797, ..., -0.05616831,\n",
              "          0.05310729,  0.04685752],\n",
              "        [-0.05206563, -0.05932875,  0.00851739, ...,  0.04992067,\n",
              "         -0.08263762,  0.02963865],\n",
              "        [-0.0592427 ,  0.17769136, -0.04637443, ...,  0.02400932,\n",
              "         -0.09374895, -0.07978795],\n",
              "        ...,\n",
              "        [ 0.03589704,  0.06426472,  0.11699631, ..., -0.02495779,\n",
              "          0.09614407,  0.11615638],\n",
              "        [-0.10541575, -0.05547469,  0.05820809, ...,  0.05611779,\n",
              "          0.07919213, -0.01355637],\n",
              "        [ 0.02535549,  0.04314366, -0.03302761, ..., -0.0111187 ,\n",
              "          0.08250114, -0.01929838]], dtype=float32),\n",
              " array([-0.02768456, -0.32409927,  0.1308004 ,  0.06842664, -0.30233648,\n",
              "         0.23643921,  0.11765528, -0.20970464,  0.12737891, -0.30941612,\n",
              "        -0.3422784 , -0.252894  ,  0.12022384,  0.31875336,  0.13829337,\n",
              "         0.2480112 ,  0.06074642,  0.12827213, -0.04951361,  0.19734442,\n",
              "         0.03738827, -0.18990456, -0.304342  , -0.0438851 ,  0.06537274,\n",
              "        -0.11759853, -0.04082467, -0.24050076, -0.12211698, -0.26755118,\n",
              "         0.42745188,  0.1348193 ,  0.04771565, -0.16957888,  0.03415546,\n",
              "        -0.05007578, -0.12943006, -0.15348303,  0.00324716,  0.10589781,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "        -0.13078806, -0.04397471, -0.27597928, -0.4289216 , -0.20216472,\n",
              "         0.31437352, -0.39077386, -0.26077884, -0.31830207, -0.41292444,\n",
              "        -0.31074107, -0.38453072, -0.28910485, -0.15200883, -0.196297  ,\n",
              "        -0.4943969 , -0.424973  , -0.41152796, -0.35593024, -0.18831627,\n",
              "        -0.25849906, -0.21329899, -0.9968933 , -0.28686357, -0.12973438,\n",
              "        -0.42874265, -0.29240215, -0.22724879, -0.14245665, -0.15846659,\n",
              "         0.13923588, -0.45228377, -0.40392196, -0.1474239 , -0.41329327,\n",
              "        -0.31641537,  0.02276545, -0.3809573 , -0.10453452, -0.40039933,\n",
              "         0.12737024, -0.37311906,  0.3533586 ,  0.21340422, -0.08627753,\n",
              "         0.09980085,  0.08303241,  0.1560408 ,  0.08203755, -0.10881772,\n",
              "        -0.25787774, -0.27107832,  0.26502395,  0.14573811,  0.10242114,\n",
              "         0.201906  , -0.10518879,  0.06254783, -0.01456563, -0.12828285,\n",
              "         0.01512515,  0.07743183,  0.02655708,  0.07333122, -0.10309242,\n",
              "         0.01298983, -0.08642726,  0.04974436, -0.16895607, -0.03761452,\n",
              "        -0.27055016,  0.13566007,  0.07967605, -0.09591121, -0.06329243,\n",
              "        -0.06064246,  0.03365317, -0.05800211,  0.18814039,  0.15141007],\n",
              "       dtype=float32),\n",
              " array([[ 0.46497554,  0.45867184,  0.3092737 , ...,  0.6529139 ,\n",
              "          0.07282063, -0.2724878 ],\n",
              "        [ 0.10569703,  1.1246376 , -0.28303576, ...,  0.3185346 ,\n",
              "         -0.6423692 , -0.49314106],\n",
              "        [ 0.6402342 ,  0.50891566,  0.8976943 , ..., -0.0767148 ,\n",
              "         -0.02561526,  0.12093613],\n",
              "        ...,\n",
              "        [ 0.55331105, -0.09397851,  0.8069948 , ...,  0.51579714,\n",
              "         -1.2432221 , -0.11738738],\n",
              "        [ 0.36228868,  0.525355  ,  0.5869833 , ...,  0.03781711,\n",
              "         -0.45042259, -0.53536135],\n",
              "        [ 0.32869148,  1.0569284 ,  0.63044477, ...,  0.36094913,\n",
              "          0.14186496,  0.07642689]], dtype=float32),\n",
              " array([[ 0.09677166,  0.12092712,  0.10261217, ...,  0.0160776 ,\n",
              "          0.02594667, -0.03796147],\n",
              "        [ 0.21576196,  0.02283552,  0.07613515, ...,  0.03364552,\n",
              "          0.06887068, -0.10338289],\n",
              "        [-0.1681579 , -0.08706269, -0.15907773, ..., -0.10341626,\n",
              "         -0.01372563,  0.00879879],\n",
              "        ...,\n",
              "        [ 0.01729495,  0.04407736, -0.1922544 , ..., -0.10815731,\n",
              "          0.0488814 ,  0.16901976],\n",
              "        [ 0.16323698, -0.01734222, -0.14114197, ..., -0.23283552,\n",
              "         -0.15454191, -0.09123774],\n",
              "        [ 0.15457644,  0.20498228, -0.03175565, ..., -0.05193025,\n",
              "          0.10843512, -0.02300418]], dtype=float32),\n",
              " array([ 0.02456434,  0.37435317, -0.35154942,  0.1210606 ,  0.4631862 ,\n",
              "         0.5335862 ,  0.65192807, -0.03399881, -0.13400508,  0.500217  ,\n",
              "         0.6353992 ,  0.04791358,  1.087163  ,  0.24778602, -0.43413252,\n",
              "         0.443598  ,  0.27153328,  0.5845612 ,  0.61989033,  0.5055699 ,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "        -0.50586116, -1.0606794 , -0.19697286, -0.7224966 ,  0.09821673,\n",
              "         0.21076272, -0.43577513, -0.03099121, -0.38252002,  0.12036347,\n",
              "        -0.13760318, -0.36296296, -0.3510458 ,  0.06509445, -0.71533304,\n",
              "         0.19212276,  0.00271206,  0.12038681, -0.06607275, -0.299423  ,\n",
              "         0.1938167 ,  0.48485163, -0.05665399, -0.11985398,  0.33397466,\n",
              "         0.42533866, -0.06017819, -0.0197855 , -0.23427509,  0.13194923,\n",
              "         0.86690867,  0.04664478, -0.49793696,  0.76599574, -0.2127838 ,\n",
              "         0.40056702,  0.15628812,  0.74568623,  0.99813795,  0.56492907],\n",
              "       dtype=float32),\n",
              " array([[-0.1006462 ,  1.2790802 , -0.10794902, ...,  1.2435043 ,\n",
              "         -0.06028048,  1.4073875 ],\n",
              "        [ 0.22086243,  0.15937938,  0.09998468, ...,  1.7907344 ,\n",
              "          0.14179373,  1.6973581 ],\n",
              "        [-0.10532144,  0.331516  ,  0.04044463, ...,  1.7408212 ,\n",
              "         -0.14711706,  1.5290711 ],\n",
              "        ...,\n",
              "        [ 0.0691869 ,  0.43818867,  0.25849748, ..., -0.3370734 ,\n",
              "          0.02357957, -0.22724552],\n",
              "        [-0.22404636,  0.9729032 ,  0.23626627, ...,  0.96916735,\n",
              "         -0.29976106,  1.0625068 ],\n",
              "        [-0.7324028 ,  0.72097933,  0.62293315, ...,  1.0607004 ,\n",
              "         -0.8472887 ,  1.0825033 ]], dtype=float32),\n",
              " array([[-0.0739892 ,  0.19360739,  0.01894099, ...,  0.07585064,\n",
              "         -0.06613648,  0.06471027],\n",
              "        [-0.15659541,  0.06772964,  0.02722092, ..., -0.15951687,\n",
              "         -0.14997053, -0.05137487],\n",
              "        [ 0.04531895, -0.03495788,  0.08072648, ...,  0.19698346,\n",
              "         -0.08672816, -0.12925945],\n",
              "        ...,\n",
              "        [ 0.05238457, -0.03219358, -0.09169126, ...,  0.12776202,\n",
              "          0.07402162, -0.08960811],\n",
              "        [ 0.04770053,  0.07331694,  0.00054937, ..., -0.05708342,\n",
              "         -0.04313188,  0.16752113],\n",
              "        [-0.17430528,  0.04698629, -0.02575785, ...,  0.32434443,\n",
              "          0.05845176, -0.01375934]], dtype=float32),\n",
              " array([-0.49082035,  0.09193044, -0.06874062, -0.36047366,  0.32071766,\n",
              "         0.49685913,  0.5372319 ,  0.12133596,  0.14763854,  0.20673767,\n",
              "        -0.07988312, -0.06290732, -0.32301313,  0.3980382 , -0.10068308,\n",
              "         0.14326403, -0.09236086,  0.329239  ,  0.1490238 ,  0.44921964,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
              "        -0.2203117 ,  0.16499576, -0.13456121, -0.2747015 ,  0.39544773,\n",
              "         0.43821004,  0.48771486, -0.22344296, -0.24463838, -0.23219863,\n",
              "        -0.681499  , -0.09488021, -0.20547938, -0.30856472, -0.32350725,\n",
              "        -0.2986224 , -0.3486198 ,  0.4132333 , -0.2932121 ,  0.46706   ,\n",
              "         0.33127543,  0.10632369, -0.02510971,  0.25527492,  0.40730757,\n",
              "         0.44706464,  0.5768762 , -0.00434191, -0.25653094, -0.50248706,\n",
              "        -0.30278677,  0.02865086,  0.2538168 , -0.4584647 , -0.30027866,\n",
              "        -0.3237823 , -0.2327472 ,  0.3921935 , -0.38018593,  0.506464  ],\n",
              "       dtype=float32),\n",
              " array([[ 0.3583985 , -0.37254414],\n",
              "        [ 0.25540772, -0.25245997],\n",
              "        [-0.6936477 ,  0.70999295],\n",
              "        [ 0.27096358, -0.2660296 ],\n",
              "        [-0.6898942 ,  0.6638873 ],\n",
              "        [-0.53613555,  0.57000464],\n",
              "        [-0.4857    ,  0.49616143],\n",
              "        [ 0.09795146, -0.09774833],\n",
              "        [ 0.11481068, -0.11523287],\n",
              "        [ 0.35397968, -0.3583569 ],\n",
              "        [-0.38055527,  0.37332356],\n",
              "        [ 0.13401097, -0.14924262],\n",
              "        [ 0.18316649, -0.18115537],\n",
              "        [ 0.38721865, -0.3839868 ],\n",
              "        [ 0.45199147, -0.44499373],\n",
              "        [ 0.20281419, -0.2017876 ],\n",
              "        [ 0.38910684, -0.3968638 ],\n",
              "        [-0.653474  ,  0.6103051 ],\n",
              "        [ 0.25941205, -0.252584  ],\n",
              "        [-0.5037495 ,  0.5118888 ]], dtype=float32),\n",
              " array([-0.34990105,  0.35094365], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "mxOhZ0144PU_",
        "colab_type": "code",
        "outputId": "af94fa25-14ab-469c-a305-55df36f8f93d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "new_model.optimizer"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.optimizers.RMSprop at 0x7f3f1efbabe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "metadata": {
        "id": "4QZcepbW4PVE",
        "colab_type": "code",
        "outputId": "0a388400-1c98-4341-82a6-7366c1feeda7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "new_model.predict(X_train_lstm)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8850626 , 0.11416557],\n",
              "       [0.50840765, 0.49298397],\n",
              "       [0.80122423, 0.1986976 ],\n",
              "       ...,\n",
              "       [0.13003074, 0.86986136],\n",
              "       [0.19666998, 0.80330724],\n",
              "       [0.3151698 , 0.6849288 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "metadata": {
        "id": "ANL4Cen94PVJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z9EAHI344PVO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}