{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "import keras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_E6oV3lV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29720</td>\n",
       "      <td>29720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  tweet\n",
       "label              \n",
       "0      29720  29720\n",
       "1       2242   2242"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_class_0, count_class_1 = df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_0 =  df.query('label==0')\n",
    "df_class_1 =  df.query('label==1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_under = pd.concat([df_class_0_under, df_class_1],ignore_index=True ,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4484, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_under.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  tweet\n",
       "label             \n",
       "0      2242   2242\n",
       "1      2242   2242"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_under.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_under['tweet']\n",
    "Y = df_under['label']\n",
    "Y_org = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@user i cannot imagine being in the shoes of local law-enforcement and first responders.#orlando  prayers to all.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "max_features = 10000\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', split=' ', lower=True, char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(X.values)\n",
    "X = tokenizer.texts_to_sequences(X.values)\n",
    "\n",
    "# add padding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X, maxlen=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' @user lmfao pathetic #soit   #growup #funny #noonethere #iknowwhoitis ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98±ð\\x9f\\x98±ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98±ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82â\\x80¦'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['tweet'], key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'sklearn.__check_build._check_build'\n___________________________________________________________________________\nContents of C:\\Users\\hp\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\__check_build:\nsetup.py                  _check_build.cp36-win32.pyd__init__.py\n__pycache__\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it: run `python setup.py install` or\n`make` in the source directory.\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\__check_build\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_check_build\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_build\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.__check_build._check_build'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c407846c304a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;31m# process, as it may not be compiled yet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[1;31m# avoid flakes unused variable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\__check_build\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_check_build\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_build\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mraise_build_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\__check_build\\__init__.py\u001b[0m in \u001b[0;36mraise_build_error\u001b[1;34m(e)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbuild\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpackage\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0musing\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpython\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[0minstall\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0;31m`\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msource\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m %s\"\"\" % (e, local_dir, ''.join(dir_content).strip(), msg))\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'sklearn.__check_build._check_build'\n___________________________________________________________________________\nContents of C:\\Users\\hp\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\__check_build:\nsetup.py                  _check_build.cp36-win32.pyd__init__.py\n__pycache__\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it: run `python setup.py install` or\n`make` in the source directory.\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 95)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "plt.xlabel(\"No. of components\")\n",
    "plt.ylabel(\"cummulative explained Variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fa5c46ed6237>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m95\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(LSTM(units=40, activation='relu',return_sequences= True, input_shape=(None, 95)))\n",
    "classifier.add(Dropout(rate=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1333a29c2610>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "classifier.add(LSTM(units=20, return_sequences= True,activation='relu'))\n",
    "classifier.add(Dropout(rate=0.2))\n",
    "classifier.add(LSTM(units=20,activation='relu'))\n",
    "classifier.add(Dropout(rate=0.2))\n",
    "classifier.add(Dense(units = 2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='rmsprop',metrics=['accuracy'],loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y_e = OneHotEncoder()\n",
    "Y_train_org = Y_train\n",
    "Y_test_org = Y_test\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "Y_test = Y_test.reshape(-1, 1)\n",
    "y_e.fit(Y_train)\n",
    "Y_train = y_e.transform(Y_train)\n",
    "Y_test = y_e.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3587 samples, validate on 897 samples\n",
      "Epoch 1/200\n",
      "3587/3587 [==============================] - 24s 7ms/step - loss: 0.6917 - acc: 0.5776 - val_loss: 0.6888 - val_acc: 0.6795\n",
      "Epoch 2/200\n",
      "3587/3587 [==============================] - 2s 442us/step - loss: 0.6804 - acc: 0.6560 - val_loss: 0.6782 - val_acc: 0.6789\n",
      "Epoch 3/200\n",
      "3587/3587 [==============================] - 2s 438us/step - loss: 0.6643 - acc: 0.6636 - val_loss: 0.6715 - val_acc: 0.6756\n",
      "Epoch 4/200\n",
      "3587/3587 [==============================] - 2s 436us/step - loss: 0.6520 - acc: 0.6623 - val_loss: 0.6586 - val_acc: 0.6812\n",
      "Epoch 5/200\n",
      "3587/3587 [==============================] - 2s 447us/step - loss: 0.6409 - acc: 0.6715 - val_loss: 0.6449 - val_acc: 0.6828\n",
      "Epoch 6/200\n",
      "3587/3587 [==============================] - 2s 431us/step - loss: 0.6356 - acc: 0.6769 - val_loss: 0.6386 - val_acc: 0.6834\n",
      "Epoch 7/200\n",
      "3587/3587 [==============================] - 2s 447us/step - loss: 0.6290 - acc: 0.6865 - val_loss: 0.6317 - val_acc: 0.6834\n",
      "Epoch 8/200\n",
      "3587/3587 [==============================] - 2s 441us/step - loss: 0.6233 - acc: 0.6805 - val_loss: 0.6268 - val_acc: 0.6867\n",
      "Epoch 9/200\n",
      "3587/3587 [==============================] - 2s 442us/step - loss: 0.6186 - acc: 0.6850 - val_loss: 0.6234 - val_acc: 0.6817\n",
      "Epoch 10/200\n",
      "3587/3587 [==============================] - 2s 446us/step - loss: 0.6146 - acc: 0.6879 - val_loss: 0.6193 - val_acc: 0.6784\n",
      "Epoch 11/200\n",
      "3587/3587 [==============================] - 2s 444us/step - loss: 0.6176 - acc: 0.6886 - val_loss: 0.6177 - val_acc: 0.6867\n",
      "Epoch 12/200\n",
      "3587/3587 [==============================] - 2s 439us/step - loss: 0.6067 - acc: 0.6931 - val_loss: 0.6176 - val_acc: 0.6851\n",
      "Epoch 13/200\n",
      "3587/3587 [==============================] - 2s 437us/step - loss: 0.6068 - acc: 0.6967 - val_loss: 0.6182 - val_acc: 0.6828\n",
      "Epoch 14/200\n",
      "3587/3587 [==============================] - 2s 456us/step - loss: 0.6056 - acc: 0.6939 - val_loss: 0.6224 - val_acc: 0.6795\n",
      "Epoch 15/200\n",
      "3587/3587 [==============================] - 2s 444us/step - loss: 0.6017 - acc: 0.6978 - val_loss: 0.6167 - val_acc: 0.6800\n",
      "Epoch 16/200\n",
      "3587/3587 [==============================] - 2s 458us/step - loss: 0.6016 - acc: 0.7016 - val_loss: 0.6195 - val_acc: 0.6834\n",
      "Epoch 17/200\n",
      "3587/3587 [==============================] - 2s 538us/step - loss: 0.5999 - acc: 0.6993 - val_loss: 0.6158 - val_acc: 0.6867\n",
      "Epoch 18/200\n",
      "3587/3587 [==============================] - 2s 479us/step - loss: 0.6012 - acc: 0.7039 - val_loss: 0.6173 - val_acc: 0.6851\n",
      "Epoch 19/200\n",
      "3587/3587 [==============================] - 2s 445us/step - loss: 0.5908 - acc: 0.7074 - val_loss: 0.6215 - val_acc: 0.6973\n",
      "Epoch 20/200\n",
      "3587/3587 [==============================] - 2s 469us/step - loss: 0.5956 - acc: 0.7089 - val_loss: 0.6182 - val_acc: 0.6990\n",
      "Epoch 21/200\n",
      "3587/3587 [==============================] - 2s 438us/step - loss: 0.5910 - acc: 0.7169 - val_loss: 0.6191 - val_acc: 0.6951\n",
      "Epoch 22/200\n",
      "3587/3587 [==============================] - 2s 457us/step - loss: 0.5860 - acc: 0.7116 - val_loss: 0.6159 - val_acc: 0.6945\n",
      "Epoch 23/200\n",
      "3587/3587 [==============================] - 2s 451us/step - loss: 0.5840 - acc: 0.7127 - val_loss: 0.6207 - val_acc: 0.7012\n",
      "Epoch 24/200\n",
      "3587/3587 [==============================] - 2s 452us/step - loss: 0.5819 - acc: 0.7117 - val_loss: 0.6166 - val_acc: 0.6918\n",
      "Epoch 25/200\n",
      "3587/3587 [==============================] - 2s 464us/step - loss: 0.5817 - acc: 0.7130 - val_loss: 0.6165 - val_acc: 0.6923\n",
      "Epoch 26/200\n",
      "3587/3587 [==============================] - 2s 464us/step - loss: 0.5817 - acc: 0.7134 - val_loss: 0.6156 - val_acc: 0.7023\n",
      "Epoch 27/200\n",
      "3587/3587 [==============================] - 2s 484us/step - loss: 0.5822 - acc: 0.7184 - val_loss: 0.6165 - val_acc: 0.6996\n",
      "Epoch 28/200\n",
      "3587/3587 [==============================] - 2s 451us/step - loss: 0.5749 - acc: 0.7215 - val_loss: 0.6221 - val_acc: 0.6973\n",
      "Epoch 29/200\n",
      "3587/3587 [==============================] - 2s 456us/step - loss: 0.5783 - acc: 0.7195 - val_loss: 0.6221 - val_acc: 0.7012\n",
      "Epoch 30/200\n",
      "3587/3587 [==============================] - 2s 456us/step - loss: 0.5790 - acc: 0.7198 - val_loss: 0.6150 - val_acc: 0.6968\n",
      "Epoch 31/200\n",
      "3587/3587 [==============================] - 2s 464us/step - loss: 0.5692 - acc: 0.7285 - val_loss: 0.6282 - val_acc: 0.7068\n",
      "Epoch 32/200\n",
      "3587/3587 [==============================] - 2s 436us/step - loss: 0.5698 - acc: 0.7283 - val_loss: 0.6292 - val_acc: 0.6951\n",
      "Epoch 33/200\n",
      "3587/3587 [==============================] - 2s 553us/step - loss: 0.5679 - acc: 0.7314 - val_loss: 0.6225 - val_acc: 0.6990\n",
      "Epoch 34/200\n",
      "3587/3587 [==============================] - 2s 451us/step - loss: 0.5652 - acc: 0.7338 - val_loss: 0.6261 - val_acc: 0.6957\n",
      "Epoch 35/200\n",
      "3587/3587 [==============================] - 2s 445us/step - loss: 0.5711 - acc: 0.7290 - val_loss: 0.6321 - val_acc: 0.6951\n",
      "Epoch 36/200\n",
      "3587/3587 [==============================] - 2s 440us/step - loss: 0.5592 - acc: 0.7359 - val_loss: 0.6394 - val_acc: 0.6934\n",
      "Epoch 37/200\n",
      "3587/3587 [==============================] - 2s 436us/step - loss: 0.5611 - acc: 0.7349 - val_loss: 0.6300 - val_acc: 0.6996\n",
      "Epoch 38/200\n",
      "3587/3587 [==============================] - 3s 782us/step - loss: 0.5655 - acc: 0.7282 - val_loss: 0.6399 - val_acc: 0.6940\n",
      "Epoch 39/200\n",
      "3587/3587 [==============================] - 2s 527us/step - loss: 0.5598 - acc: 0.7352 - val_loss: 0.6502 - val_acc: 0.6940\n",
      "Epoch 40/200\n",
      "3587/3587 [==============================] - 2s 473us/step - loss: 0.5618 - acc: 0.7359 - val_loss: 0.6489 - val_acc: 0.6884\n",
      "Epoch 41/200\n",
      "3587/3587 [==============================] - 2s 462us/step - loss: 0.5603 - acc: 0.7308 - val_loss: 0.6491 - val_acc: 0.6996\n",
      "Epoch 42/200\n",
      "3587/3587 [==============================] - 2s 526us/step - loss: 0.5679 - acc: 0.7336 - val_loss: 0.6477 - val_acc: 0.6945\n",
      "Epoch 43/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.5548 - acc: 0.7345 - val_loss: 0.6378 - val_acc: 0.6990\n",
      "Epoch 44/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.5574 - acc: 0.7357 - val_loss: 0.6454 - val_acc: 0.6990\n",
      "Epoch 45/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.5541 - acc: 0.7375 - val_loss: 0.6424 - val_acc: 0.6996\n",
      "Epoch 46/200\n",
      "3587/3587 [==============================] - 2s 475us/step - loss: 0.5589 - acc: 0.7331 - val_loss: 0.6434 - val_acc: 0.7001\n",
      "Epoch 47/200\n",
      "3587/3587 [==============================] - 2s 476us/step - loss: 0.5505 - acc: 0.7384 - val_loss: 0.6560 - val_acc: 0.6929\n",
      "Epoch 48/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.5487 - acc: 0.7481 - val_loss: 0.6593 - val_acc: 0.7001\n",
      "Epoch 49/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.5554 - acc: 0.7388 - val_loss: 0.6502 - val_acc: 0.6979\n",
      "Epoch 50/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.5482 - acc: 0.7392 - val_loss: 0.6521 - val_acc: 0.6918\n",
      "Epoch 51/200\n",
      "3587/3587 [==============================] - 2s 470us/step - loss: 0.5463 - acc: 0.7423 - val_loss: 0.6550 - val_acc: 0.6968\n",
      "Epoch 52/200\n",
      "3587/3587 [==============================] - 2s 479us/step - loss: 0.5514 - acc: 0.7425 - val_loss: 0.6543 - val_acc: 0.6973\n",
      "Epoch 53/200\n",
      "3587/3587 [==============================] - 2s 480us/step - loss: 0.5400 - acc: 0.7442 - val_loss: 0.6633 - val_acc: 0.6918\n",
      "Epoch 54/200\n",
      "3587/3587 [==============================] - 2s 467us/step - loss: 0.5412 - acc: 0.7467 - val_loss: 0.6576 - val_acc: 0.6934\n",
      "Epoch 55/200\n",
      "3587/3587 [==============================] - 2s 480us/step - loss: 0.5402 - acc: 0.7445 - val_loss: 0.6619 - val_acc: 0.6945\n",
      "Epoch 56/200\n",
      "3587/3587 [==============================] - 2s 473us/step - loss: 0.5391 - acc: 0.7477 - val_loss: 0.6602 - val_acc: 0.6884\n",
      "Epoch 57/200\n",
      "3587/3587 [==============================] - 2s 468us/step - loss: 0.5448 - acc: 0.7425 - val_loss: 0.6553 - val_acc: 0.6923\n",
      "Epoch 58/200\n",
      "3587/3587 [==============================] - 2s 469us/step - loss: 0.5358 - acc: 0.7513 - val_loss: 0.6714 - val_acc: 0.6934\n",
      "Epoch 59/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.5507 - acc: 0.7417 - val_loss: 0.6659 - val_acc: 0.6951\n",
      "Epoch 60/200\n",
      "3587/3587 [==============================] - 2s 465us/step - loss: 0.5392 - acc: 0.7490 - val_loss: 0.6851 - val_acc: 0.6996\n",
      "Epoch 61/200\n",
      "3587/3587 [==============================] - 2s 449us/step - loss: 0.5375 - acc: 0.7529 - val_loss: 0.6733 - val_acc: 0.6951\n",
      "Epoch 62/200\n",
      "3587/3587 [==============================] - 2s 449us/step - loss: 0.5321 - acc: 0.7554 - val_loss: 0.6625 - val_acc: 0.6962\n",
      "Epoch 63/200\n",
      "3587/3587 [==============================] - 2s 445us/step - loss: 0.5372 - acc: 0.7423 - val_loss: 0.6760 - val_acc: 0.6979\n",
      "Epoch 64/200\n",
      "3587/3587 [==============================] - 2s 446us/step - loss: 0.5324 - acc: 0.7545 - val_loss: 0.6688 - val_acc: 0.6912\n",
      "Epoch 65/200\n",
      "3587/3587 [==============================] - 2s 456us/step - loss: 0.5300 - acc: 0.7485 - val_loss: 0.6790 - val_acc: 0.6973\n",
      "Epoch 66/200\n",
      "3587/3587 [==============================] - 2s 466us/step - loss: 0.5321 - acc: 0.7543 - val_loss: 0.6615 - val_acc: 0.7035\n",
      "Epoch 67/200\n",
      "3587/3587 [==============================] - 2s 502us/step - loss: 0.5294 - acc: 0.7508 - val_loss: 0.6744 - val_acc: 0.7057\n",
      "Epoch 68/200\n",
      "3587/3587 [==============================] - 2s 490us/step - loss: 0.5314 - acc: 0.7551 - val_loss: 0.6615 - val_acc: 0.7029\n",
      "Epoch 69/200\n",
      "3587/3587 [==============================] - 2s 450us/step - loss: 0.5278 - acc: 0.7519 - val_loss: 0.6709 - val_acc: 0.7068\n",
      "Epoch 70/200\n",
      "3587/3587 [==============================] - 2s 458us/step - loss: 0.5341 - acc: 0.7524 - val_loss: 0.6704 - val_acc: 0.7079\n",
      "Epoch 71/200\n",
      "3587/3587 [==============================] - 2s 458us/step - loss: 0.5288 - acc: 0.7570 - val_loss: 0.6619 - val_acc: 0.7079\n",
      "Epoch 72/200\n",
      "3587/3587 [==============================] - 2s 460us/step - loss: 0.5189 - acc: 0.7630 - val_loss: 0.6566 - val_acc: 0.7101\n",
      "Epoch 73/200\n",
      "3587/3587 [==============================] - 2s 453us/step - loss: 0.5275 - acc: 0.7537 - val_loss: 0.6542 - val_acc: 0.7029\n",
      "Epoch 74/200\n",
      "3587/3587 [==============================] - 2s 601us/step - loss: 0.5215 - acc: 0.7579 - val_loss: 0.6565 - val_acc: 0.7057\n",
      "Epoch 75/200\n",
      "3587/3587 [==============================] - 2s 615us/step - loss: 0.5221 - acc: 0.7572 - val_loss: 0.6694 - val_acc: 0.7135\n",
      "Epoch 76/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.5195 - acc: 0.7703 - val_loss: 0.6740 - val_acc: 0.7124\n",
      "Epoch 77/200\n",
      "3587/3587 [==============================] - 2s 476us/step - loss: 0.5188 - acc: 0.7611 - val_loss: 0.6723 - val_acc: 0.7113\n",
      "Epoch 78/200\n",
      "3587/3587 [==============================] - 2s 473us/step - loss: 0.5278 - acc: 0.7580 - val_loss: 0.6604 - val_acc: 0.7079\n",
      "Epoch 79/200\n",
      "3587/3587 [==============================] - 2s 473us/step - loss: 0.5241 - acc: 0.7554 - val_loss: 0.6716 - val_acc: 0.7146\n",
      "Epoch 80/200\n",
      "3587/3587 [==============================] - 2s 512us/step - loss: 0.5108 - acc: 0.7693 - val_loss: 0.6554 - val_acc: 0.7168\n",
      "Epoch 81/200\n",
      "3587/3587 [==============================] - 2s 505us/step - loss: 0.5130 - acc: 0.7683 - val_loss: 0.6715 - val_acc: 0.7185\n",
      "Epoch 82/200\n",
      "3587/3587 [==============================] - 2s 451us/step - loss: 0.5195 - acc: 0.7694 - val_loss: 0.6600 - val_acc: 0.7157\n",
      "Epoch 83/200\n",
      "3587/3587 [==============================] - 2s 421us/step - loss: 0.5125 - acc: 0.7678 - val_loss: 0.6748 - val_acc: 0.7096\n",
      "Epoch 84/200\n",
      "3587/3587 [==============================] - 1s 398us/step - loss: 0.5143 - acc: 0.7651 - val_loss: 0.6606 - val_acc: 0.7168\n",
      "Epoch 85/200\n",
      "3587/3587 [==============================] - 1s 415us/step - loss: 0.5133 - acc: 0.7676 - val_loss: 0.6684 - val_acc: 0.7168\n",
      "Epoch 86/200\n",
      "3587/3587 [==============================] - 2s 484us/step - loss: 0.5161 - acc: 0.7602 - val_loss: 0.6747 - val_acc: 0.7113\n",
      "Epoch 87/200\n",
      "3587/3587 [==============================] - 2s 496us/step - loss: 0.5046 - acc: 0.7699 - val_loss: 0.6989 - val_acc: 0.7101\n",
      "Epoch 88/200\n",
      "3587/3587 [==============================] - 2s 475us/step - loss: 0.5103 - acc: 0.7724 - val_loss: 0.6696 - val_acc: 0.7179\n",
      "Epoch 89/200\n",
      "3587/3587 [==============================] - 2s 461us/step - loss: 0.5019 - acc: 0.7671 - val_loss: 0.6938 - val_acc: 0.7196\n",
      "Epoch 90/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.5061 - acc: 0.7669 - val_loss: 0.6690 - val_acc: 0.7179\n",
      "Epoch 91/200\n",
      "3587/3587 [==============================] - 2s 465us/step - loss: 0.5134 - acc: 0.7647 - val_loss: 0.6710 - val_acc: 0.7157\n",
      "Epoch 92/200\n",
      "3587/3587 [==============================] - 2s 462us/step - loss: 0.5049 - acc: 0.7714 - val_loss: 0.6907 - val_acc: 0.7191\n",
      "Epoch 93/200\n",
      "3587/3587 [==============================] - 2s 469us/step - loss: 0.5057 - acc: 0.7773 - val_loss: 0.6682 - val_acc: 0.7213\n",
      "Epoch 94/200\n",
      "3587/3587 [==============================] - 2s 464us/step - loss: 0.5005 - acc: 0.7704 - val_loss: 0.6677 - val_acc: 0.7146\n",
      "Epoch 95/200\n",
      "3587/3587 [==============================] - 2s 465us/step - loss: 0.5053 - acc: 0.7731 - val_loss: 0.6645 - val_acc: 0.7263\n",
      "Epoch 96/200\n",
      "3587/3587 [==============================] - 2s 467us/step - loss: 0.5018 - acc: 0.7676 - val_loss: 0.6556 - val_acc: 0.7241\n",
      "Epoch 97/200\n",
      "3587/3587 [==============================] - 2s 464us/step - loss: 0.5019 - acc: 0.7764 - val_loss: 0.6676 - val_acc: 0.7202\n",
      "Epoch 98/200\n",
      "3587/3587 [==============================] - 2s 462us/step - loss: 0.4876 - acc: 0.7785 - val_loss: 0.7014 - val_acc: 0.7241\n",
      "Epoch 99/200\n",
      "3587/3587 [==============================] - 2s 475us/step - loss: 0.4937 - acc: 0.7753 - val_loss: 0.6920 - val_acc: 0.7258\n",
      "Epoch 100/200\n",
      "3587/3587 [==============================] - 2s 474us/step - loss: 0.4936 - acc: 0.7793 - val_loss: 0.6749 - val_acc: 0.7246\n",
      "Epoch 101/200\n",
      "3587/3587 [==============================] - 2s 460us/step - loss: 0.4897 - acc: 0.7777 - val_loss: 0.6933 - val_acc: 0.7168\n",
      "Epoch 102/200\n",
      "3587/3587 [==============================] - 2s 466us/step - loss: 0.4985 - acc: 0.7679 - val_loss: 0.6930 - val_acc: 0.7191\n",
      "Epoch 103/200\n",
      "3587/3587 [==============================] - 2s 474us/step - loss: 0.4931 - acc: 0.7752 - val_loss: 0.6992 - val_acc: 0.7213\n",
      "Epoch 104/200\n",
      "3587/3587 [==============================] - 2s 470us/step - loss: 0.5006 - acc: 0.7685 - val_loss: 0.6727 - val_acc: 0.7224\n",
      "Epoch 105/200\n",
      "3587/3587 [==============================] - 2s 477us/step - loss: 0.5050 - acc: 0.7689 - val_loss: 0.6773 - val_acc: 0.7258\n",
      "Epoch 106/200\n",
      "3587/3587 [==============================] - 2s 474us/step - loss: 0.4860 - acc: 0.7810 - val_loss: 0.6780 - val_acc: 0.7274\n",
      "Epoch 107/200\n",
      "3587/3587 [==============================] - 2s 462us/step - loss: 0.4966 - acc: 0.7696 - val_loss: 0.6750 - val_acc: 0.7258\n",
      "Epoch 108/200\n",
      "3587/3587 [==============================] - 2s 468us/step - loss: 0.4990 - acc: 0.7729 - val_loss: 0.6852 - val_acc: 0.7152\n",
      "Epoch 109/200\n",
      "3587/3587 [==============================] - 2s 471us/step - loss: 0.4930 - acc: 0.7708 - val_loss: 0.6794 - val_acc: 0.7269\n",
      "Epoch 110/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.4900 - acc: 0.7853 - val_loss: 0.6843 - val_acc: 0.7280\n",
      "Epoch 111/200\n",
      "3587/3587 [==============================] - 2s 518us/step - loss: 0.4811 - acc: 0.7791 - val_loss: 0.6804 - val_acc: 0.7269\n",
      "Epoch 112/200\n",
      "3587/3587 [==============================] - 3s 719us/step - loss: 0.4925 - acc: 0.7759 - val_loss: 0.6848 - val_acc: 0.7252\n",
      "Epoch 113/200\n",
      "3587/3587 [==============================] - 2s 470us/step - loss: 0.4833 - acc: 0.7778 - val_loss: 0.6801 - val_acc: 0.7269\n",
      "Epoch 114/200\n",
      "3587/3587 [==============================] - 2s 531us/step - loss: 0.4887 - acc: 0.7754 - val_loss: 0.6815 - val_acc: 0.7224\n",
      "Epoch 115/200\n",
      "3587/3587 [==============================] - 2s 484us/step - loss: 0.4803 - acc: 0.7846 - val_loss: 0.6851 - val_acc: 0.7258\n",
      "Epoch 116/200\n",
      "3587/3587 [==============================] - 2s 488us/step - loss: 0.4852 - acc: 0.7806 - val_loss: 0.6817 - val_acc: 0.7241\n",
      "Epoch 117/200\n",
      "3587/3587 [==============================] - 2s 516us/step - loss: 0.4752 - acc: 0.7848 - val_loss: 0.7004 - val_acc: 0.7263\n",
      "Epoch 118/200\n",
      "3587/3587 [==============================] - 2s 556us/step - loss: 0.4785 - acc: 0.7827 - val_loss: 0.7049 - val_acc: 0.7269\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3587/3587 [==============================] - 2s 485us/step - loss: 0.4757 - acc: 0.7873 - val_loss: 0.6932 - val_acc: 0.7330\n",
      "Epoch 120/200\n",
      "3587/3587 [==============================] - 2s 424us/step - loss: 0.4699 - acc: 0.7898 - val_loss: 0.6862 - val_acc: 0.7319\n",
      "Epoch 121/200\n",
      "3587/3587 [==============================] - 2s 426us/step - loss: 0.4665 - acc: 0.7895 - val_loss: 0.7027 - val_acc: 0.7358\n",
      "Epoch 122/200\n",
      "3587/3587 [==============================] - 2s 431us/step - loss: 0.4750 - acc: 0.7817 - val_loss: 0.6907 - val_acc: 0.7358\n",
      "Epoch 123/200\n",
      "3587/3587 [==============================] - 2s 426us/step - loss: 0.4712 - acc: 0.7845 - val_loss: 0.7012 - val_acc: 0.7269\n",
      "Epoch 124/200\n",
      "3587/3587 [==============================] - 2s 424us/step - loss: 0.4785 - acc: 0.7825 - val_loss: 0.6800 - val_acc: 0.7263\n",
      "Epoch 125/200\n",
      "3587/3587 [==============================] - 1s 417us/step - loss: 0.4717 - acc: 0.7906 - val_loss: 0.6881 - val_acc: 0.7330\n",
      "Epoch 126/200\n",
      "3587/3587 [==============================] - 2s 429us/step - loss: 0.4740 - acc: 0.7849 - val_loss: 0.6808 - val_acc: 0.7324\n",
      "Epoch 127/200\n",
      "3587/3587 [==============================] - 2s 423us/step - loss: 0.4719 - acc: 0.7888 - val_loss: 0.6819 - val_acc: 0.7313\n",
      "Epoch 128/200\n",
      "3587/3587 [==============================] - 2s 426us/step - loss: 0.4794 - acc: 0.7858 - val_loss: 0.6915 - val_acc: 0.7358\n",
      "Epoch 129/200\n",
      "3587/3587 [==============================] - 2s 438us/step - loss: 0.4720 - acc: 0.7849 - val_loss: 0.6955 - val_acc: 0.7258\n",
      "Epoch 130/200\n",
      "3587/3587 [==============================] - 2s 428us/step - loss: 0.4711 - acc: 0.7866 - val_loss: 0.7043 - val_acc: 0.7274\n",
      "Epoch 131/200\n",
      "3587/3587 [==============================] - 2s 418us/step - loss: 0.4770 - acc: 0.7877 - val_loss: 0.6894 - val_acc: 0.7313\n",
      "Epoch 132/200\n",
      "3587/3587 [==============================] - 2s 425us/step - loss: 0.4683 - acc: 0.7856 - val_loss: 0.6954 - val_acc: 0.7330\n",
      "Epoch 133/200\n",
      "3587/3587 [==============================] - 2s 427us/step - loss: 0.4676 - acc: 0.7856 - val_loss: 0.6969 - val_acc: 0.7391\n",
      "Epoch 134/200\n",
      "3587/3587 [==============================] - 2s 424us/step - loss: 0.4642 - acc: 0.7869 - val_loss: 0.6906 - val_acc: 0.7419\n",
      "Epoch 135/200\n",
      "3587/3587 [==============================] - 2s 431us/step - loss: 0.4664 - acc: 0.7905 - val_loss: 0.6865 - val_acc: 0.7347\n",
      "Epoch 136/200\n",
      "3587/3587 [==============================] - 2s 433us/step - loss: 0.4719 - acc: 0.7906 - val_loss: 0.6818 - val_acc: 0.7391\n",
      "Epoch 137/200\n",
      "3587/3587 [==============================] - 2s 506us/step - loss: 0.4602 - acc: 0.7934 - val_loss: 0.7025 - val_acc: 0.7313\n",
      "Epoch 138/200\n",
      "3587/3587 [==============================] - 2s 491us/step - loss: 0.4645 - acc: 0.7983 - val_loss: 0.6829 - val_acc: 0.7358\n",
      "Epoch 139/200\n",
      "3587/3587 [==============================] - 2s 456us/step - loss: 0.4662 - acc: 0.7870 - val_loss: 0.7002 - val_acc: 0.7397\n",
      "Epoch 140/200\n",
      "3587/3587 [==============================] - 2s 455us/step - loss: 0.4647 - acc: 0.7915 - val_loss: 0.6966 - val_acc: 0.7341\n",
      "Epoch 141/200\n",
      "3587/3587 [==============================] - 2s 455us/step - loss: 0.4581 - acc: 0.7959 - val_loss: 0.6903 - val_acc: 0.7308\n",
      "Epoch 142/200\n",
      "3587/3587 [==============================] - 2s 473us/step - loss: 0.4636 - acc: 0.7990 - val_loss: 0.7093 - val_acc: 0.7363\n",
      "Epoch 143/200\n",
      "3587/3587 [==============================] - 2s 537us/step - loss: 0.4557 - acc: 0.7940 - val_loss: 0.7135 - val_acc: 0.7336\n",
      "Epoch 144/200\n",
      "3587/3587 [==============================] - 2s 471us/step - loss: 0.4535 - acc: 0.7998 - val_loss: 0.7029 - val_acc: 0.7347\n",
      "Epoch 145/200\n",
      "3587/3587 [==============================] - 2s 450us/step - loss: 0.4590 - acc: 0.7883 - val_loss: 0.7251 - val_acc: 0.7336\n",
      "Epoch 146/200\n",
      "3587/3587 [==============================] - 2s 457us/step - loss: 0.4629 - acc: 0.7950 - val_loss: 0.6986 - val_acc: 0.7402\n",
      "Epoch 147/200\n",
      "3587/3587 [==============================] - 2s 454us/step - loss: 0.4520 - acc: 0.7940 - val_loss: 0.7096 - val_acc: 0.7358\n",
      "Epoch 148/200\n",
      "3587/3587 [==============================] - 2s 455us/step - loss: 0.4513 - acc: 0.8036 - val_loss: 0.7259 - val_acc: 0.7447\n",
      "Epoch 149/200\n",
      "3587/3587 [==============================] - 2s 507us/step - loss: 0.4550 - acc: 0.7920 - val_loss: 0.6994 - val_acc: 0.7391\n",
      "Epoch 150/200\n",
      "3587/3587 [==============================] - 2s 639us/step - loss: 0.4513 - acc: 0.7897 - val_loss: 0.6980 - val_acc: 0.7408\n",
      "Epoch 151/200\n",
      "3587/3587 [==============================] - 2s 455us/step - loss: 0.4645 - acc: 0.7866 - val_loss: 0.6988 - val_acc: 0.7469\n",
      "Epoch 152/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.4524 - acc: 0.7947 - val_loss: 0.7101 - val_acc: 0.7480\n",
      "Epoch 153/200\n",
      "3587/3587 [==============================] - 2s 460us/step - loss: 0.4538 - acc: 0.7930 - val_loss: 0.7200 - val_acc: 0.7408\n",
      "Epoch 154/200\n",
      "3587/3587 [==============================] - 2s 489us/step - loss: 0.4556 - acc: 0.7936 - val_loss: 0.7024 - val_acc: 0.7447\n",
      "Epoch 155/200\n",
      "3587/3587 [==============================] - 2s 502us/step - loss: 0.4530 - acc: 0.7965 - val_loss: 0.7131 - val_acc: 0.7464\n",
      "Epoch 156/200\n",
      "3587/3587 [==============================] - 2s 492us/step - loss: 0.4520 - acc: 0.7923 - val_loss: 0.7298 - val_acc: 0.7497\n",
      "Epoch 157/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.4431 - acc: 0.7954 - val_loss: 0.7425 - val_acc: 0.7414\n",
      "Epoch 158/200\n",
      "3587/3587 [==============================] - 2s 459us/step - loss: 0.4395 - acc: 0.8022 - val_loss: 0.7504 - val_acc: 0.7441\n",
      "Epoch 159/200\n",
      "3587/3587 [==============================] - 2s 460us/step - loss: 0.4480 - acc: 0.7963 - val_loss: 0.7046 - val_acc: 0.7425\n",
      "Epoch 160/200\n",
      "3587/3587 [==============================] - 2s 465us/step - loss: 0.4551 - acc: 0.7934 - val_loss: 0.7169 - val_acc: 0.7369\n",
      "Epoch 161/200\n",
      "3587/3587 [==============================] - 2s 469us/step - loss: 0.4560 - acc: 0.7902 - val_loss: 0.7421 - val_acc: 0.7402\n",
      "Epoch 162/200\n",
      "3587/3587 [==============================] - 2s 458us/step - loss: 0.4376 - acc: 0.7951 - val_loss: 0.7381 - val_acc: 0.7480\n",
      "Epoch 163/200\n",
      "3587/3587 [==============================] - 2s 457us/step - loss: 0.4493 - acc: 0.8009 - val_loss: 0.7216 - val_acc: 0.7441\n",
      "Epoch 164/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.4476 - acc: 0.7975 - val_loss: 0.7075 - val_acc: 0.7492\n",
      "Epoch 165/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.4533 - acc: 0.7917 - val_loss: 0.7230 - val_acc: 0.7458\n",
      "Epoch 166/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.4408 - acc: 0.7972 - val_loss: 0.7321 - val_acc: 0.7469\n",
      "Epoch 167/200\n",
      "3587/3587 [==============================] - 2s 464us/step - loss: 0.4404 - acc: 0.8025 - val_loss: 0.7338 - val_acc: 0.7469\n",
      "Epoch 168/200\n",
      "3587/3587 [==============================] - 2s 462us/step - loss: 0.4426 - acc: 0.8026 - val_loss: 0.7075 - val_acc: 0.7497\n",
      "Epoch 169/200\n",
      "3587/3587 [==============================] - 2s 468us/step - loss: 0.4439 - acc: 0.7927 - val_loss: 0.7123 - val_acc: 0.7458\n",
      "Epoch 170/200\n",
      "3587/3587 [==============================] - 2s 462us/step - loss: 0.4387 - acc: 0.8054 - val_loss: 0.7222 - val_acc: 0.7547\n",
      "Epoch 171/200\n",
      "3587/3587 [==============================] - 2s 468us/step - loss: 0.4444 - acc: 0.8016 - val_loss: 0.7114 - val_acc: 0.7458\n",
      "Epoch 172/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.4370 - acc: 0.8055 - val_loss: 0.7308 - val_acc: 0.7492\n",
      "Epoch 173/200\n",
      "3587/3587 [==============================] - 2s 467us/step - loss: 0.4439 - acc: 0.7977 - val_loss: 0.7386 - val_acc: 0.7447\n",
      "Epoch 174/200\n",
      "3587/3587 [==============================] - 2s 471us/step - loss: 0.4424 - acc: 0.7970 - val_loss: 0.7175 - val_acc: 0.7469\n",
      "Epoch 175/200\n",
      "3587/3587 [==============================] - 2s 473us/step - loss: 0.4464 - acc: 0.8012 - val_loss: 0.7179 - val_acc: 0.7503\n",
      "Epoch 176/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.4384 - acc: 0.8036 - val_loss: 0.7137 - val_acc: 0.7480\n",
      "Epoch 177/200\n",
      "3587/3587 [==============================] - 2s 469us/step - loss: 0.4483 - acc: 0.7996 - val_loss: 0.7160 - val_acc: 0.7469\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3587/3587 [==============================] - 2s 435us/step - loss: 0.4254 - acc: 0.8107 - val_loss: 0.7202 - val_acc: 0.7514\n",
      "Epoch 179/200\n",
      "3587/3587 [==============================] - 2s 427us/step - loss: 0.4333 - acc: 0.8009 - val_loss: 0.7203 - val_acc: 0.7436\n",
      "Epoch 180/200\n",
      "3587/3587 [==============================] - 2s 477us/step - loss: 0.4348 - acc: 0.8022 - val_loss: 0.7256 - val_acc: 0.7458\n",
      "Epoch 181/200\n",
      "3587/3587 [==============================] - 2s 441us/step - loss: 0.4326 - acc: 0.8050 - val_loss: 0.7435 - val_acc: 0.7436\n",
      "Epoch 182/200\n",
      "3587/3587 [==============================] - 2s 448us/step - loss: 0.4353 - acc: 0.8090 - val_loss: 0.7137 - val_acc: 0.7492\n",
      "Epoch 183/200\n",
      "3587/3587 [==============================] - 2s 450us/step - loss: 0.4397 - acc: 0.8030 - val_loss: 0.7135 - val_acc: 0.7414\n",
      "Epoch 184/200\n",
      "3587/3587 [==============================] - 2s 434us/step - loss: 0.4374 - acc: 0.8015 - val_loss: 0.7265 - val_acc: 0.7453\n",
      "Epoch 185/200\n",
      "3587/3587 [==============================] - 2s 442us/step - loss: 0.4302 - acc: 0.8095 - val_loss: 0.7250 - val_acc: 0.7469\n",
      "Epoch 186/200\n",
      "3587/3587 [==============================] - 2s 442us/step - loss: 0.4362 - acc: 0.8068 - val_loss: 0.7171 - val_acc: 0.7458\n",
      "Epoch 187/200\n",
      "3587/3587 [==============================] - 2s 534us/step - loss: 0.4333 - acc: 0.7976 - val_loss: 0.7322 - val_acc: 0.7447\n",
      "Epoch 188/200\n",
      "3587/3587 [==============================] - 2s 568us/step - loss: 0.4280 - acc: 0.8081 - val_loss: 0.7366 - val_acc: 0.7475\n",
      "Epoch 189/200\n",
      "3587/3587 [==============================] - 2s 443us/step - loss: 0.4245 - acc: 0.8069 - val_loss: 0.7355 - val_acc: 0.7503\n",
      "Epoch 190/200\n",
      "3587/3587 [==============================] - 2s 439us/step - loss: 0.4263 - acc: 0.8053 - val_loss: 0.7523 - val_acc: 0.7447\n",
      "Epoch 191/200\n",
      "3587/3587 [==============================] - 2s 442us/step - loss: 0.4363 - acc: 0.8029 - val_loss: 0.7596 - val_acc: 0.7514\n",
      "Epoch 192/200\n",
      "3587/3587 [==============================] - 2s 441us/step - loss: 0.4399 - acc: 0.8058 - val_loss: 0.7222 - val_acc: 0.7425\n",
      "Epoch 193/200\n",
      "3587/3587 [==============================] - 2s 441us/step - loss: 0.4282 - acc: 0.8035 - val_loss: 0.7514 - val_acc: 0.7480\n",
      "Epoch 194/200\n",
      "3587/3587 [==============================] - 2s 444us/step - loss: 0.4273 - acc: 0.8081 - val_loss: 0.7412 - val_acc: 0.7492\n",
      "Epoch 195/200\n",
      "3587/3587 [==============================] - 2s 436us/step - loss: 0.4352 - acc: 0.8072 - val_loss: 0.7296 - val_acc: 0.7520\n",
      "Epoch 196/200\n",
      "3587/3587 [==============================] - 2s 446us/step - loss: 0.4207 - acc: 0.8097 - val_loss: 0.7612 - val_acc: 0.7458\n",
      "Epoch 197/200\n",
      "3587/3587 [==============================] - 2s 447us/step - loss: 0.4358 - acc: 0.8060 - val_loss: 0.7569 - val_acc: 0.7458\n",
      "Epoch 198/200\n",
      "3587/3587 [==============================] - 2s 446us/step - loss: 0.4304 - acc: 0.8040 - val_loss: 0.7226 - val_acc: 0.7536\n",
      "Epoch 199/200\n",
      "3587/3587 [==============================] - 2s 446us/step - loss: 0.4313 - acc: 0.8021 - val_loss: 0.7177 - val_acc: 0.7525\n",
      "Epoch 200/200\n",
      "3587/3587 [==============================] - 2s 446us/step - loss: 0.4328 - acc: 0.8076 - val_loss: 0.7202 - val_acc: 0.7536\n"
     ]
    }
   ],
   "source": [
    "X_train_lstm = np.reshape(X_train, (X_train.shape[0],1, X_train.shape[1]))\n",
    "X_test_lstm = np.reshape(X_test, (X_test.shape[0],1 ,X_test.shape[1]))\n",
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    checker = classifier.fit(X_train_lstm, Y_train, batch_size=32, epochs=200, validation_data = (X_test_lstm, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_test_label = classifier.predict(X_test_lstm)\n",
    "y_pred_test=np.argmax(Y_pred_test_label,axis =1)\n",
    "y_pred_test\n",
    "Y_pred_train_label = classifier.predict(X_train_lstm)\n",
    "y_pred_train = np.argmax(Y_pred_train_label,axis=1)\n",
    "Y_test_true = Y_test_org.astype(np.int)\n",
    "Y_train_true = Y_train_org.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.71      0.74       443\n",
      "          1       0.74      0.80      0.77       454\n",
      "\n",
      "avg / total       0.75      0.75      0.75       897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test_true,y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  \n",
      " [[1373  128]\n",
      " [ 426 1660]]\n",
      "\n",
      "Test:  \n",
      " [[315  93]\n",
      " [128 361]]\n"
     ]
    }
   ],
   "source": [
    "### for softmax function\n",
    "print(\"TRAIN:  \\n\",confusion_matrix(y_pred_train,Y_train_true))\n",
    "print(\"\\nTest:  \\n\",confusion_matrix(y_pred_test,Y_test_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.76      0.83      1799\n",
      "          1       0.80      0.93      0.86      1788\n",
      "\n",
      "avg / total       0.86      0.85      0.84      3587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train_true,y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.save(\"rnn_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model(\"rnn_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 40)          21760     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 40)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 20)          4880      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 20)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 20)                3280      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 29,962\n",
      "Trainable params: 29,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.11310695,  0.140307  , -0.3356271 , ..., -0.16759749,\n",
       "         -0.2781836 , -0.9401908 ],\n",
       "        [ 0.09020896, -0.22769712, -0.06153519, ...,  0.2232761 ,\n",
       "         -0.14894578,  0.55924535],\n",
       "        [ 0.16288431,  0.32513678, -0.04923659, ..., -0.51932377,\n",
       "          0.24815445, -0.01057171],\n",
       "        ...,\n",
       "        [-0.04493226,  0.12000737,  0.09115861, ...,  0.12171158,\n",
       "          0.07524186,  0.14667505],\n",
       "        [ 0.03513572, -0.04742594, -0.13730517, ...,  0.04065315,\n",
       "          0.04260868,  0.11560479],\n",
       "        [ 0.10513681, -0.07802351,  0.00621991, ...,  0.0745271 ,\n",
       "         -0.06921871,  0.05683559]], dtype=float32),\n",
       " array([[-0.06442357, -0.02605159, -0.04323145, ..., -0.04987266,\n",
       "          0.06833354, -0.14754143],\n",
       "        [-0.05491902,  0.09304494,  0.03452028, ..., -0.03596656,\n",
       "         -0.02188935, -0.08795702],\n",
       "        [-0.00672218, -0.0197031 , -0.10169733, ..., -0.02838954,\n",
       "         -0.02927258, -0.11133979],\n",
       "        ...,\n",
       "        [ 0.00604066,  0.01927673,  0.05301465, ...,  0.05123032,\n",
       "         -0.07815466,  0.01054049],\n",
       "        [-0.10036889, -0.03800025, -0.03497114, ..., -0.04158446,\n",
       "          0.11868522, -0.09212317],\n",
       "        [-0.01799201, -0.15533476, -0.02820942, ...,  0.0782618 ,\n",
       "         -0.05937856,  0.04444469]], dtype=float32),\n",
       " array([ 4.20623034e-01, -2.01468125e-01,  1.58369597e-02,  1.45036457e-02,\n",
       "        -4.54615429e-02, -2.29406402e-01,  7.27523044e-02, -1.40669256e-01,\n",
       "         1.49086133e-01,  4.46907848e-01,  9.28760245e-02, -2.02501237e-01,\n",
       "         1.57948077e-01, -2.76014894e-01,  1.13218971e-01,  3.13112140e-01,\n",
       "         6.22682869e-01,  2.23360270e-01,  4.29427326e-01, -8.41634646e-02,\n",
       "        -1.05229184e-01,  8.44428167e-02,  1.66654840e-01,  2.58649051e-01,\n",
       "         2.22636923e-01,  4.52513210e-02,  1.76872667e-02,  4.08268809e-01,\n",
       "        -1.83693305e-01,  1.77092887e-02,  1.22727357e-01, -7.47001097e-02,\n",
       "         1.11699728e-02,  8.10248926e-02, -3.31679344e-01,  2.62796432e-01,\n",
       "         9.26800519e-02,  1.82952121e-01, -2.73341350e-02,  1.48035690e-01,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        -5.51410496e-01, -1.15632391e+00, -2.84151196e-01, -5.34445584e-01,\n",
       "        -3.76857400e-01,  3.37132476e-02,  4.87302877e-02, -3.56226861e-01,\n",
       "        -4.85349864e-01, -3.08822900e-01, -4.22528058e-01, -7.16236457e-02,\n",
       "        -2.74454653e-01, -3.57588470e-01,  2.33579785e-01,  2.26970956e-01,\n",
       "        -4.90770549e-01, -1.22373201e-01, -4.11841244e-01, -4.47001666e-01,\n",
       "        -5.37956595e-01, -3.90740752e-01, -3.26634765e-01, -3.39296997e-01,\n",
       "        -5.40999353e-01, -3.50490883e-02, -4.47388649e-01, -3.05249572e-01,\n",
       "        -4.15806562e-01,  2.12655410e-01, -4.52137738e-01, -4.02470142e-01,\n",
       "        -3.42949003e-01, -2.72471488e-01, -3.45744938e-01, -1.02518296e+00,\n",
       "        -6.33296788e-01, -3.60647172e-01, -4.81465042e-01, -2.59548306e-01,\n",
       "        -5.26147112e-02, -4.52939458e-02,  2.35905088e-02, -3.81200425e-02,\n",
       "        -4.01212573e-02, -2.64013141e-01, -1.09112384e-02,  4.15720731e-01,\n",
       "        -1.95316285e-01,  3.68796699e-02,  3.53153735e-01, -3.51386160e-01,\n",
       "        -7.28761181e-02, -2.59419501e-01, -1.06260225e-01,  2.80846328e-01,\n",
       "         4.33320701e-01,  3.10938716e-01,  1.28015652e-01, -1.62171438e-01,\n",
       "        -1.00210784e-02,  3.55001748e-01,  2.08439127e-01,  5.82513697e-02,\n",
       "         3.60021234e-01, -4.63809408e-02,  2.59331107e-01, -6.52229413e-02,\n",
       "         9.21268538e-02,  1.01620273e-03,  1.01859637e-01,  9.93843824e-02,\n",
       "         9.53297392e-02,  2.09815770e-01, -5.36023736e-01,  4.74572815e-02,\n",
       "         3.68876517e-01,  2.42652185e-02,  1.50286406e-01,  1.90501794e-01],\n",
       "       dtype=float32),\n",
       " array([[ 1.0694517 ,  0.47651604,  0.9122786 , ...,  1.5697391 ,\n",
       "          1.4753311 ,  1.7700303 ],\n",
       "        [ 1.474063  ,  2.1041179 , -0.42140743, ...,  0.6526925 ,\n",
       "          0.44614664,  0.33262375],\n",
       "        [ 0.41222838,  0.28946212,  0.48911744, ...,  0.96431166,\n",
       "          0.7717971 ,  1.8378046 ],\n",
       "        ...,\n",
       "        [ 0.3425941 ,  0.28660914,  0.8028868 , ...,  0.7759061 ,\n",
       "          0.7749513 ,  1.5162458 ],\n",
       "        [ 0.45317852,  0.633058  ,  0.46525264, ...,  0.9492276 ,\n",
       "          0.9937012 ,  1.417984  ],\n",
       "        [-0.87419826,  0.99150157,  0.7165898 , ...,  1.4334797 ,\n",
       "          1.2816664 ,  0.8063989 ]], dtype=float32),\n",
       " array([[-0.05740755, -0.02741251,  0.13994667, ..., -0.06050767,\n",
       "          0.188733  , -0.14142932],\n",
       "        [ 0.0680439 , -0.01097375,  0.03211001, ..., -0.0138363 ,\n",
       "         -0.03413808,  0.01741688],\n",
       "        [ 0.1815692 , -0.15446973, -0.00512208, ..., -0.00627421,\n",
       "          0.10506888,  0.10432336],\n",
       "        ...,\n",
       "        [ 0.00486669, -0.2766275 , -0.04290835, ..., -0.07050695,\n",
       "          0.155536  ,  0.01816445],\n",
       "        [ 0.08928416, -0.10194883,  0.02102962, ..., -0.16823144,\n",
       "          0.06083879, -0.05720262],\n",
       "        [ 0.1739068 ,  0.03076227,  0.03622082, ...,  0.05897458,\n",
       "          0.10928333, -0.05107159]], dtype=float32),\n",
       " array([ 0.5454316 ,  0.28554958,  0.40638417,  1.177794  ,  0.23251544,\n",
       "         1.0706966 ,  0.5881349 ,  0.73756975,  1.0493253 ,  0.4289327 ,\n",
       "         0.6997997 ,  0.58141404,  0.5374424 ,  0.48935813,  1.1925961 ,\n",
       "         0.42582285,  1.3645506 ,  0.6367456 ,  0.35805723, -0.4125383 ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.34778467, -0.676124  , -0.01862124,  0.4654709 , -0.26900452,\n",
       "         0.03851245, -0.19524665,  0.29707468, -0.20187667, -0.06308125,\n",
       "         0.00202777,  0.01002238, -0.04576936, -0.5982975 , -0.01908175,\n",
       "        -0.06456482,  0.19258231, -0.32061496, -0.2006031 ,  0.0187201 ,\n",
       "         0.5954352 ,  0.16017967,  1.0628523 ,  0.6776288 ,  0.10892668,\n",
       "         0.9761598 , -0.1923025 ,  0.6239593 ,  1.0110875 ,  0.49295187,\n",
       "         0.7291411 ,  1.0959413 ,  0.3695808 ,  0.98426276,  1.3770982 ,\n",
       "         0.36649373,  1.2823684 ,  0.65374655,  0.3766938 , -0.37756357],\n",
       "       dtype=float32),\n",
       " array([[ 0.3713969 ,  0.23629764, -0.15179022, ..., -0.0624232 ,\n",
       "          0.19957738,  0.29300308],\n",
       "        [ 1.6277709 ,  1.8191054 ,  0.4799076 , ...,  0.33747882,\n",
       "          2.078467  ,  1.858322  ],\n",
       "        [ 0.7855366 ,  0.7050484 ,  0.7507111 , ..., -0.16205837,\n",
       "          0.7028538 ,  0.64769375],\n",
       "        ...,\n",
       "        [ 0.53515416,  0.5900962 ,  1.2902826 , ..., -0.03843456,\n",
       "          0.6997473 ,  0.97617   ],\n",
       "        [ 0.17112708,  0.4504235 ,  0.89944005, ..., -0.21613576,\n",
       "          0.73396635,  0.73148453],\n",
       "        [ 0.9454183 ,  1.2813138 ,  0.79385513, ...,  0.06926814,\n",
       "          1.4261553 ,  1.2561504 ]], dtype=float32),\n",
       " array([[ 0.00311221,  0.05533476,  0.01300392, ..., -0.10385159,\n",
       "         -0.0663966 , -0.08943792],\n",
       "        [-0.01974344, -0.06016648,  0.11880335, ...,  0.10701304,\n",
       "         -0.01709785, -0.16351628],\n",
       "        [-0.02195874,  0.04741657, -0.09600414, ...,  0.02926966,\n",
       "         -0.01123678, -0.01416242],\n",
       "        ...,\n",
       "        [-0.11510451,  0.03390775, -0.00512377, ...,  0.25055382,\n",
       "         -0.15562992,  0.03975494],\n",
       "        [ 0.04651685, -0.07025083, -0.08457635, ..., -0.07941562,\n",
       "         -0.01911857, -0.1544117 ],\n",
       "        [ 0.13249977, -0.04983875,  0.08142719, ...,  0.26016104,\n",
       "          0.25623247,  0.03508294]], dtype=float32),\n",
       " array([ 0.74338126,  0.74525017,  0.701561  , -0.16391452,  0.47345337,\n",
       "         0.83358043,  1.0156054 ,  0.51100755,  0.7006597 ,  0.7449891 ,\n",
       "         0.8086828 ,  0.84018064,  0.34716743, -0.0696771 ,  0.4546222 ,\n",
       "         0.47205663,  0.63089645,  0.6464181 ,  0.76245433,  0.7568718 ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.3594939 ,  0.26512507, -0.16785727, -0.11959606,  0.25758225,\n",
       "         0.32100034,  0.2600735 , -0.10818887,  0.27112222, -0.14595649,\n",
       "         0.1811912 ,  0.28781155, -0.0981876 , -0.00203791, -0.09274025,\n",
       "         0.24824205,  0.38090527, -0.12070718,  0.19371544,  0.22652812,\n",
       "         0.7317925 ,  0.8783711 , -0.29322293,  0.6715073 ,  0.52921224,\n",
       "         0.84553033,  0.9994223 , -0.2298165 ,  0.80972767, -0.3777241 ,\n",
       "         0.785841  ,  0.80919856,  0.34604874,  0.69066143, -0.3989177 ,\n",
       "         0.5539688 ,  0.67827886, -0.32303438,  0.8211932 ,  0.75055313],\n",
       "       dtype=float32),\n",
       " array([[-0.31718442,  0.30250344],\n",
       "        [-0.4439642 ,  0.41014504],\n",
       "        [ 0.5382883 , -0.54598624],\n",
       "        [ 0.5324978 , -0.5011728 ],\n",
       "        [ 0.5204823 , -0.540388  ],\n",
       "        [-0.43841475,  0.39302057],\n",
       "        [-0.469857  ,  0.51162785],\n",
       "        [ 0.46131444, -0.43413433],\n",
       "        [-0.41136312,  0.39434382],\n",
       "        [ 0.609403  , -0.67656255],\n",
       "        [-0.58317757,  0.46610522],\n",
       "        [-0.4299733 ,  0.4112976 ],\n",
       "        [ 0.46788213, -0.48610708],\n",
       "        [ 0.29167026, -0.3062448 ],\n",
       "        [ 0.38941574, -0.36299443],\n",
       "        [-0.53044   ,  0.54780114],\n",
       "        [ 0.18571456, -0.1774876 ],\n",
       "        [ 0.46554726, -0.4738565 ],\n",
       "        [-0.6030028 ,  0.65128   ],\n",
       "        [-0.34616604,  0.4812819 ]], dtype=float32),\n",
       " array([-0.27757105,  0.2794262 ], dtype=float32)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.optimizers.RMSprop at 0x7f0e72531d30>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9880548 , 0.0117742 ],\n",
       "       [0.34999084, 0.64975554],\n",
       "       [0.25240347, 0.7473473 ],\n",
       "       ...,\n",
       "       [0.30437657, 0.6952641 ],\n",
       "       [0.18941046, 0.81051505],\n",
       "       [0.2679293 , 0.73183596]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict(X_train_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
