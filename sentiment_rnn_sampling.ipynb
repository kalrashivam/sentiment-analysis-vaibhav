{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "import keras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_E6oV3lV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29720</td>\n",
       "      <td>29720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  tweet\n",
       "label              \n",
       "0      29720  29720\n",
       "1       2242   2242"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_class_0, count_class_1 = df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_class_0 =  df.query('label==0')\n",
    "df_class_1 =  df.query('label==1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_under = pd.concat([df_class_0_under, df_class_1],ignore_index=True ,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4484, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_under.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  tweet\n",
       "label             \n",
       "0      2242   2242\n",
       "1      2242   2242"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_under.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_under['tweet']\n",
    "Y = df_under['label']\n",
    "Y_org = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lights on and now lights off.. thank you @user for the last 2.5 weeks!! #vividsydney   '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "max_features = 10000\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', split=' ', lower=True, char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(X.values)\n",
    "X = tokenizer.texts_to_sequences(X.values)\n",
    "\n",
    "# add padding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X, maxlen=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' @user lmfao pathetic #soit   #growup #funny #noonethere #iknowwhoitis ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98±ð\\x9f\\x98±ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98±ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82â\\x80¦'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['tweet'], key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.conda/envs/my_root/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 95)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSQVCRERSUoEQEZGU/h+lLNr88w88SQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "plt.xlabel(\"No. of components\")\n",
    "plt.ylabel(\"cummulative explained Variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(LSTM(units=40, activation='relu',return_sequences= True, input_shape=(None, 95)))\n",
    "classifier.add(Dropout(rate=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(LSTM(units=20, return_sequences= True,activation='relu'))\n",
    "classifier.add(Dropout(rate=0.2))\n",
    "classifier.add(LSTM(units=20,activation='relu'))\n",
    "classifier.add(Dropout(rate=0.2))\n",
    "classifier.add(Dense(units = 2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='rmsprop',metrics=['accuracy'],loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.conda/envs/my_root/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"\n",
      "/home/shivam/.conda/envs/my_root/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y_e = OneHotEncoder()\n",
    "Y_train_org = Y_train\n",
    "Y_test_org = Y_test\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "Y_test = Y_test.reshape(-1, 1)\n",
    "y_e.fit(Y_train)\n",
    "Y_train = y_e.transform(Y_train)\n",
    "Y_test = y_e.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3587 samples, validate on 897 samples\n",
      "Epoch 1/200\n",
      "3587/3587 [==============================] - 4s 1ms/step - loss: 0.6924 - acc: 0.5290 - val_loss: 0.6900 - val_acc: 0.6750\n",
      "Epoch 2/200\n",
      "3587/3587 [==============================] - 2s 445us/step - loss: 0.6826 - acc: 0.6620 - val_loss: 0.6707 - val_acc: 0.7007\n",
      "Epoch 3/200\n",
      "3587/3587 [==============================] - 2s 441us/step - loss: 0.6594 - acc: 0.6862 - val_loss: 0.6472 - val_acc: 0.7051\n",
      "Epoch 4/200\n",
      "3587/3587 [==============================] - 2s 423us/step - loss: 0.6363 - acc: 0.6905 - val_loss: 0.6374 - val_acc: 0.7090\n",
      "Epoch 5/200\n",
      "3587/3587 [==============================] - 2s 436us/step - loss: 0.6332 - acc: 0.6908 - val_loss: 0.6375 - val_acc: 0.7135\n",
      "Epoch 6/200\n",
      "3587/3587 [==============================] - 2s 429us/step - loss: 0.6197 - acc: 0.6977 - val_loss: 0.6356 - val_acc: 0.7124\n",
      "Epoch 7/200\n",
      "3587/3587 [==============================] - 1s 382us/step - loss: 0.6148 - acc: 0.6982 - val_loss: 0.6338 - val_acc: 0.7179\n",
      "Epoch 8/200\n",
      "3587/3587 [==============================] - 1s 381us/step - loss: 0.6159 - acc: 0.7011 - val_loss: 0.6284 - val_acc: 0.7140\n",
      "Epoch 9/200\n",
      "3587/3587 [==============================] - 1s 381us/step - loss: 0.6100 - acc: 0.6964 - val_loss: 0.6216 - val_acc: 0.7174\n",
      "Epoch 10/200\n",
      "3587/3587 [==============================] - 2s 483us/step - loss: 0.6059 - acc: 0.7052 - val_loss: 0.6136 - val_acc: 0.7213\n",
      "Epoch 11/200\n",
      "3587/3587 [==============================] - 1s 400us/step - loss: 0.6010 - acc: 0.7087 - val_loss: 0.6088 - val_acc: 0.7191\n",
      "Epoch 12/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.5992 - acc: 0.7106 - val_loss: 0.6025 - val_acc: 0.7179\n",
      "Epoch 13/200\n",
      "3587/3587 [==============================] - 2s 433us/step - loss: 0.5945 - acc: 0.7119 - val_loss: 0.6006 - val_acc: 0.7174\n",
      "Epoch 14/200\n",
      "3587/3587 [==============================] - 1s 400us/step - loss: 0.5932 - acc: 0.7103 - val_loss: 0.6010 - val_acc: 0.7191\n",
      "Epoch 15/200\n",
      "3587/3587 [==============================] - 2s 469us/step - loss: 0.5906 - acc: 0.7197 - val_loss: 0.5963 - val_acc: 0.7213\n",
      "Epoch 16/200\n",
      "3587/3587 [==============================] - 1s 403us/step - loss: 0.5861 - acc: 0.7152 - val_loss: 0.5932 - val_acc: 0.7280\n",
      "Epoch 17/200\n",
      "3587/3587 [==============================] - 2s 473us/step - loss: 0.5878 - acc: 0.7204 - val_loss: 0.5942 - val_acc: 0.7313\n",
      "Epoch 18/200\n",
      "3587/3587 [==============================] - 1s 380us/step - loss: 0.5822 - acc: 0.7198 - val_loss: 0.5890 - val_acc: 0.7302\n",
      "Epoch 19/200\n",
      "3587/3587 [==============================] - 1s 385us/step - loss: 0.5832 - acc: 0.7229 - val_loss: 0.5895 - val_acc: 0.7308\n",
      "Epoch 20/200\n",
      "3587/3587 [==============================] - 1s 343us/step - loss: 0.5743 - acc: 0.7294 - val_loss: 0.5924 - val_acc: 0.7274\n",
      "Epoch 21/200\n",
      "3587/3587 [==============================] - 1s 363us/step - loss: 0.5769 - acc: 0.7321 - val_loss: 0.5941 - val_acc: 0.7302\n",
      "Epoch 22/200\n",
      "3587/3587 [==============================] - 1s 353us/step - loss: 0.5693 - acc: 0.7331 - val_loss: 0.5837 - val_acc: 0.7280\n",
      "Epoch 23/200\n",
      "3587/3587 [==============================] - 1s 342us/step - loss: 0.5683 - acc: 0.7410 - val_loss: 0.5831 - val_acc: 0.7352\n",
      "Epoch 24/200\n",
      "3587/3587 [==============================] - 1s 358us/step - loss: 0.5697 - acc: 0.7353 - val_loss: 0.5856 - val_acc: 0.7391\n",
      "Epoch 25/200\n",
      "3587/3587 [==============================] - 1s 366us/step - loss: 0.5664 - acc: 0.7392 - val_loss: 0.5853 - val_acc: 0.7386\n",
      "Epoch 26/200\n",
      "3587/3587 [==============================] - 1s 347us/step - loss: 0.5676 - acc: 0.7361 - val_loss: 0.5824 - val_acc: 0.7380\n",
      "Epoch 27/200\n",
      "3587/3587 [==============================] - 1s 344us/step - loss: 0.5584 - acc: 0.7477 - val_loss: 0.5756 - val_acc: 0.7313\n",
      "Epoch 28/200\n",
      "3587/3587 [==============================] - 1s 341us/step - loss: 0.5651 - acc: 0.7379 - val_loss: 0.5771 - val_acc: 0.7347\n",
      "Epoch 29/200\n",
      "3587/3587 [==============================] - 1s 341us/step - loss: 0.5564 - acc: 0.7427 - val_loss: 0.5755 - val_acc: 0.7336\n",
      "Epoch 30/200\n",
      "3587/3587 [==============================] - 1s 345us/step - loss: 0.5517 - acc: 0.7501 - val_loss: 0.5796 - val_acc: 0.7330\n",
      "Epoch 31/200\n",
      "3587/3587 [==============================] - 1s 342us/step - loss: 0.5557 - acc: 0.7410 - val_loss: 0.5786 - val_acc: 0.7347\n",
      "Epoch 32/200\n",
      "3587/3587 [==============================] - 1s 374us/step - loss: 0.5531 - acc: 0.7459 - val_loss: 0.5765 - val_acc: 0.7324\n",
      "Epoch 33/200\n",
      "3587/3587 [==============================] - 1s 381us/step - loss: 0.5577 - acc: 0.7425 - val_loss: 0.5781 - val_acc: 0.7336\n",
      "Epoch 34/200\n",
      "3587/3587 [==============================] - 1s 345us/step - loss: 0.5469 - acc: 0.7561 - val_loss: 0.5754 - val_acc: 0.7352\n",
      "Epoch 35/200\n",
      "3587/3587 [==============================] - 1s 345us/step - loss: 0.5472 - acc: 0.7548 - val_loss: 0.5769 - val_acc: 0.7330\n",
      "Epoch 36/200\n",
      "3587/3587 [==============================] - 1s 343us/step - loss: 0.5471 - acc: 0.7536 - val_loss: 0.5682 - val_acc: 0.7358\n",
      "Epoch 37/200\n",
      "3587/3587 [==============================] - 1s 343us/step - loss: 0.5442 - acc: 0.7547 - val_loss: 0.5765 - val_acc: 0.7352\n",
      "Epoch 38/200\n",
      "3587/3587 [==============================] - 1s 372us/step - loss: 0.5423 - acc: 0.7551 - val_loss: 0.5737 - val_acc: 0.7386\n",
      "Epoch 39/200\n",
      "3587/3587 [==============================] - 1s 347us/step - loss: 0.5412 - acc: 0.7513 - val_loss: 0.5782 - val_acc: 0.7308\n",
      "Epoch 40/200\n",
      "3587/3587 [==============================] - 1s 366us/step - loss: 0.5424 - acc: 0.7580 - val_loss: 0.5771 - val_acc: 0.7386\n",
      "Epoch 41/200\n",
      "3587/3587 [==============================] - 2s 474us/step - loss: 0.5339 - acc: 0.7608 - val_loss: 0.5807 - val_acc: 0.7330\n",
      "Epoch 42/200\n",
      "3587/3587 [==============================] - 1s 368us/step - loss: 0.5300 - acc: 0.7646 - val_loss: 0.5889 - val_acc: 0.7375\n",
      "Epoch 43/200\n",
      "3587/3587 [==============================] - 1s 357us/step - loss: 0.5303 - acc: 0.7621 - val_loss: 0.5883 - val_acc: 0.7347\n",
      "Epoch 44/200\n",
      "3587/3587 [==============================] - 1s 344us/step - loss: 0.5384 - acc: 0.7615 - val_loss: 0.5720 - val_acc: 0.7369\n",
      "Epoch 45/200\n",
      "3587/3587 [==============================] - 1s 352us/step - loss: 0.5303 - acc: 0.7589 - val_loss: 0.5838 - val_acc: 0.7324\n",
      "Epoch 46/200\n",
      "3587/3587 [==============================] - 1s 363us/step - loss: 0.5262 - acc: 0.7660 - val_loss: 0.5831 - val_acc: 0.7369\n",
      "Epoch 47/200\n",
      "3587/3587 [==============================] - 1s 359us/step - loss: 0.5315 - acc: 0.7674 - val_loss: 0.5796 - val_acc: 0.7358\n",
      "Epoch 48/200\n",
      "3587/3587 [==============================] - 1s 345us/step - loss: 0.5252 - acc: 0.7727 - val_loss: 0.5807 - val_acc: 0.7386\n",
      "Epoch 49/200\n",
      "3587/3587 [==============================] - 1s 350us/step - loss: 0.5244 - acc: 0.7689 - val_loss: 0.5785 - val_acc: 0.7380\n",
      "Epoch 50/200\n",
      "3587/3587 [==============================] - 1s 357us/step - loss: 0.5267 - acc: 0.7672 - val_loss: 0.5793 - val_acc: 0.7391\n",
      "Epoch 51/200\n",
      "3587/3587 [==============================] - 1s 348us/step - loss: 0.5228 - acc: 0.7650 - val_loss: 0.5871 - val_acc: 0.7380\n",
      "Epoch 52/200\n",
      "3587/3587 [==============================] - 1s 361us/step - loss: 0.5258 - acc: 0.7646 - val_loss: 0.5845 - val_acc: 0.7414\n",
      "Epoch 53/200\n",
      "3587/3587 [==============================] - 1s 385us/step - loss: 0.5247 - acc: 0.7635 - val_loss: 0.5839 - val_acc: 0.7402\n",
      "Epoch 54/200\n",
      "3587/3587 [==============================] - 1s 362us/step - loss: 0.5254 - acc: 0.7628 - val_loss: 0.5782 - val_acc: 0.7402\n",
      "Epoch 55/200\n",
      "3587/3587 [==============================] - 1s 370us/step - loss: 0.5294 - acc: 0.7636 - val_loss: 0.5794 - val_acc: 0.7419\n",
      "Epoch 56/200\n",
      "3587/3587 [==============================] - 1s 352us/step - loss: 0.5186 - acc: 0.7710 - val_loss: 0.5885 - val_acc: 0.7402\n",
      "Epoch 57/200\n",
      "3587/3587 [==============================] - 1s 370us/step - loss: 0.5195 - acc: 0.7717 - val_loss: 0.5886 - val_acc: 0.7397\n",
      "Epoch 58/200\n",
      "3587/3587 [==============================] - 1s 351us/step - loss: 0.5187 - acc: 0.7707 - val_loss: 0.5758 - val_acc: 0.7464\n",
      "Epoch 59/200\n",
      "3587/3587 [==============================] - 1s 357us/step - loss: 0.5130 - acc: 0.7715 - val_loss: 0.5823 - val_acc: 0.7464\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3587/3587 [==============================] - 1s 372us/step - loss: 0.5219 - acc: 0.7682 - val_loss: 0.5764 - val_acc: 0.7447\n",
      "Epoch 61/200\n",
      "3587/3587 [==============================] - 1s 348us/step - loss: 0.5175 - acc: 0.7738 - val_loss: 0.5784 - val_acc: 0.7492\n",
      "Epoch 62/200\n",
      "3587/3587 [==============================] - 1s 376us/step - loss: 0.5102 - acc: 0.7763 - val_loss: 0.5729 - val_acc: 0.7447\n",
      "Epoch 63/200\n",
      "3587/3587 [==============================] - 1s 346us/step - loss: 0.5169 - acc: 0.7692 - val_loss: 0.5792 - val_acc: 0.7397\n",
      "Epoch 64/200\n",
      "3587/3587 [==============================] - 1s 352us/step - loss: 0.5072 - acc: 0.7788 - val_loss: 0.5804 - val_acc: 0.7425\n",
      "Epoch 65/200\n",
      "3587/3587 [==============================] - 1s 370us/step - loss: 0.5054 - acc: 0.7781 - val_loss: 0.5811 - val_acc: 0.7447\n",
      "Epoch 66/200\n",
      "3587/3587 [==============================] - 1s 354us/step - loss: 0.5068 - acc: 0.7761 - val_loss: 0.5770 - val_acc: 0.7486\n",
      "Epoch 67/200\n",
      "3587/3587 [==============================] - 1s 365us/step - loss: 0.5166 - acc: 0.7678 - val_loss: 0.5723 - val_acc: 0.7458\n",
      "Epoch 68/200\n",
      "3587/3587 [==============================] - 1s 360us/step - loss: 0.5126 - acc: 0.7747 - val_loss: 0.5698 - val_acc: 0.7475\n",
      "Epoch 69/200\n",
      "3587/3587 [==============================] - 1s 343us/step - loss: 0.5005 - acc: 0.7809 - val_loss: 0.5711 - val_acc: 0.7486\n",
      "Epoch 70/200\n",
      "3587/3587 [==============================] - 1s 339us/step - loss: 0.5065 - acc: 0.7752 - val_loss: 0.5842 - val_acc: 0.7464\n",
      "Epoch 71/200\n",
      "3587/3587 [==============================] - 1s 352us/step - loss: 0.5065 - acc: 0.7828 - val_loss: 0.5914 - val_acc: 0.7386\n",
      "Epoch 72/200\n",
      "3587/3587 [==============================] - 1s 374us/step - loss: 0.4950 - acc: 0.7837 - val_loss: 0.5821 - val_acc: 0.7436\n",
      "Epoch 73/200\n",
      "3587/3587 [==============================] - 1s 352us/step - loss: 0.5009 - acc: 0.7827 - val_loss: 0.5791 - val_acc: 0.7508\n",
      "Epoch 74/200\n",
      "3587/3587 [==============================] - 1s 352us/step - loss: 0.5164 - acc: 0.7768 - val_loss: 0.5781 - val_acc: 0.7503\n",
      "Epoch 75/200\n",
      "3587/3587 [==============================] - 1s 353us/step - loss: 0.5059 - acc: 0.7788 - val_loss: 0.5731 - val_acc: 0.7441\n",
      "Epoch 76/200\n",
      "3587/3587 [==============================] - 1s 364us/step - loss: 0.5100 - acc: 0.7763 - val_loss: 0.5715 - val_acc: 0.7408\n",
      "Epoch 77/200\n",
      "3587/3587 [==============================] - 1s 349us/step - loss: 0.5035 - acc: 0.7816 - val_loss: 0.5732 - val_acc: 0.7436\n",
      "Epoch 78/200\n",
      "3587/3587 [==============================] - 1s 350us/step - loss: 0.4948 - acc: 0.7812 - val_loss: 0.5729 - val_acc: 0.7464\n",
      "Epoch 79/200\n",
      "3587/3587 [==============================] - 1s 350us/step - loss: 0.4955 - acc: 0.7867 - val_loss: 0.5836 - val_acc: 0.7430\n",
      "Epoch 80/200\n",
      "3587/3587 [==============================] - 1s 347us/step - loss: 0.5034 - acc: 0.7820 - val_loss: 0.5820 - val_acc: 0.7486\n",
      "Epoch 81/200\n",
      "3587/3587 [==============================] - 1s 341us/step - loss: 0.4990 - acc: 0.7859 - val_loss: 0.5755 - val_acc: 0.7486\n",
      "Epoch 82/200\n",
      "3587/3587 [==============================] - 1s 358us/step - loss: 0.4972 - acc: 0.7789 - val_loss: 0.5817 - val_acc: 0.7436\n",
      "Epoch 83/200\n",
      "3587/3587 [==============================] - 1s 343us/step - loss: 0.5022 - acc: 0.7813 - val_loss: 0.5820 - val_acc: 0.7458\n",
      "Epoch 84/200\n",
      "3587/3587 [==============================] - 1s 375us/step - loss: 0.4958 - acc: 0.7806 - val_loss: 0.5789 - val_acc: 0.7458\n",
      "Epoch 85/200\n",
      "3587/3587 [==============================] - 1s 344us/step - loss: 0.4962 - acc: 0.7852 - val_loss: 0.5894 - val_acc: 0.7480\n",
      "Epoch 86/200\n",
      "3587/3587 [==============================] - 1s 345us/step - loss: 0.4901 - acc: 0.7869 - val_loss: 0.5895 - val_acc: 0.7436\n",
      "Epoch 87/200\n",
      "3587/3587 [==============================] - 1s 354us/step - loss: 0.4852 - acc: 0.7856 - val_loss: 0.5969 - val_acc: 0.7397\n",
      "Epoch 88/200\n",
      "3587/3587 [==============================] - 1s 361us/step - loss: 0.4837 - acc: 0.7891 - val_loss: 0.5922 - val_acc: 0.7469\n",
      "Epoch 89/200\n",
      "3587/3587 [==============================] - 1s 388us/step - loss: 0.4906 - acc: 0.7913 - val_loss: 0.5944 - val_acc: 0.7441\n",
      "Epoch 90/200\n",
      "3587/3587 [==============================] - 2s 498us/step - loss: 0.4848 - acc: 0.7856 - val_loss: 0.5962 - val_acc: 0.7408\n",
      "Epoch 91/200\n",
      "3587/3587 [==============================] - 1s 365us/step - loss: 0.4804 - acc: 0.7952 - val_loss: 0.5874 - val_acc: 0.7436\n",
      "Epoch 92/200\n",
      "3587/3587 [==============================] - 1s 366us/step - loss: 0.4891 - acc: 0.7877 - val_loss: 0.5938 - val_acc: 0.7447\n",
      "Epoch 93/200\n",
      "3587/3587 [==============================] - 1s 338us/step - loss: 0.4826 - acc: 0.7899 - val_loss: 0.5954 - val_acc: 0.7447\n",
      "Epoch 94/200\n",
      "3587/3587 [==============================] - 1s 337us/step - loss: 0.4818 - acc: 0.7912 - val_loss: 0.6001 - val_acc: 0.7464\n",
      "Epoch 95/200\n",
      "3587/3587 [==============================] - 1s 339us/step - loss: 0.4794 - acc: 0.7906 - val_loss: 0.5938 - val_acc: 0.7492\n",
      "Epoch 96/200\n",
      "3587/3587 [==============================] - 1s 344us/step - loss: 0.4823 - acc: 0.7908 - val_loss: 0.5804 - val_acc: 0.7458\n",
      "Epoch 97/200\n",
      "3587/3587 [==============================] - 1s 341us/step - loss: 0.4855 - acc: 0.7936 - val_loss: 0.5871 - val_acc: 0.7492\n",
      "Epoch 98/200\n",
      "3587/3587 [==============================] - 1s 337us/step - loss: 0.4823 - acc: 0.7880 - val_loss: 0.5897 - val_acc: 0.7525\n",
      "Epoch 99/200\n",
      "3587/3587 [==============================] - 1s 336us/step - loss: 0.4854 - acc: 0.7930 - val_loss: 0.5913 - val_acc: 0.7514\n",
      "Epoch 100/200\n",
      "3587/3587 [==============================] - 1s 338us/step - loss: 0.4715 - acc: 0.7950 - val_loss: 0.5940 - val_acc: 0.7447\n",
      "Epoch 101/200\n",
      "3587/3587 [==============================] - 1s 337us/step - loss: 0.4862 - acc: 0.7848 - val_loss: 0.5783 - val_acc: 0.7414\n",
      "Epoch 102/200\n",
      "3587/3587 [==============================] - 1s 369us/step - loss: 0.4821 - acc: 0.7934 - val_loss: 0.5962 - val_acc: 0.7503\n",
      "Epoch 103/200\n",
      "3587/3587 [==============================] - 1s 346us/step - loss: 0.4737 - acc: 0.7984 - val_loss: 0.5883 - val_acc: 0.7475\n",
      "Epoch 104/200\n",
      "3587/3587 [==============================] - 1s 339us/step - loss: 0.4700 - acc: 0.7987 - val_loss: 0.6023 - val_acc: 0.7497\n",
      "Epoch 105/200\n",
      "3587/3587 [==============================] - 1s 338us/step - loss: 0.4811 - acc: 0.7951 - val_loss: 0.5921 - val_acc: 0.7503\n",
      "Epoch 106/200\n",
      "3587/3587 [==============================] - 1s 338us/step - loss: 0.4752 - acc: 0.7977 - val_loss: 0.5845 - val_acc: 0.7480\n",
      "Epoch 107/200\n",
      "3587/3587 [==============================] - 1s 339us/step - loss: 0.4717 - acc: 0.7913 - val_loss: 0.5825 - val_acc: 0.7464\n",
      "Epoch 108/200\n",
      "3587/3587 [==============================] - 1s 339us/step - loss: 0.4691 - acc: 0.7979 - val_loss: 0.5820 - val_acc: 0.7480\n",
      "Epoch 109/200\n",
      "3587/3587 [==============================] - 1s 345us/step - loss: 0.4835 - acc: 0.7901 - val_loss: 0.5906 - val_acc: 0.7447\n",
      "Epoch 110/200\n",
      "3587/3587 [==============================] - 1s 345us/step - loss: 0.4729 - acc: 0.7922 - val_loss: 0.5935 - val_acc: 0.7480\n",
      "Epoch 111/200\n",
      "3587/3587 [==============================] - 1s 344us/step - loss: 0.4715 - acc: 0.7984 - val_loss: 0.5931 - val_acc: 0.7475\n",
      "Epoch 112/200\n",
      "3587/3587 [==============================] - 1s 349us/step - loss: 0.4716 - acc: 0.7958 - val_loss: 0.5921 - val_acc: 0.7447\n",
      "Epoch 113/200\n",
      "3587/3587 [==============================] - 1s 339us/step - loss: 0.4535 - acc: 0.8064 - val_loss: 0.5984 - val_acc: 0.7397\n",
      "Epoch 114/200\n",
      "3587/3587 [==============================] - 1s 339us/step - loss: 0.4662 - acc: 0.7965 - val_loss: 0.5994 - val_acc: 0.7414\n",
      "Epoch 115/200\n",
      "3587/3587 [==============================] - 1s 338us/step - loss: 0.4720 - acc: 0.7945 - val_loss: 0.5839 - val_acc: 0.7492\n",
      "Epoch 116/200\n",
      "3587/3587 [==============================] - 1s 339us/step - loss: 0.4644 - acc: 0.8021 - val_loss: 0.5935 - val_acc: 0.7503\n",
      "Epoch 117/200\n",
      "3587/3587 [==============================] - 1s 339us/step - loss: 0.4602 - acc: 0.8023 - val_loss: 0.6076 - val_acc: 0.7508\n",
      "Epoch 118/200\n",
      "3587/3587 [==============================] - 1s 341us/step - loss: 0.4534 - acc: 0.8085 - val_loss: 0.5961 - val_acc: 0.7492\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3587/3587 [==============================] - 1s 338us/step - loss: 0.4620 - acc: 0.8009 - val_loss: 0.6064 - val_acc: 0.7447\n",
      "Epoch 120/200\n",
      "3587/3587 [==============================] - 1s 336us/step - loss: 0.4494 - acc: 0.8089 - val_loss: 0.5993 - val_acc: 0.7464\n",
      "Epoch 121/200\n",
      "3587/3587 [==============================] - 1s 336us/step - loss: 0.4614 - acc: 0.8018 - val_loss: 0.6154 - val_acc: 0.7453\n",
      "Epoch 122/200\n",
      "3587/3587 [==============================] - 1s 335us/step - loss: 0.4627 - acc: 0.7986 - val_loss: 0.5949 - val_acc: 0.7469\n",
      "Epoch 123/200\n",
      "3587/3587 [==============================] - 1s 337us/step - loss: 0.4598 - acc: 0.8004 - val_loss: 0.5882 - val_acc: 0.7475\n",
      "Epoch 124/200\n",
      "3587/3587 [==============================] - 1s 337us/step - loss: 0.4577 - acc: 0.8069 - val_loss: 0.6004 - val_acc: 0.7469\n",
      "Epoch 125/200\n",
      "3587/3587 [==============================] - 1s 337us/step - loss: 0.4621 - acc: 0.7996 - val_loss: 0.6135 - val_acc: 0.7514\n",
      "Epoch 126/200\n",
      "3587/3587 [==============================] - 1s 336us/step - loss: 0.4623 - acc: 0.8015 - val_loss: 0.5989 - val_acc: 0.7497\n",
      "Epoch 127/200\n",
      "3587/3587 [==============================] - 1s 337us/step - loss: 0.4572 - acc: 0.8014 - val_loss: 0.5988 - val_acc: 0.7475\n",
      "Epoch 128/200\n",
      "3587/3587 [==============================] - 1s 337us/step - loss: 0.4488 - acc: 0.8068 - val_loss: 0.6223 - val_acc: 0.7497\n",
      "Epoch 129/200\n",
      "3587/3587 [==============================] - 1s 368us/step - loss: 0.4642 - acc: 0.8032 - val_loss: 0.5927 - val_acc: 0.7497\n",
      "Epoch 130/200\n",
      "3587/3587 [==============================] - 1s 411us/step - loss: 0.4551 - acc: 0.8032 - val_loss: 0.6073 - val_acc: 0.7464\n",
      "Epoch 131/200\n",
      "3587/3587 [==============================] - 2s 433us/step - loss: 0.4553 - acc: 0.8076 - val_loss: 0.6068 - val_acc: 0.7469\n",
      "Epoch 132/200\n",
      "3587/3587 [==============================] - 2s 423us/step - loss: 0.4547 - acc: 0.8043 - val_loss: 0.6100 - val_acc: 0.7458\n",
      "Epoch 133/200\n",
      "3587/3587 [==============================] - 1s 405us/step - loss: 0.4542 - acc: 0.8042 - val_loss: 0.6205 - val_acc: 0.7458\n",
      "Epoch 134/200\n",
      "3587/3587 [==============================] - 1s 407us/step - loss: 0.4538 - acc: 0.8019 - val_loss: 0.6310 - val_acc: 0.7497\n",
      "Epoch 135/200\n",
      "3587/3587 [==============================] - 1s 396us/step - loss: 0.4549 - acc: 0.8055 - val_loss: 0.6243 - val_acc: 0.7531\n",
      "Epoch 136/200\n",
      "3587/3587 [==============================] - 1s 401us/step - loss: 0.4440 - acc: 0.8127 - val_loss: 0.6190 - val_acc: 0.7536\n",
      "Epoch 137/200\n",
      "3587/3587 [==============================] - 1s 408us/step - loss: 0.4410 - acc: 0.8089 - val_loss: 0.6157 - val_acc: 0.7525\n",
      "Epoch 138/200\n",
      "3587/3587 [==============================] - 2s 451us/step - loss: 0.4469 - acc: 0.8061 - val_loss: 0.6231 - val_acc: 0.7525\n",
      "Epoch 139/200\n",
      "3587/3587 [==============================] - 2s 607us/step - loss: 0.4523 - acc: 0.8019 - val_loss: 0.6379 - val_acc: 0.7520\n",
      "Epoch 140/200\n",
      "3587/3587 [==============================] - 1s 408us/step - loss: 0.4493 - acc: 0.8057 - val_loss: 0.6381 - val_acc: 0.7514\n",
      "Epoch 141/200\n",
      "3587/3587 [==============================] - 2s 428us/step - loss: 0.4497 - acc: 0.8132 - val_loss: 0.6121 - val_acc: 0.7536\n",
      "Epoch 142/200\n",
      "3587/3587 [==============================] - 2s 447us/step - loss: 0.4353 - acc: 0.8168 - val_loss: 0.6131 - val_acc: 0.7536\n",
      "Epoch 143/200\n",
      "3587/3587 [==============================] - 1s 362us/step - loss: 0.4339 - acc: 0.8124 - val_loss: 0.6083 - val_acc: 0.7547\n",
      "Epoch 144/200\n",
      "3587/3587 [==============================] - 1s 340us/step - loss: 0.4456 - acc: 0.8095 - val_loss: 0.5906 - val_acc: 0.7581\n",
      "Epoch 145/200\n",
      "3587/3587 [==============================] - 1s 355us/step - loss: 0.4399 - acc: 0.8180 - val_loss: 0.6233 - val_acc: 0.7559\n",
      "Epoch 146/200\n",
      "3587/3587 [==============================] - 1s 350us/step - loss: 0.4466 - acc: 0.8082 - val_loss: 0.6078 - val_acc: 0.7536\n",
      "Epoch 147/200\n",
      "3587/3587 [==============================] - 1s 338us/step - loss: 0.4370 - acc: 0.8114 - val_loss: 0.6470 - val_acc: 0.7492\n",
      "Epoch 148/200\n",
      "3587/3587 [==============================] - 1s 339us/step - loss: 0.4455 - acc: 0.8138 - val_loss: 0.6285 - val_acc: 0.7547\n",
      "Epoch 149/200\n",
      "3587/3587 [==============================] - 1s 340us/step - loss: 0.4355 - acc: 0.8184 - val_loss: 0.6146 - val_acc: 0.7603\n",
      "Epoch 150/200\n",
      "3587/3587 [==============================] - 1s 338us/step - loss: 0.4387 - acc: 0.8113 - val_loss: 0.5891 - val_acc: 0.7592\n",
      "Epoch 151/200\n",
      "3587/3587 [==============================] - 1s 338us/step - loss: 0.4369 - acc: 0.8128 - val_loss: 0.5925 - val_acc: 0.7620\n",
      "Epoch 152/200\n",
      "3587/3587 [==============================] - 1s 368us/step - loss: 0.4400 - acc: 0.8141 - val_loss: 0.6041 - val_acc: 0.7547\n",
      "Epoch 153/200\n",
      "3587/3587 [==============================] - 1s 345us/step - loss: 0.4267 - acc: 0.8154 - val_loss: 0.6277 - val_acc: 0.7542\n",
      "Epoch 154/200\n",
      "3587/3587 [==============================] - 1s 341us/step - loss: 0.4371 - acc: 0.8154 - val_loss: 0.6034 - val_acc: 0.7520\n",
      "Epoch 155/200\n",
      "3587/3587 [==============================] - 1s 341us/step - loss: 0.4369 - acc: 0.8111 - val_loss: 0.5959 - val_acc: 0.7553\n",
      "Epoch 156/200\n",
      "3587/3587 [==============================] - 1s 367us/step - loss: 0.4344 - acc: 0.8118 - val_loss: 0.6094 - val_acc: 0.7614\n",
      "Epoch 157/200\n",
      "3587/3587 [==============================] - 1s 373us/step - loss: 0.4278 - acc: 0.8174 - val_loss: 0.6125 - val_acc: 0.7570\n",
      "Epoch 158/200\n",
      "3587/3587 [==============================] - 1s 341us/step - loss: 0.4235 - acc: 0.8200 - val_loss: 0.6211 - val_acc: 0.7648\n",
      "Epoch 159/200\n",
      "3587/3587 [==============================] - 1s 362us/step - loss: 0.4178 - acc: 0.8219 - val_loss: 0.6184 - val_acc: 0.7542\n",
      "Epoch 160/200\n",
      "3587/3587 [==============================] - 1s 365us/step - loss: 0.4313 - acc: 0.8152 - val_loss: 0.6156 - val_acc: 0.7542\n",
      "Epoch 161/200\n",
      "3587/3587 [==============================] - 1s 348us/step - loss: 0.4392 - acc: 0.8096 - val_loss: 0.5999 - val_acc: 0.7609\n",
      "Epoch 162/200\n",
      "3587/3587 [==============================] - 1s 347us/step - loss: 0.4130 - acc: 0.8224 - val_loss: 0.5968 - val_acc: 0.7653\n",
      "Epoch 163/200\n",
      "3587/3587 [==============================] - 1s 352us/step - loss: 0.4109 - acc: 0.8269 - val_loss: 0.6090 - val_acc: 0.7614\n",
      "Epoch 164/200\n",
      "3587/3587 [==============================] - 1s 347us/step - loss: 0.4218 - acc: 0.8227 - val_loss: 0.6126 - val_acc: 0.7614\n",
      "Epoch 165/200\n",
      "3587/3587 [==============================] - 1s 356us/step - loss: 0.4263 - acc: 0.8164 - val_loss: 0.6070 - val_acc: 0.7659\n",
      "Epoch 166/200\n",
      "3587/3587 [==============================] - 1s 345us/step - loss: 0.4158 - acc: 0.8249 - val_loss: 0.6168 - val_acc: 0.7598\n",
      "Epoch 167/200\n",
      "3587/3587 [==============================] - 1s 348us/step - loss: 0.4228 - acc: 0.8189 - val_loss: 0.6260 - val_acc: 0.7609\n",
      "Epoch 168/200\n",
      "3587/3587 [==============================] - 1s 388us/step - loss: 0.4118 - acc: 0.8301 - val_loss: 0.6153 - val_acc: 0.7625\n",
      "Epoch 169/200\n",
      "3587/3587 [==============================] - 1s 346us/step - loss: 0.4147 - acc: 0.8265 - val_loss: 0.6180 - val_acc: 0.7564\n",
      "Epoch 170/200\n",
      "3587/3587 [==============================] - 1s 353us/step - loss: 0.4036 - acc: 0.8234 - val_loss: 0.6339 - val_acc: 0.7553\n",
      "Epoch 171/200\n",
      "3587/3587 [==============================] - 1s 342us/step - loss: 0.4160 - acc: 0.8167 - val_loss: 0.6245 - val_acc: 0.7592\n",
      "Epoch 172/200\n",
      "3587/3587 [==============================] - 1s 352us/step - loss: 0.4184 - acc: 0.8239 - val_loss: 0.6266 - val_acc: 0.7586\n",
      "Epoch 173/200\n",
      "3587/3587 [==============================] - 1s 343us/step - loss: 0.4184 - acc: 0.8191 - val_loss: 0.6412 - val_acc: 0.7592\n",
      "Epoch 174/200\n",
      "3587/3587 [==============================] - 1s 342us/step - loss: 0.4043 - acc: 0.8285 - val_loss: 0.6473 - val_acc: 0.7637\n",
      "Epoch 175/200\n",
      "3587/3587 [==============================] - 1s 341us/step - loss: 0.4172 - acc: 0.8180 - val_loss: 0.6103 - val_acc: 0.7659\n",
      "Epoch 176/200\n",
      "3587/3587 [==============================] - 1s 343us/step - loss: 0.4211 - acc: 0.8214 - val_loss: 0.6190 - val_acc: 0.7603\n",
      "Epoch 177/200\n",
      "3587/3587 [==============================] - 1s 344us/step - loss: 0.4141 - acc: 0.8221 - val_loss: 0.6275 - val_acc: 0.7670\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3587/3587 [==============================] - 1s 353us/step - loss: 0.4132 - acc: 0.8163 - val_loss: 0.6234 - val_acc: 0.7648\n",
      "Epoch 179/200\n",
      "3587/3587 [==============================] - 1s 345us/step - loss: 0.4187 - acc: 0.8198 - val_loss: 0.5998 - val_acc: 0.7637\n",
      "Epoch 180/200\n",
      "3587/3587 [==============================] - 1s 352us/step - loss: 0.4024 - acc: 0.8276 - val_loss: 0.6336 - val_acc: 0.7625\n",
      "Epoch 181/200\n",
      "3587/3587 [==============================] - 1s 349us/step - loss: 0.4100 - acc: 0.8220 - val_loss: 0.6122 - val_acc: 0.7614\n",
      "Epoch 182/200\n",
      "3587/3587 [==============================] - 1s 351us/step - loss: 0.4026 - acc: 0.8270 - val_loss: 0.6296 - val_acc: 0.7659\n",
      "Epoch 183/200\n",
      "3587/3587 [==============================] - 1s 355us/step - loss: 0.4156 - acc: 0.8251 - val_loss: 0.6128 - val_acc: 0.7581\n",
      "Epoch 184/200\n",
      "3587/3587 [==============================] - 1s 348us/step - loss: 0.4084 - acc: 0.8219 - val_loss: 0.6242 - val_acc: 0.7614\n",
      "Epoch 185/200\n",
      "3587/3587 [==============================] - 1s 348us/step - loss: 0.4025 - acc: 0.8292 - val_loss: 0.6147 - val_acc: 0.7648\n",
      "Epoch 186/200\n",
      "3587/3587 [==============================] - 1s 358us/step - loss: 0.3991 - acc: 0.8277 - val_loss: 0.6094 - val_acc: 0.7748\n",
      "Epoch 187/200\n",
      "3587/3587 [==============================] - 2s 460us/step - loss: 0.3974 - acc: 0.8309 - val_loss: 0.6257 - val_acc: 0.7726\n",
      "Epoch 188/200\n",
      "3587/3587 [==============================] - 2s 476us/step - loss: 0.4073 - acc: 0.8280 - val_loss: 0.6138 - val_acc: 0.7748\n",
      "Epoch 189/200\n",
      "3587/3587 [==============================] - 1s 346us/step - loss: 0.3991 - acc: 0.8291 - val_loss: 0.6354 - val_acc: 0.7726\n",
      "Epoch 190/200\n",
      "3587/3587 [==============================] - 1s 360us/step - loss: 0.4089 - acc: 0.8260 - val_loss: 0.6149 - val_acc: 0.7781\n",
      "Epoch 191/200\n",
      "3587/3587 [==============================] - 1s 352us/step - loss: 0.3958 - acc: 0.8266 - val_loss: 0.6329 - val_acc: 0.7726\n",
      "Epoch 192/200\n",
      "3587/3587 [==============================] - 1s 359us/step - loss: 0.4032 - acc: 0.8315 - val_loss: 0.6254 - val_acc: 0.7742\n",
      "Epoch 193/200\n",
      "3587/3587 [==============================] - 1s 354us/step - loss: 0.3934 - acc: 0.8235 - val_loss: 0.6380 - val_acc: 0.7737\n",
      "Epoch 194/200\n",
      "3587/3587 [==============================] - 1s 339us/step - loss: 0.4061 - acc: 0.8255 - val_loss: 0.6160 - val_acc: 0.7659\n",
      "Epoch 195/200\n",
      "3587/3587 [==============================] - 1s 345us/step - loss: 0.4014 - acc: 0.8330 - val_loss: 0.6499 - val_acc: 0.7703\n",
      "Epoch 196/200\n",
      "3587/3587 [==============================] - 1s 341us/step - loss: 0.3898 - acc: 0.8364 - val_loss: 0.6346 - val_acc: 0.7703\n",
      "Epoch 197/200\n",
      "3587/3587 [==============================] - 1s 341us/step - loss: 0.3966 - acc: 0.8299 - val_loss: 0.6361 - val_acc: 0.7715\n",
      "Epoch 198/200\n",
      "3587/3587 [==============================] - 1s 338us/step - loss: 0.3924 - acc: 0.8329 - val_loss: 0.6398 - val_acc: 0.7715\n",
      "Epoch 199/200\n",
      "3587/3587 [==============================] - 2s 433us/step - loss: 0.3905 - acc: 0.8322 - val_loss: 0.6286 - val_acc: 0.7715\n",
      "Epoch 200/200\n",
      "3587/3587 [==============================] - 1s 417us/step - loss: 0.3974 - acc: 0.8306 - val_loss: 0.6302 - val_acc: 0.7648\n"
     ]
    }
   ],
   "source": [
    "X_train_lstm = np.reshape(X_train, (X_train.shape[0],1, X_train.shape[1]))\n",
    "X_test_lstm = np.reshape(X_test, (X_test.shape[0],1 ,X_test.shape[1]))\n",
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    checker = classifier.fit(X_train_lstm, Y_train, batch_size=32, epochs=200, validation_data = (X_test_lstm, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_test_label = classifier.predict(X_test_lstm)\n",
    "y_pred_test=np.argmax(Y_pred_test_label,axis =1)\n",
    "y_pred_test\n",
    "Y_pred_train_label = classifier.predict(X_train_lstm)\n",
    "y_pred_train = np.argmax(Y_pred_train_label,axis=1)\n",
    "Y_test_true = Y_test_org.astype(np.int)\n",
    "Y_train_true = Y_train_org.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.73      0.75       443\n",
      "          1       0.75      0.80      0.77       454\n",
      "\n",
      "avg / total       0.77      0.76      0.76       897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test_true,y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  \n",
      " [[1423  101]\n",
      " [ 376 1687]]\n",
      "\n",
      "Test:  \n",
      " [[324  92]\n",
      " [119 362]]\n"
     ]
    }
   ],
   "source": [
    "### for softmax function\n",
    "print(\"TRAIN:  \\n\",confusion_matrix(y_pred_train,Y_train_true))\n",
    "print(\"\\nTest:  \\n\",confusion_matrix(y_pred_test,Y_test_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.79      0.86      1799\n",
      "          1       0.82      0.94      0.88      1788\n",
      "\n",
      "avg / total       0.88      0.87      0.87      3587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train_true,y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.save(\"rnn_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model(\"rnn_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, None, 40)          21760     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, None, 40)          0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, None, 20)          4880      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, None, 20)          0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 20)                3280      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 29,962\n",
      "Trainable params: 29,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.14559674,  0.7160488 ,  0.13177043, ..., -0.3410427 ,\n",
       "          0.25402558, -0.365889  ],\n",
       "        [ 0.43805093, -0.03930505,  0.42130852, ...,  0.01915116,\n",
       "          0.46031323,  0.26138148],\n",
       "        [ 0.0038806 , -0.71913564,  0.2839417 , ...,  0.17708573,\n",
       "         -0.30368337,  0.2878247 ],\n",
       "        ...,\n",
       "        [-0.10714342,  0.11328012, -0.09651805, ...,  0.04101078,\n",
       "         -0.02902809,  0.13805145],\n",
       "        [-0.08265503,  0.01472561, -0.12156597, ...,  0.02105242,\n",
       "         -0.11887933, -0.06857432],\n",
       "        [ 0.12370333, -0.09113126,  0.003426  , ...,  0.03459573,\n",
       "         -0.0954856 ,  0.11739311]], dtype=float32),\n",
       " array([[ 0.07421079,  0.04894492, -0.09644005, ..., -0.01759761,\n",
       "         -0.04478638,  0.19051749],\n",
       "        [-0.04148018,  0.02589066, -0.03653965, ...,  0.00762378,\n",
       "         -0.0292593 ,  0.00283599],\n",
       "        [-0.03455735, -0.07841709,  0.16490877, ..., -0.12657033,\n",
       "         -0.02161739,  0.03423002],\n",
       "        ...,\n",
       "        [ 0.19854218,  0.00813805,  0.06661864, ..., -0.00236205,\n",
       "          0.10509006, -0.16978557],\n",
       "        [ 0.08821157,  0.10996906,  0.07370839, ...,  0.08907415,\n",
       "          0.02620108, -0.05524605],\n",
       "        [ 0.1004497 , -0.09301174, -0.04064795, ..., -0.02176834,\n",
       "         -0.17422493,  0.0076182 ]], dtype=float32),\n",
       " array([-0.01428096, -0.03654743,  0.43458763, -0.0626974 , -0.09164155,\n",
       "         0.0132891 ,  0.37646037,  0.1628592 , -0.13786118, -0.05624367,\n",
       "        -0.19872831, -0.3001198 , -0.24720246, -0.2864497 ,  0.2765172 ,\n",
       "         0.15744004,  0.04446374,  0.12375719,  0.2741893 , -0.05806046,\n",
       "         0.41909546, -0.07020451, -0.07683285, -0.18500954, -0.01856647,\n",
       "        -0.13870564, -0.22812307, -0.00409553, -0.16864645,  0.45911375,\n",
       "         0.12003275,  0.02766914,  0.17671864,  1.2802866 ,  0.5923417 ,\n",
       "        -0.07864476, -0.11931595,  0.32507744,  0.09213009,  0.15318246,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -0.52106357,  0.22509916, -0.40905693, -0.4487736 ,  0.53158313,\n",
       "        -0.57382005, -0.34474573, -0.42681068,  0.06236032, -0.5836568 ,\n",
       "        -0.4559133 , -0.25442785, -0.22963814,  0.17679305, -0.38805363,\n",
       "        -0.28585175, -0.57820565,  0.3244784 ,  0.28098696,  0.15932927,\n",
       "        -0.39702258,  0.3269148 , -0.5791593 , -0.42573607, -0.30689684,\n",
       "        -0.34700698, -0.32225487, -0.04416803, -0.4283904 , -0.45200402,\n",
       "        -0.43997312, -0.4355084 , -0.5012733 , -0.8242621 , -0.41480625,\n",
       "        -0.28563276,  0.13122255, -0.3848552 ,  0.19760686, -0.38996607,\n",
       "         0.27742007,  0.14412434, -0.12947616,  0.08571048, -0.20886515,\n",
       "         0.0596022 ,  0.21772896, -0.16260377, -0.34296548,  0.48721647,\n",
       "        -0.27364486, -0.05791295,  0.22213784, -0.01876067,  0.2016223 ,\n",
       "         0.30244303, -0.00457295, -0.04286622,  0.02138982,  0.00530803,\n",
       "         0.29666477, -0.05393067,  0.02607989,  0.19148545,  0.19381204,\n",
       "        -0.17002341, -0.2039206 , -0.03228488,  0.21686141,  0.36330122,\n",
       "        -0.05446185,  0.05279045,  0.00938943,  0.44950807,  0.51190287,\n",
       "         0.18247968, -0.24816023,  0.16192226,  0.22247782,  0.08192343],\n",
       "       dtype=float32),\n",
       " array([[ 1.6489486 , -0.7089941 , -1.2346727 , ...,  1.8393239 ,\n",
       "          0.9853549 , -0.49789736],\n",
       "        [ 0.07027561,  0.2928629 ,  0.47639856, ...,  0.13532634,\n",
       "          0.44966254,  0.04537895],\n",
       "        [ 1.7182113 ,  0.02583371,  0.12128189, ...,  1.1310043 ,\n",
       "          0.42128304, -0.00813743],\n",
       "        ...,\n",
       "        [ 1.3046597 , -0.59192485, -0.77792853, ...,  1.6298772 ,\n",
       "          0.85045904, -0.47232375],\n",
       "        [ 0.8492406 , -0.20796289, -0.68059045, ...,  1.2521324 ,\n",
       "          1.086745  ,  0.00358222],\n",
       "        [ 1.9500517 , -0.8569139 , -0.8962843 , ...,  2.3489783 ,\n",
       "          0.6932141 , -0.5867049 ]], dtype=float32),\n",
       " array([[ 0.1667333 , -0.12755781, -0.19424726, ..., -0.05673238,\n",
       "         -0.21036772, -0.06483285],\n",
       "        [ 0.06994133, -0.13548365, -0.11221536, ..., -0.05476404,\n",
       "         -0.06145276, -0.01023741],\n",
       "        [ 0.16769172, -0.03911728,  0.06490601, ...,  0.19798121,\n",
       "          0.06046074,  0.20825675],\n",
       "        ...,\n",
       "        [-0.07197175,  0.19512177, -0.11040291, ..., -0.14507918,\n",
       "          0.00316359, -0.20238876],\n",
       "        [ 0.11564764, -0.0730119 ,  0.10038939, ...,  0.11314248,\n",
       "          0.08238833, -0.00919655],\n",
       "        [-0.14443396, -0.02781468, -0.13133104, ..., -0.1186414 ,\n",
       "          0.12849827,  0.18185475]], dtype=float32),\n",
       " array([ 1.2002816 ,  1.6761043 ,  1.5527776 ,  1.1559973 ,  1.7639889 ,\n",
       "         0.57968265,  1.2702096 ,  1.2835634 ,  1.7678902 ,  0.3936505 ,\n",
       "         1.0792624 , -0.46430507,  0.59299755,  1.2548497 ,  0.5220455 ,\n",
       "         0.8329771 ,  0.14138407,  0.80164576,  0.71245265,  1.3908367 ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -0.07757986,  0.04843542,  0.4082183 , -0.08932582, -0.01722512,\n",
       "         0.20119457, -0.13946168,  0.24807346,  0.47009057,  0.07843743,\n",
       "        -0.01387331, -0.37127373,  0.10637566,  0.80818033,  0.18334761,\n",
       "         0.17827332,  0.01169273,  0.08725898,  0.29379892,  0.4501662 ,\n",
       "         1.16217   ,  1.7610501 ,  1.5517278 ,  1.2612286 ,  1.7089834 ,\n",
       "         0.5140728 ,  1.2583019 ,  1.3825297 ,  1.7433506 ,  0.37978804,\n",
       "         1.552953  ,  0.87324667,  0.66878   ,  1.169484  ,  0.53110296,\n",
       "         0.7878264 ,  0.15874738,  0.87668717,  0.8401586 ,  1.4048345 ],\n",
       "       dtype=float32),\n",
       " array([[-0.19739944,  0.8291713 ,  0.6972704 , ..., -0.18800834,\n",
       "          0.9353662 , -0.15397635],\n",
       "        [-0.32912976,  0.6618763 ,  1.0928627 , ..., -0.5915665 ,\n",
       "          0.64436334, -0.21949224],\n",
       "        [-0.67864656,  0.8918298 ,  1.2714071 , ..., -0.1856538 ,\n",
       "          0.5727209 , -0.45853215],\n",
       "        ...,\n",
       "        [-0.12893622,  1.3997787 ,  1.4242413 , ...,  0.29129052,\n",
       "          0.40210956, -0.10515608],\n",
       "        [-0.25442487,  0.52579004,  0.54084224, ..., -0.09000266,\n",
       "          0.09901588,  0.2479392 ],\n",
       "        [-0.6318381 ,  1.3845034 ,  1.4903516 , ..., -0.0661982 ,\n",
       "          0.03985325, -0.46046916]], dtype=float32),\n",
       " array([[ 0.31313697,  0.05127769, -0.1698887 , ...,  0.07881488,\n",
       "         -0.1441966 ,  0.02660509],\n",
       "        [ 0.0628736 , -0.15261996,  0.00042839, ..., -0.14956117,\n",
       "          0.20172027,  0.06169889],\n",
       "        [-0.02950642,  0.07292426,  0.03149007, ...,  0.03298488,\n",
       "          0.01216084,  0.09212239],\n",
       "        ...,\n",
       "        [ 0.10512948,  0.00894814,  0.1074644 , ..., -0.08891952,\n",
       "         -0.01276565,  0.04847427],\n",
       "        [ 0.13293323, -0.02759923,  0.12720494, ...,  0.01171077,\n",
       "          0.0139597 , -0.03516725],\n",
       "        [-0.21640362, -0.08079448,  0.14294675, ...,  0.03227579,\n",
       "         -0.1630678 , -0.13597901]], dtype=float32),\n",
       " array([-0.581203  ,  0.38060054,  0.66789246, -0.34620717,  0.5667286 ,\n",
       "         0.6707367 ,  0.11572653,  0.53209245, -0.08041092, -0.01483918,\n",
       "         0.39283136, -0.2942218 ,  0.55122066,  0.80787665,  0.6981702 ,\n",
       "         0.5133889 ,  0.1664578 , -0.32267228, -0.5116615 ,  0.50790685,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.10742956,  0.166976  ,  0.23316307,  0.17061083,  0.18655197,\n",
       "         0.152519  ,  0.25620335,  0.19168547, -0.15436356,  0.21851811,\n",
       "         0.24847633,  0.34576017,  0.12671804,  0.26212013,  0.20961803,\n",
       "         0.19996637,  0.14745133,  0.27781713,  0.20757203,  0.05576497,\n",
       "         0.7378612 ,  0.49811313,  0.7032206 , -0.05542592,  0.54156876,\n",
       "         0.6783238 , -0.3918589 , -0.33358815, -0.07404429, -0.21462485,\n",
       "         0.5132166 ,  0.73371583,  0.46014503, -0.30556804,  0.6811908 ,\n",
       "         0.48806527,  0.00788415,  0.22229439,  0.9037634 , -0.36705494],\n",
       "       dtype=float32),\n",
       " array([[ 0.5718589 , -0.5899301 ],\n",
       "        [-0.50253916,  0.5090939 ],\n",
       "        [-0.5702344 ,  0.51653564],\n",
       "        [ 0.6703092 , -0.62510175],\n",
       "        [-0.6926312 ,  0.6849102 ],\n",
       "        [-0.602106  ,  0.58189166],\n",
       "        [ 0.4660293 , -0.4765275 ],\n",
       "        [ 0.6979109 , -0.6577746 ],\n",
       "        [ 0.16184166,  0.16662195],\n",
       "        [ 0.71405286, -0.7361469 ],\n",
       "        [-0.5553362 ,  0.5329401 ],\n",
       "        [ 0.5420615 , -0.54624635],\n",
       "        [-0.4947048 ,  0.5228915 ],\n",
       "        [ 0.38222718, -0.36479187],\n",
       "        [-0.72416687,  0.6962692 ],\n",
       "        [-0.33346668,  0.40545362],\n",
       "        [ 1.0612464 , -1.1230634 ],\n",
       "        [ 0.5348277 , -0.54783976],\n",
       "        [ 0.5135993 , -0.52078485],\n",
       "        [ 0.7950331 , -0.7867301 ]], dtype=float32),\n",
       " array([-0.5665975 ,  0.57145214], dtype=float32)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.optimizers.RMSprop at 0x7f3ac3c14eb8>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3660645 , 0.6349361 ],\n",
       "       [0.0431054 , 0.9568307 ],\n",
       "       [0.8538095 , 0.14581214],\n",
       "       ...,\n",
       "       [0.14776681, 0.8518005 ],\n",
       "       [0.19093817, 0.80888367],\n",
       "       [0.99290085, 0.00700216]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict(X_train_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
