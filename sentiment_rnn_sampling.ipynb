{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.conda/envs/my_root/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "import keras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_E6oV3lV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29720</td>\n",
       "      <td>29720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  tweet\n",
       "label              \n",
       "0      29720  29720\n",
       "1       2242   2242"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_class_0, count_class_1 = df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_class_0 =  df.query('label==0')\n",
    "df_class_1 =  df.query('label==1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_under = pd.concat([df_class_0_under, df_class_1],ignore_index=True ,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4484, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_under.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  tweet\n",
       "label             \n",
       "0      2242   2242\n",
       "1      2242   2242"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_under.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_under['tweet']\n",
    "Y = df_under['label']\n",
    "Y_org = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@user we are   to be sponsors   #proud @user @user sat july 2nd '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "max_features = 10000\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', split=' ', lower=True, char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(X.values)\n",
    "X = tokenizer.texts_to_sequences(X.values)\n",
    "\n",
    "# add padding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X, maxlen=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' @user lmfao pathetic #soit   #growup #funny #noonethere #iknowwhoitis ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98±ð\\x9f\\x98±ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98±ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82â\\x80¦'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['tweet'], key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.conda/envs/my_root/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 95)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpKSCoSIiKSkAiEiIimpQIiISEr/H29g8tnzHKAsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "plt.xlabel(\"No. of components\")\n",
    "plt.ylabel(\"cummulative explained Variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(LSTM(units=40, activation='relu',return_sequences= True, input_shape=(None, 95)))\n",
    "classifier.add(Dropout(rate=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(LSTM(units=20, return_sequences= True,activation='relu'))\n",
    "classifier.add(Dropout(rate=0.2))\n",
    "classifier.add(LSTM(units=20,activation='relu'))\n",
    "classifier.add(Dropout(rate=0.2))\n",
    "classifier.add(Dense(units = 2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='rmsprop',metrics=['accuracy'],loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.conda/envs/my_root/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"\n",
      "/home/shivam/.conda/envs/my_root/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y_e = OneHotEncoder()\n",
    "Y_train_org = Y_train\n",
    "Y_test_org = Y_test\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "Y_test = Y_test.reshape(-1, 1)\n",
    "y_e.fit(Y_train)\n",
    "Y_train = y_e.transform(Y_train)\n",
    "Y_test = y_e.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3587 samples, validate on 897 samples\n",
      "Epoch 1/200\n",
      "3587/3587 [==============================] - 24s 7ms/step - loss: 0.6917 - acc: 0.5776 - val_loss: 0.6888 - val_acc: 0.6795\n",
      "Epoch 2/200\n",
      "3587/3587 [==============================] - 2s 442us/step - loss: 0.6804 - acc: 0.6560 - val_loss: 0.6782 - val_acc: 0.6789\n",
      "Epoch 3/200\n",
      "3587/3587 [==============================] - 2s 438us/step - loss: 0.6643 - acc: 0.6636 - val_loss: 0.6715 - val_acc: 0.6756\n",
      "Epoch 4/200\n",
      "3587/3587 [==============================] - 2s 436us/step - loss: 0.6520 - acc: 0.6623 - val_loss: 0.6586 - val_acc: 0.6812\n",
      "Epoch 5/200\n",
      "3587/3587 [==============================] - 2s 447us/step - loss: 0.6409 - acc: 0.6715 - val_loss: 0.6449 - val_acc: 0.6828\n",
      "Epoch 6/200\n",
      "3587/3587 [==============================] - 2s 431us/step - loss: 0.6356 - acc: 0.6769 - val_loss: 0.6386 - val_acc: 0.6834\n",
      "Epoch 7/200\n",
      "3587/3587 [==============================] - 2s 447us/step - loss: 0.6290 - acc: 0.6865 - val_loss: 0.6317 - val_acc: 0.6834\n",
      "Epoch 8/200\n",
      "3587/3587 [==============================] - 2s 441us/step - loss: 0.6233 - acc: 0.6805 - val_loss: 0.6268 - val_acc: 0.6867\n",
      "Epoch 9/200\n",
      "3587/3587 [==============================] - 2s 442us/step - loss: 0.6186 - acc: 0.6850 - val_loss: 0.6234 - val_acc: 0.6817\n",
      "Epoch 10/200\n",
      "3587/3587 [==============================] - 2s 446us/step - loss: 0.6146 - acc: 0.6879 - val_loss: 0.6193 - val_acc: 0.6784\n",
      "Epoch 11/200\n",
      "3587/3587 [==============================] - 2s 444us/step - loss: 0.6176 - acc: 0.6886 - val_loss: 0.6177 - val_acc: 0.6867\n",
      "Epoch 12/200\n",
      "3587/3587 [==============================] - 2s 439us/step - loss: 0.6067 - acc: 0.6931 - val_loss: 0.6176 - val_acc: 0.6851\n",
      "Epoch 13/200\n",
      "3587/3587 [==============================] - 2s 437us/step - loss: 0.6068 - acc: 0.6967 - val_loss: 0.6182 - val_acc: 0.6828\n",
      "Epoch 14/200\n",
      "3587/3587 [==============================] - 2s 456us/step - loss: 0.6056 - acc: 0.6939 - val_loss: 0.6224 - val_acc: 0.6795\n",
      "Epoch 15/200\n",
      "3587/3587 [==============================] - 2s 444us/step - loss: 0.6017 - acc: 0.6978 - val_loss: 0.6167 - val_acc: 0.6800\n",
      "Epoch 16/200\n",
      "3587/3587 [==============================] - 2s 458us/step - loss: 0.6016 - acc: 0.7016 - val_loss: 0.6195 - val_acc: 0.6834\n",
      "Epoch 17/200\n",
      "3587/3587 [==============================] - 2s 538us/step - loss: 0.5999 - acc: 0.6993 - val_loss: 0.6158 - val_acc: 0.6867\n",
      "Epoch 18/200\n",
      "3587/3587 [==============================] - 2s 479us/step - loss: 0.6012 - acc: 0.7039 - val_loss: 0.6173 - val_acc: 0.6851\n",
      "Epoch 19/200\n",
      "3587/3587 [==============================] - 2s 445us/step - loss: 0.5908 - acc: 0.7074 - val_loss: 0.6215 - val_acc: 0.6973\n",
      "Epoch 20/200\n",
      "3587/3587 [==============================] - 2s 469us/step - loss: 0.5956 - acc: 0.7089 - val_loss: 0.6182 - val_acc: 0.6990\n",
      "Epoch 21/200\n",
      "3587/3587 [==============================] - 2s 438us/step - loss: 0.5910 - acc: 0.7169 - val_loss: 0.6191 - val_acc: 0.6951\n",
      "Epoch 22/200\n",
      "3587/3587 [==============================] - 2s 457us/step - loss: 0.5860 - acc: 0.7116 - val_loss: 0.6159 - val_acc: 0.6945\n",
      "Epoch 23/200\n",
      "3587/3587 [==============================] - 2s 451us/step - loss: 0.5840 - acc: 0.7127 - val_loss: 0.6207 - val_acc: 0.7012\n",
      "Epoch 24/200\n",
      "3587/3587 [==============================] - 2s 452us/step - loss: 0.5819 - acc: 0.7117 - val_loss: 0.6166 - val_acc: 0.6918\n",
      "Epoch 25/200\n",
      "3587/3587 [==============================] - 2s 464us/step - loss: 0.5817 - acc: 0.7130 - val_loss: 0.6165 - val_acc: 0.6923\n",
      "Epoch 26/200\n",
      "3587/3587 [==============================] - 2s 464us/step - loss: 0.5817 - acc: 0.7134 - val_loss: 0.6156 - val_acc: 0.7023\n",
      "Epoch 27/200\n",
      "3587/3587 [==============================] - 2s 484us/step - loss: 0.5822 - acc: 0.7184 - val_loss: 0.6165 - val_acc: 0.6996\n",
      "Epoch 28/200\n",
      "3587/3587 [==============================] - 2s 451us/step - loss: 0.5749 - acc: 0.7215 - val_loss: 0.6221 - val_acc: 0.6973\n",
      "Epoch 29/200\n",
      "3587/3587 [==============================] - 2s 456us/step - loss: 0.5783 - acc: 0.7195 - val_loss: 0.6221 - val_acc: 0.7012\n",
      "Epoch 30/200\n",
      "3587/3587 [==============================] - 2s 456us/step - loss: 0.5790 - acc: 0.7198 - val_loss: 0.6150 - val_acc: 0.6968\n",
      "Epoch 31/200\n",
      "3587/3587 [==============================] - 2s 464us/step - loss: 0.5692 - acc: 0.7285 - val_loss: 0.6282 - val_acc: 0.7068\n",
      "Epoch 32/200\n",
      "3587/3587 [==============================] - 2s 436us/step - loss: 0.5698 - acc: 0.7283 - val_loss: 0.6292 - val_acc: 0.6951\n",
      "Epoch 33/200\n",
      "3587/3587 [==============================] - 2s 553us/step - loss: 0.5679 - acc: 0.7314 - val_loss: 0.6225 - val_acc: 0.6990\n",
      "Epoch 34/200\n",
      "3587/3587 [==============================] - 2s 451us/step - loss: 0.5652 - acc: 0.7338 - val_loss: 0.6261 - val_acc: 0.6957\n",
      "Epoch 35/200\n",
      "3587/3587 [==============================] - 2s 445us/step - loss: 0.5711 - acc: 0.7290 - val_loss: 0.6321 - val_acc: 0.6951\n",
      "Epoch 36/200\n",
      "3587/3587 [==============================] - 2s 440us/step - loss: 0.5592 - acc: 0.7359 - val_loss: 0.6394 - val_acc: 0.6934\n",
      "Epoch 37/200\n",
      "3587/3587 [==============================] - 2s 436us/step - loss: 0.5611 - acc: 0.7349 - val_loss: 0.6300 - val_acc: 0.6996\n",
      "Epoch 38/200\n",
      "3587/3587 [==============================] - 3s 782us/step - loss: 0.5655 - acc: 0.7282 - val_loss: 0.6399 - val_acc: 0.6940\n",
      "Epoch 39/200\n",
      "3587/3587 [==============================] - 2s 527us/step - loss: 0.5598 - acc: 0.7352 - val_loss: 0.6502 - val_acc: 0.6940\n",
      "Epoch 40/200\n",
      "3587/3587 [==============================] - 2s 473us/step - loss: 0.5618 - acc: 0.7359 - val_loss: 0.6489 - val_acc: 0.6884\n",
      "Epoch 41/200\n",
      "3587/3587 [==============================] - 2s 462us/step - loss: 0.5603 - acc: 0.7308 - val_loss: 0.6491 - val_acc: 0.6996\n",
      "Epoch 42/200\n",
      "3587/3587 [==============================] - 2s 526us/step - loss: 0.5679 - acc: 0.7336 - val_loss: 0.6477 - val_acc: 0.6945\n",
      "Epoch 43/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.5548 - acc: 0.7345 - val_loss: 0.6378 - val_acc: 0.6990\n",
      "Epoch 44/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.5574 - acc: 0.7357 - val_loss: 0.6454 - val_acc: 0.6990\n",
      "Epoch 45/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.5541 - acc: 0.7375 - val_loss: 0.6424 - val_acc: 0.6996\n",
      "Epoch 46/200\n",
      "3587/3587 [==============================] - 2s 475us/step - loss: 0.5589 - acc: 0.7331 - val_loss: 0.6434 - val_acc: 0.7001\n",
      "Epoch 47/200\n",
      "3587/3587 [==============================] - 2s 476us/step - loss: 0.5505 - acc: 0.7384 - val_loss: 0.6560 - val_acc: 0.6929\n",
      "Epoch 48/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.5487 - acc: 0.7481 - val_loss: 0.6593 - val_acc: 0.7001\n",
      "Epoch 49/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.5554 - acc: 0.7388 - val_loss: 0.6502 - val_acc: 0.6979\n",
      "Epoch 50/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.5482 - acc: 0.7392 - val_loss: 0.6521 - val_acc: 0.6918\n",
      "Epoch 51/200\n",
      "3587/3587 [==============================] - 2s 470us/step - loss: 0.5463 - acc: 0.7423 - val_loss: 0.6550 - val_acc: 0.6968\n",
      "Epoch 52/200\n",
      "3587/3587 [==============================] - 2s 479us/step - loss: 0.5514 - acc: 0.7425 - val_loss: 0.6543 - val_acc: 0.6973\n",
      "Epoch 53/200\n",
      "3587/3587 [==============================] - 2s 480us/step - loss: 0.5400 - acc: 0.7442 - val_loss: 0.6633 - val_acc: 0.6918\n",
      "Epoch 54/200\n",
      "3587/3587 [==============================] - 2s 467us/step - loss: 0.5412 - acc: 0.7467 - val_loss: 0.6576 - val_acc: 0.6934\n",
      "Epoch 55/200\n",
      "3587/3587 [==============================] - 2s 480us/step - loss: 0.5402 - acc: 0.7445 - val_loss: 0.6619 - val_acc: 0.6945\n",
      "Epoch 56/200\n",
      "3587/3587 [==============================] - 2s 473us/step - loss: 0.5391 - acc: 0.7477 - val_loss: 0.6602 - val_acc: 0.6884\n",
      "Epoch 57/200\n",
      "3587/3587 [==============================] - 2s 468us/step - loss: 0.5448 - acc: 0.7425 - val_loss: 0.6553 - val_acc: 0.6923\n",
      "Epoch 58/200\n",
      "3587/3587 [==============================] - 2s 469us/step - loss: 0.5358 - acc: 0.7513 - val_loss: 0.6714 - val_acc: 0.6934\n",
      "Epoch 59/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.5507 - acc: 0.7417 - val_loss: 0.6659 - val_acc: 0.6951\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3587/3587 [==============================] - 2s 465us/step - loss: 0.5392 - acc: 0.7490 - val_loss: 0.6851 - val_acc: 0.6996\n",
      "Epoch 61/200\n",
      "3587/3587 [==============================] - 2s 449us/step - loss: 0.5375 - acc: 0.7529 - val_loss: 0.6733 - val_acc: 0.6951\n",
      "Epoch 62/200\n",
      "3587/3587 [==============================] - 2s 449us/step - loss: 0.5321 - acc: 0.7554 - val_loss: 0.6625 - val_acc: 0.6962\n",
      "Epoch 63/200\n",
      "3587/3587 [==============================] - 2s 445us/step - loss: 0.5372 - acc: 0.7423 - val_loss: 0.6760 - val_acc: 0.6979\n",
      "Epoch 64/200\n",
      "3587/3587 [==============================] - 2s 446us/step - loss: 0.5324 - acc: 0.7545 - val_loss: 0.6688 - val_acc: 0.6912\n",
      "Epoch 65/200\n",
      "3587/3587 [==============================] - 2s 456us/step - loss: 0.5300 - acc: 0.7485 - val_loss: 0.6790 - val_acc: 0.6973\n",
      "Epoch 66/200\n",
      "3587/3587 [==============================] - 2s 466us/step - loss: 0.5321 - acc: 0.7543 - val_loss: 0.6615 - val_acc: 0.7035\n",
      "Epoch 67/200\n",
      "3587/3587 [==============================] - 2s 502us/step - loss: 0.5294 - acc: 0.7508 - val_loss: 0.6744 - val_acc: 0.7057\n",
      "Epoch 68/200\n",
      "3587/3587 [==============================] - 2s 490us/step - loss: 0.5314 - acc: 0.7551 - val_loss: 0.6615 - val_acc: 0.7029\n",
      "Epoch 69/200\n",
      "3587/3587 [==============================] - 2s 450us/step - loss: 0.5278 - acc: 0.7519 - val_loss: 0.6709 - val_acc: 0.7068\n",
      "Epoch 70/200\n",
      "3587/3587 [==============================] - 2s 458us/step - loss: 0.5341 - acc: 0.7524 - val_loss: 0.6704 - val_acc: 0.7079\n",
      "Epoch 71/200\n",
      "3587/3587 [==============================] - 2s 458us/step - loss: 0.5288 - acc: 0.7570 - val_loss: 0.6619 - val_acc: 0.7079\n",
      "Epoch 72/200\n",
      "3587/3587 [==============================] - 2s 460us/step - loss: 0.5189 - acc: 0.7630 - val_loss: 0.6566 - val_acc: 0.7101\n",
      "Epoch 73/200\n",
      "3587/3587 [==============================] - 2s 453us/step - loss: 0.5275 - acc: 0.7537 - val_loss: 0.6542 - val_acc: 0.7029\n",
      "Epoch 74/200\n",
      "3587/3587 [==============================] - 2s 601us/step - loss: 0.5215 - acc: 0.7579 - val_loss: 0.6565 - val_acc: 0.7057\n",
      "Epoch 75/200\n",
      "3587/3587 [==============================] - 2s 615us/step - loss: 0.5221 - acc: 0.7572 - val_loss: 0.6694 - val_acc: 0.7135\n",
      "Epoch 76/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.5195 - acc: 0.7703 - val_loss: 0.6740 - val_acc: 0.7124\n",
      "Epoch 77/200\n",
      "3587/3587 [==============================] - 2s 476us/step - loss: 0.5188 - acc: 0.7611 - val_loss: 0.6723 - val_acc: 0.7113\n",
      "Epoch 78/200\n",
      "3587/3587 [==============================] - 2s 473us/step - loss: 0.5278 - acc: 0.7580 - val_loss: 0.6604 - val_acc: 0.7079\n",
      "Epoch 79/200\n",
      "3587/3587 [==============================] - 2s 473us/step - loss: 0.5241 - acc: 0.7554 - val_loss: 0.6716 - val_acc: 0.7146\n",
      "Epoch 80/200\n",
      "3587/3587 [==============================] - 2s 512us/step - loss: 0.5108 - acc: 0.7693 - val_loss: 0.6554 - val_acc: 0.7168\n",
      "Epoch 81/200\n",
      "3587/3587 [==============================] - 2s 505us/step - loss: 0.5130 - acc: 0.7683 - val_loss: 0.6715 - val_acc: 0.7185\n",
      "Epoch 82/200\n",
      "3587/3587 [==============================] - 2s 451us/step - loss: 0.5195 - acc: 0.7694 - val_loss: 0.6600 - val_acc: 0.7157\n",
      "Epoch 83/200\n",
      "3587/3587 [==============================] - 2s 421us/step - loss: 0.5125 - acc: 0.7678 - val_loss: 0.6748 - val_acc: 0.7096\n",
      "Epoch 84/200\n",
      "3587/3587 [==============================] - 1s 398us/step - loss: 0.5143 - acc: 0.7651 - val_loss: 0.6606 - val_acc: 0.7168\n",
      "Epoch 85/200\n",
      "3587/3587 [==============================] - 1s 415us/step - loss: 0.5133 - acc: 0.7676 - val_loss: 0.6684 - val_acc: 0.7168\n",
      "Epoch 86/200\n",
      "3587/3587 [==============================] - 2s 484us/step - loss: 0.5161 - acc: 0.7602 - val_loss: 0.6747 - val_acc: 0.7113\n",
      "Epoch 87/200\n",
      "3587/3587 [==============================] - 2s 496us/step - loss: 0.5046 - acc: 0.7699 - val_loss: 0.6989 - val_acc: 0.7101\n",
      "Epoch 88/200\n",
      "3587/3587 [==============================] - 2s 475us/step - loss: 0.5103 - acc: 0.7724 - val_loss: 0.6696 - val_acc: 0.7179\n",
      "Epoch 89/200\n",
      "3587/3587 [==============================] - 2s 461us/step - loss: 0.5019 - acc: 0.7671 - val_loss: 0.6938 - val_acc: 0.7196\n",
      "Epoch 90/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.5061 - acc: 0.7669 - val_loss: 0.6690 - val_acc: 0.7179\n",
      "Epoch 91/200\n",
      "3587/3587 [==============================] - 2s 465us/step - loss: 0.5134 - acc: 0.7647 - val_loss: 0.6710 - val_acc: 0.7157\n",
      "Epoch 92/200\n",
      "3587/3587 [==============================] - 2s 462us/step - loss: 0.5049 - acc: 0.7714 - val_loss: 0.6907 - val_acc: 0.7191\n",
      "Epoch 93/200\n",
      "3587/3587 [==============================] - 2s 469us/step - loss: 0.5057 - acc: 0.7773 - val_loss: 0.6682 - val_acc: 0.7213\n",
      "Epoch 94/200\n",
      "3587/3587 [==============================] - 2s 464us/step - loss: 0.5005 - acc: 0.7704 - val_loss: 0.6677 - val_acc: 0.7146\n",
      "Epoch 95/200\n",
      "3587/3587 [==============================] - 2s 465us/step - loss: 0.5053 - acc: 0.7731 - val_loss: 0.6645 - val_acc: 0.7263\n",
      "Epoch 96/200\n",
      "3587/3587 [==============================] - 2s 467us/step - loss: 0.5018 - acc: 0.7676 - val_loss: 0.6556 - val_acc: 0.7241\n",
      "Epoch 97/200\n",
      "3587/3587 [==============================] - 2s 464us/step - loss: 0.5019 - acc: 0.7764 - val_loss: 0.6676 - val_acc: 0.7202\n",
      "Epoch 98/200\n",
      "3587/3587 [==============================] - 2s 462us/step - loss: 0.4876 - acc: 0.7785 - val_loss: 0.7014 - val_acc: 0.7241\n",
      "Epoch 99/200\n",
      "3587/3587 [==============================] - 2s 475us/step - loss: 0.4937 - acc: 0.7753 - val_loss: 0.6920 - val_acc: 0.7258\n",
      "Epoch 100/200\n",
      "3587/3587 [==============================] - 2s 474us/step - loss: 0.4936 - acc: 0.7793 - val_loss: 0.6749 - val_acc: 0.7246\n",
      "Epoch 101/200\n",
      "3587/3587 [==============================] - 2s 460us/step - loss: 0.4897 - acc: 0.7777 - val_loss: 0.6933 - val_acc: 0.7168\n",
      "Epoch 102/200\n",
      "3587/3587 [==============================] - 2s 466us/step - loss: 0.4985 - acc: 0.7679 - val_loss: 0.6930 - val_acc: 0.7191\n",
      "Epoch 103/200\n",
      "3587/3587 [==============================] - 2s 474us/step - loss: 0.4931 - acc: 0.7752 - val_loss: 0.6992 - val_acc: 0.7213\n",
      "Epoch 104/200\n",
      "3587/3587 [==============================] - 2s 470us/step - loss: 0.5006 - acc: 0.7685 - val_loss: 0.6727 - val_acc: 0.7224\n",
      "Epoch 105/200\n",
      "3587/3587 [==============================] - 2s 477us/step - loss: 0.5050 - acc: 0.7689 - val_loss: 0.6773 - val_acc: 0.7258\n",
      "Epoch 106/200\n",
      "3587/3587 [==============================] - 2s 474us/step - loss: 0.4860 - acc: 0.7810 - val_loss: 0.6780 - val_acc: 0.7274\n",
      "Epoch 107/200\n",
      "3587/3587 [==============================] - 2s 462us/step - loss: 0.4966 - acc: 0.7696 - val_loss: 0.6750 - val_acc: 0.7258\n",
      "Epoch 108/200\n",
      "3587/3587 [==============================] - 2s 468us/step - loss: 0.4990 - acc: 0.7729 - val_loss: 0.6852 - val_acc: 0.7152\n",
      "Epoch 109/200\n",
      "3587/3587 [==============================] - 2s 471us/step - loss: 0.4930 - acc: 0.7708 - val_loss: 0.6794 - val_acc: 0.7269\n",
      "Epoch 110/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.4900 - acc: 0.7853 - val_loss: 0.6843 - val_acc: 0.7280\n",
      "Epoch 111/200\n",
      "3587/3587 [==============================] - 2s 518us/step - loss: 0.4811 - acc: 0.7791 - val_loss: 0.6804 - val_acc: 0.7269\n",
      "Epoch 112/200\n",
      "3587/3587 [==============================] - 3s 719us/step - loss: 0.4925 - acc: 0.7759 - val_loss: 0.6848 - val_acc: 0.7252\n",
      "Epoch 113/200\n",
      "3587/3587 [==============================] - 2s 470us/step - loss: 0.4833 - acc: 0.7778 - val_loss: 0.6801 - val_acc: 0.7269\n",
      "Epoch 114/200\n",
      "3587/3587 [==============================] - 2s 531us/step - loss: 0.4887 - acc: 0.7754 - val_loss: 0.6815 - val_acc: 0.7224\n",
      "Epoch 115/200\n",
      "3587/3587 [==============================] - 2s 484us/step - loss: 0.4803 - acc: 0.7846 - val_loss: 0.6851 - val_acc: 0.7258\n",
      "Epoch 116/200\n",
      "3587/3587 [==============================] - 2s 488us/step - loss: 0.4852 - acc: 0.7806 - val_loss: 0.6817 - val_acc: 0.7241\n",
      "Epoch 117/200\n",
      "3587/3587 [==============================] - 2s 516us/step - loss: 0.4752 - acc: 0.7848 - val_loss: 0.7004 - val_acc: 0.7263\n",
      "Epoch 118/200\n",
      "3587/3587 [==============================] - 2s 556us/step - loss: 0.4785 - acc: 0.7827 - val_loss: 0.7049 - val_acc: 0.7269\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3587/3587 [==============================] - 2s 485us/step - loss: 0.4757 - acc: 0.7873 - val_loss: 0.6932 - val_acc: 0.7330\n",
      "Epoch 120/200\n",
      "3587/3587 [==============================] - 2s 424us/step - loss: 0.4699 - acc: 0.7898 - val_loss: 0.6862 - val_acc: 0.7319\n",
      "Epoch 121/200\n",
      "3587/3587 [==============================] - 2s 426us/step - loss: 0.4665 - acc: 0.7895 - val_loss: 0.7027 - val_acc: 0.7358\n",
      "Epoch 122/200\n",
      "3587/3587 [==============================] - 2s 431us/step - loss: 0.4750 - acc: 0.7817 - val_loss: 0.6907 - val_acc: 0.7358\n",
      "Epoch 123/200\n",
      "3587/3587 [==============================] - 2s 426us/step - loss: 0.4712 - acc: 0.7845 - val_loss: 0.7012 - val_acc: 0.7269\n",
      "Epoch 124/200\n",
      "3587/3587 [==============================] - 2s 424us/step - loss: 0.4785 - acc: 0.7825 - val_loss: 0.6800 - val_acc: 0.7263\n",
      "Epoch 125/200\n",
      "3587/3587 [==============================] - 1s 417us/step - loss: 0.4717 - acc: 0.7906 - val_loss: 0.6881 - val_acc: 0.7330\n",
      "Epoch 126/200\n",
      "3587/3587 [==============================] - 2s 429us/step - loss: 0.4740 - acc: 0.7849 - val_loss: 0.6808 - val_acc: 0.7324\n",
      "Epoch 127/200\n",
      "3587/3587 [==============================] - 2s 423us/step - loss: 0.4719 - acc: 0.7888 - val_loss: 0.6819 - val_acc: 0.7313\n",
      "Epoch 128/200\n",
      "3587/3587 [==============================] - 2s 426us/step - loss: 0.4794 - acc: 0.7858 - val_loss: 0.6915 - val_acc: 0.7358\n",
      "Epoch 129/200\n",
      "3587/3587 [==============================] - 2s 438us/step - loss: 0.4720 - acc: 0.7849 - val_loss: 0.6955 - val_acc: 0.7258\n",
      "Epoch 130/200\n",
      "3587/3587 [==============================] - 2s 428us/step - loss: 0.4711 - acc: 0.7866 - val_loss: 0.7043 - val_acc: 0.7274\n",
      "Epoch 131/200\n",
      "3587/3587 [==============================] - 2s 418us/step - loss: 0.4770 - acc: 0.7877 - val_loss: 0.6894 - val_acc: 0.7313\n",
      "Epoch 132/200\n",
      "3587/3587 [==============================] - 2s 425us/step - loss: 0.4683 - acc: 0.7856 - val_loss: 0.6954 - val_acc: 0.7330\n",
      "Epoch 133/200\n",
      "3587/3587 [==============================] - 2s 427us/step - loss: 0.4676 - acc: 0.7856 - val_loss: 0.6969 - val_acc: 0.7391\n",
      "Epoch 134/200\n",
      "3587/3587 [==============================] - 2s 424us/step - loss: 0.4642 - acc: 0.7869 - val_loss: 0.6906 - val_acc: 0.7419\n",
      "Epoch 135/200\n",
      "3587/3587 [==============================] - 2s 431us/step - loss: 0.4664 - acc: 0.7905 - val_loss: 0.6865 - val_acc: 0.7347\n",
      "Epoch 136/200\n",
      "3587/3587 [==============================] - 2s 433us/step - loss: 0.4719 - acc: 0.7906 - val_loss: 0.6818 - val_acc: 0.7391\n",
      "Epoch 137/200\n",
      "3587/3587 [==============================] - 2s 506us/step - loss: 0.4602 - acc: 0.7934 - val_loss: 0.7025 - val_acc: 0.7313\n",
      "Epoch 138/200\n",
      "3587/3587 [==============================] - 2s 491us/step - loss: 0.4645 - acc: 0.7983 - val_loss: 0.6829 - val_acc: 0.7358\n",
      "Epoch 139/200\n",
      "3587/3587 [==============================] - 2s 456us/step - loss: 0.4662 - acc: 0.7870 - val_loss: 0.7002 - val_acc: 0.7397\n",
      "Epoch 140/200\n",
      "3587/3587 [==============================] - 2s 455us/step - loss: 0.4647 - acc: 0.7915 - val_loss: 0.6966 - val_acc: 0.7341\n",
      "Epoch 141/200\n",
      "3587/3587 [==============================] - 2s 455us/step - loss: 0.4581 - acc: 0.7959 - val_loss: 0.6903 - val_acc: 0.7308\n",
      "Epoch 142/200\n",
      "3587/3587 [==============================] - 2s 473us/step - loss: 0.4636 - acc: 0.7990 - val_loss: 0.7093 - val_acc: 0.7363\n",
      "Epoch 143/200\n",
      "3587/3587 [==============================] - 2s 537us/step - loss: 0.4557 - acc: 0.7940 - val_loss: 0.7135 - val_acc: 0.7336\n",
      "Epoch 144/200\n",
      "3587/3587 [==============================] - 2s 471us/step - loss: 0.4535 - acc: 0.7998 - val_loss: 0.7029 - val_acc: 0.7347\n",
      "Epoch 145/200\n",
      "3587/3587 [==============================] - 2s 450us/step - loss: 0.4590 - acc: 0.7883 - val_loss: 0.7251 - val_acc: 0.7336\n",
      "Epoch 146/200\n",
      "3587/3587 [==============================] - 2s 457us/step - loss: 0.4629 - acc: 0.7950 - val_loss: 0.6986 - val_acc: 0.7402\n",
      "Epoch 147/200\n",
      "3587/3587 [==============================] - 2s 454us/step - loss: 0.4520 - acc: 0.7940 - val_loss: 0.7096 - val_acc: 0.7358\n",
      "Epoch 148/200\n",
      "3587/3587 [==============================] - 2s 455us/step - loss: 0.4513 - acc: 0.8036 - val_loss: 0.7259 - val_acc: 0.7447\n",
      "Epoch 149/200\n",
      "3587/3587 [==============================] - 2s 507us/step - loss: 0.4550 - acc: 0.7920 - val_loss: 0.6994 - val_acc: 0.7391\n",
      "Epoch 150/200\n",
      "3587/3587 [==============================] - 2s 639us/step - loss: 0.4513 - acc: 0.7897 - val_loss: 0.6980 - val_acc: 0.7408\n",
      "Epoch 151/200\n",
      "3587/3587 [==============================] - 2s 455us/step - loss: 0.4645 - acc: 0.7866 - val_loss: 0.6988 - val_acc: 0.7469\n",
      "Epoch 152/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.4524 - acc: 0.7947 - val_loss: 0.7101 - val_acc: 0.7480\n",
      "Epoch 153/200\n",
      "3587/3587 [==============================] - 2s 460us/step - loss: 0.4538 - acc: 0.7930 - val_loss: 0.7200 - val_acc: 0.7408\n",
      "Epoch 154/200\n",
      "3587/3587 [==============================] - 2s 489us/step - loss: 0.4556 - acc: 0.7936 - val_loss: 0.7024 - val_acc: 0.7447\n",
      "Epoch 155/200\n",
      "3587/3587 [==============================] - 2s 502us/step - loss: 0.4530 - acc: 0.7965 - val_loss: 0.7131 - val_acc: 0.7464\n",
      "Epoch 156/200\n",
      "3587/3587 [==============================] - 2s 492us/step - loss: 0.4520 - acc: 0.7923 - val_loss: 0.7298 - val_acc: 0.7497\n",
      "Epoch 157/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.4431 - acc: 0.7954 - val_loss: 0.7425 - val_acc: 0.7414\n",
      "Epoch 158/200\n",
      "3587/3587 [==============================] - 2s 459us/step - loss: 0.4395 - acc: 0.8022 - val_loss: 0.7504 - val_acc: 0.7441\n",
      "Epoch 159/200\n",
      "3587/3587 [==============================] - 2s 460us/step - loss: 0.4480 - acc: 0.7963 - val_loss: 0.7046 - val_acc: 0.7425\n",
      "Epoch 160/200\n",
      "3587/3587 [==============================] - 2s 465us/step - loss: 0.4551 - acc: 0.7934 - val_loss: 0.7169 - val_acc: 0.7369\n",
      "Epoch 161/200\n",
      "3587/3587 [==============================] - 2s 469us/step - loss: 0.4560 - acc: 0.7902 - val_loss: 0.7421 - val_acc: 0.7402\n",
      "Epoch 162/200\n",
      "3587/3587 [==============================] - 2s 458us/step - loss: 0.4376 - acc: 0.7951 - val_loss: 0.7381 - val_acc: 0.7480\n",
      "Epoch 163/200\n",
      "3587/3587 [==============================] - 2s 457us/step - loss: 0.4493 - acc: 0.8009 - val_loss: 0.7216 - val_acc: 0.7441\n",
      "Epoch 164/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.4476 - acc: 0.7975 - val_loss: 0.7075 - val_acc: 0.7492\n",
      "Epoch 165/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.4533 - acc: 0.7917 - val_loss: 0.7230 - val_acc: 0.7458\n",
      "Epoch 166/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.4408 - acc: 0.7972 - val_loss: 0.7321 - val_acc: 0.7469\n",
      "Epoch 167/200\n",
      "3587/3587 [==============================] - 2s 464us/step - loss: 0.4404 - acc: 0.8025 - val_loss: 0.7338 - val_acc: 0.7469\n",
      "Epoch 168/200\n",
      "3587/3587 [==============================] - 2s 462us/step - loss: 0.4426 - acc: 0.8026 - val_loss: 0.7075 - val_acc: 0.7497\n",
      "Epoch 169/200\n",
      "3587/3587 [==============================] - 2s 468us/step - loss: 0.4439 - acc: 0.7927 - val_loss: 0.7123 - val_acc: 0.7458\n",
      "Epoch 170/200\n",
      "3587/3587 [==============================] - 2s 462us/step - loss: 0.4387 - acc: 0.8054 - val_loss: 0.7222 - val_acc: 0.7547\n",
      "Epoch 171/200\n",
      "3587/3587 [==============================] - 2s 468us/step - loss: 0.4444 - acc: 0.8016 - val_loss: 0.7114 - val_acc: 0.7458\n",
      "Epoch 172/200\n",
      "3587/3587 [==============================] - 2s 472us/step - loss: 0.4370 - acc: 0.8055 - val_loss: 0.7308 - val_acc: 0.7492\n",
      "Epoch 173/200\n",
      "3587/3587 [==============================] - 2s 467us/step - loss: 0.4439 - acc: 0.7977 - val_loss: 0.7386 - val_acc: 0.7447\n",
      "Epoch 174/200\n",
      "3587/3587 [==============================] - 2s 471us/step - loss: 0.4424 - acc: 0.7970 - val_loss: 0.7175 - val_acc: 0.7469\n",
      "Epoch 175/200\n",
      "3587/3587 [==============================] - 2s 473us/step - loss: 0.4464 - acc: 0.8012 - val_loss: 0.7179 - val_acc: 0.7503\n",
      "Epoch 176/200\n",
      "3587/3587 [==============================] - 2s 463us/step - loss: 0.4384 - acc: 0.8036 - val_loss: 0.7137 - val_acc: 0.7480\n",
      "Epoch 177/200\n",
      "3587/3587 [==============================] - 2s 469us/step - loss: 0.4483 - acc: 0.7996 - val_loss: 0.7160 - val_acc: 0.7469\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3587/3587 [==============================] - 2s 435us/step - loss: 0.4254 - acc: 0.8107 - val_loss: 0.7202 - val_acc: 0.7514\n",
      "Epoch 179/200\n",
      "3587/3587 [==============================] - 2s 427us/step - loss: 0.4333 - acc: 0.8009 - val_loss: 0.7203 - val_acc: 0.7436\n",
      "Epoch 180/200\n",
      "3587/3587 [==============================] - 2s 477us/step - loss: 0.4348 - acc: 0.8022 - val_loss: 0.7256 - val_acc: 0.7458\n",
      "Epoch 181/200\n",
      "3587/3587 [==============================] - 2s 441us/step - loss: 0.4326 - acc: 0.8050 - val_loss: 0.7435 - val_acc: 0.7436\n",
      "Epoch 182/200\n",
      "3587/3587 [==============================] - 2s 448us/step - loss: 0.4353 - acc: 0.8090 - val_loss: 0.7137 - val_acc: 0.7492\n",
      "Epoch 183/200\n",
      "3587/3587 [==============================] - 2s 450us/step - loss: 0.4397 - acc: 0.8030 - val_loss: 0.7135 - val_acc: 0.7414\n",
      "Epoch 184/200\n",
      "3587/3587 [==============================] - 2s 434us/step - loss: 0.4374 - acc: 0.8015 - val_loss: 0.7265 - val_acc: 0.7453\n",
      "Epoch 185/200\n",
      "3587/3587 [==============================] - 2s 442us/step - loss: 0.4302 - acc: 0.8095 - val_loss: 0.7250 - val_acc: 0.7469\n",
      "Epoch 186/200\n",
      "3587/3587 [==============================] - 2s 442us/step - loss: 0.4362 - acc: 0.8068 - val_loss: 0.7171 - val_acc: 0.7458\n",
      "Epoch 187/200\n",
      "3587/3587 [==============================] - 2s 534us/step - loss: 0.4333 - acc: 0.7976 - val_loss: 0.7322 - val_acc: 0.7447\n",
      "Epoch 188/200\n",
      "3587/3587 [==============================] - 2s 568us/step - loss: 0.4280 - acc: 0.8081 - val_loss: 0.7366 - val_acc: 0.7475\n",
      "Epoch 189/200\n",
      "3587/3587 [==============================] - 2s 443us/step - loss: 0.4245 - acc: 0.8069 - val_loss: 0.7355 - val_acc: 0.7503\n",
      "Epoch 190/200\n",
      "3587/3587 [==============================] - 2s 439us/step - loss: 0.4263 - acc: 0.8053 - val_loss: 0.7523 - val_acc: 0.7447\n",
      "Epoch 191/200\n",
      "3587/3587 [==============================] - 2s 442us/step - loss: 0.4363 - acc: 0.8029 - val_loss: 0.7596 - val_acc: 0.7514\n",
      "Epoch 192/200\n",
      "3587/3587 [==============================] - 2s 441us/step - loss: 0.4399 - acc: 0.8058 - val_loss: 0.7222 - val_acc: 0.7425\n",
      "Epoch 193/200\n",
      "3587/3587 [==============================] - 2s 441us/step - loss: 0.4282 - acc: 0.8035 - val_loss: 0.7514 - val_acc: 0.7480\n",
      "Epoch 194/200\n",
      "3587/3587 [==============================] - 2s 444us/step - loss: 0.4273 - acc: 0.8081 - val_loss: 0.7412 - val_acc: 0.7492\n",
      "Epoch 195/200\n",
      "3587/3587 [==============================] - 2s 436us/step - loss: 0.4352 - acc: 0.8072 - val_loss: 0.7296 - val_acc: 0.7520\n",
      "Epoch 196/200\n",
      "3587/3587 [==============================] - 2s 446us/step - loss: 0.4207 - acc: 0.8097 - val_loss: 0.7612 - val_acc: 0.7458\n",
      "Epoch 197/200\n",
      "3587/3587 [==============================] - 2s 447us/step - loss: 0.4358 - acc: 0.8060 - val_loss: 0.7569 - val_acc: 0.7458\n",
      "Epoch 198/200\n",
      "3587/3587 [==============================] - 2s 446us/step - loss: 0.4304 - acc: 0.8040 - val_loss: 0.7226 - val_acc: 0.7536\n",
      "Epoch 199/200\n",
      "3587/3587 [==============================] - 2s 446us/step - loss: 0.4313 - acc: 0.8021 - val_loss: 0.7177 - val_acc: 0.7525\n",
      "Epoch 200/200\n",
      "3587/3587 [==============================] - 2s 446us/step - loss: 0.4328 - acc: 0.8076 - val_loss: 0.7202 - val_acc: 0.7536\n"
     ]
    }
   ],
   "source": [
    "X_train_lstm = np.reshape(X_train, (X_train.shape[0],1, X_train.shape[1]))\n",
    "X_test_lstm = np.reshape(X_test, (X_test.shape[0],1 ,X_test.shape[1]))\n",
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    checker = classifier.fit(X_train_lstm, Y_train, batch_size=32, epochs=200, validation_data = (X_test_lstm, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_test_label = classifier.predict(X_test_lstm)\n",
    "y_pred_test=np.argmax(Y_pred_test_label,axis =1)\n",
    "y_pred_test\n",
    "Y_pred_train_label = classifier.predict(X_train_lstm)\n",
    "y_pred_train = np.argmax(Y_pred_train_label,axis=1)\n",
    "Y_test_true = Y_test_org.astype(np.int)\n",
    "Y_train_true = Y_train_org.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.71      0.74       443\n",
      "          1       0.74      0.80      0.77       454\n",
      "\n",
      "avg / total       0.75      0.75      0.75       897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test_true,y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  \n",
      " [[1373  128]\n",
      " [ 426 1660]]\n",
      "\n",
      "Test:  \n",
      " [[315  93]\n",
      " [128 361]]\n"
     ]
    }
   ],
   "source": [
    "### for softmax function\n",
    "print(\"TRAIN:  \\n\",confusion_matrix(y_pred_train,Y_train_true))\n",
    "print(\"\\nTest:  \\n\",confusion_matrix(y_pred_test,Y_test_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.76      0.83      1799\n",
      "          1       0.80      0.93      0.86      1788\n",
      "\n",
      "avg / total       0.86      0.85      0.84      3587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train_true,y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.save(\"ann_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model(\"rnn_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 40)          21760     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 40)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 20)          4880      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 20)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 20)                3280      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 29,962\n",
      "Trainable params: 29,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.11310695,  0.140307  , -0.3356271 , ..., -0.16759749,\n",
       "         -0.2781836 , -0.9401908 ],\n",
       "        [ 0.09020896, -0.22769712, -0.06153519, ...,  0.2232761 ,\n",
       "         -0.14894578,  0.55924535],\n",
       "        [ 0.16288431,  0.32513678, -0.04923659, ..., -0.51932377,\n",
       "          0.24815445, -0.01057171],\n",
       "        ...,\n",
       "        [-0.04493226,  0.12000737,  0.09115861, ...,  0.12171158,\n",
       "          0.07524186,  0.14667505],\n",
       "        [ 0.03513572, -0.04742594, -0.13730517, ...,  0.04065315,\n",
       "          0.04260868,  0.11560479],\n",
       "        [ 0.10513681, -0.07802351,  0.00621991, ...,  0.0745271 ,\n",
       "         -0.06921871,  0.05683559]], dtype=float32),\n",
       " array([[-0.06442357, -0.02605159, -0.04323145, ..., -0.04987266,\n",
       "          0.06833354, -0.14754143],\n",
       "        [-0.05491902,  0.09304494,  0.03452028, ..., -0.03596656,\n",
       "         -0.02188935, -0.08795702],\n",
       "        [-0.00672218, -0.0197031 , -0.10169733, ..., -0.02838954,\n",
       "         -0.02927258, -0.11133979],\n",
       "        ...,\n",
       "        [ 0.00604066,  0.01927673,  0.05301465, ...,  0.05123032,\n",
       "         -0.07815466,  0.01054049],\n",
       "        [-0.10036889, -0.03800025, -0.03497114, ..., -0.04158446,\n",
       "          0.11868522, -0.09212317],\n",
       "        [-0.01799201, -0.15533476, -0.02820942, ...,  0.0782618 ,\n",
       "         -0.05937856,  0.04444469]], dtype=float32),\n",
       " array([ 4.20623034e-01, -2.01468125e-01,  1.58369597e-02,  1.45036457e-02,\n",
       "        -4.54615429e-02, -2.29406402e-01,  7.27523044e-02, -1.40669256e-01,\n",
       "         1.49086133e-01,  4.46907848e-01,  9.28760245e-02, -2.02501237e-01,\n",
       "         1.57948077e-01, -2.76014894e-01,  1.13218971e-01,  3.13112140e-01,\n",
       "         6.22682869e-01,  2.23360270e-01,  4.29427326e-01, -8.41634646e-02,\n",
       "        -1.05229184e-01,  8.44428167e-02,  1.66654840e-01,  2.58649051e-01,\n",
       "         2.22636923e-01,  4.52513210e-02,  1.76872667e-02,  4.08268809e-01,\n",
       "        -1.83693305e-01,  1.77092887e-02,  1.22727357e-01, -7.47001097e-02,\n",
       "         1.11699728e-02,  8.10248926e-02, -3.31679344e-01,  2.62796432e-01,\n",
       "         9.26800519e-02,  1.82952121e-01, -2.73341350e-02,  1.48035690e-01,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        -5.51410496e-01, -1.15632391e+00, -2.84151196e-01, -5.34445584e-01,\n",
       "        -3.76857400e-01,  3.37132476e-02,  4.87302877e-02, -3.56226861e-01,\n",
       "        -4.85349864e-01, -3.08822900e-01, -4.22528058e-01, -7.16236457e-02,\n",
       "        -2.74454653e-01, -3.57588470e-01,  2.33579785e-01,  2.26970956e-01,\n",
       "        -4.90770549e-01, -1.22373201e-01, -4.11841244e-01, -4.47001666e-01,\n",
       "        -5.37956595e-01, -3.90740752e-01, -3.26634765e-01, -3.39296997e-01,\n",
       "        -5.40999353e-01, -3.50490883e-02, -4.47388649e-01, -3.05249572e-01,\n",
       "        -4.15806562e-01,  2.12655410e-01, -4.52137738e-01, -4.02470142e-01,\n",
       "        -3.42949003e-01, -2.72471488e-01, -3.45744938e-01, -1.02518296e+00,\n",
       "        -6.33296788e-01, -3.60647172e-01, -4.81465042e-01, -2.59548306e-01,\n",
       "        -5.26147112e-02, -4.52939458e-02,  2.35905088e-02, -3.81200425e-02,\n",
       "        -4.01212573e-02, -2.64013141e-01, -1.09112384e-02,  4.15720731e-01,\n",
       "        -1.95316285e-01,  3.68796699e-02,  3.53153735e-01, -3.51386160e-01,\n",
       "        -7.28761181e-02, -2.59419501e-01, -1.06260225e-01,  2.80846328e-01,\n",
       "         4.33320701e-01,  3.10938716e-01,  1.28015652e-01, -1.62171438e-01,\n",
       "        -1.00210784e-02,  3.55001748e-01,  2.08439127e-01,  5.82513697e-02,\n",
       "         3.60021234e-01, -4.63809408e-02,  2.59331107e-01, -6.52229413e-02,\n",
       "         9.21268538e-02,  1.01620273e-03,  1.01859637e-01,  9.93843824e-02,\n",
       "         9.53297392e-02,  2.09815770e-01, -5.36023736e-01,  4.74572815e-02,\n",
       "         3.68876517e-01,  2.42652185e-02,  1.50286406e-01,  1.90501794e-01],\n",
       "       dtype=float32),\n",
       " array([[ 1.0694517 ,  0.47651604,  0.9122786 , ...,  1.5697391 ,\n",
       "          1.4753311 ,  1.7700303 ],\n",
       "        [ 1.474063  ,  2.1041179 , -0.42140743, ...,  0.6526925 ,\n",
       "          0.44614664,  0.33262375],\n",
       "        [ 0.41222838,  0.28946212,  0.48911744, ...,  0.96431166,\n",
       "          0.7717971 ,  1.8378046 ],\n",
       "        ...,\n",
       "        [ 0.3425941 ,  0.28660914,  0.8028868 , ...,  0.7759061 ,\n",
       "          0.7749513 ,  1.5162458 ],\n",
       "        [ 0.45317852,  0.633058  ,  0.46525264, ...,  0.9492276 ,\n",
       "          0.9937012 ,  1.417984  ],\n",
       "        [-0.87419826,  0.99150157,  0.7165898 , ...,  1.4334797 ,\n",
       "          1.2816664 ,  0.8063989 ]], dtype=float32),\n",
       " array([[-0.05740755, -0.02741251,  0.13994667, ..., -0.06050767,\n",
       "          0.188733  , -0.14142932],\n",
       "        [ 0.0680439 , -0.01097375,  0.03211001, ..., -0.0138363 ,\n",
       "         -0.03413808,  0.01741688],\n",
       "        [ 0.1815692 , -0.15446973, -0.00512208, ..., -0.00627421,\n",
       "          0.10506888,  0.10432336],\n",
       "        ...,\n",
       "        [ 0.00486669, -0.2766275 , -0.04290835, ..., -0.07050695,\n",
       "          0.155536  ,  0.01816445],\n",
       "        [ 0.08928416, -0.10194883,  0.02102962, ..., -0.16823144,\n",
       "          0.06083879, -0.05720262],\n",
       "        [ 0.1739068 ,  0.03076227,  0.03622082, ...,  0.05897458,\n",
       "          0.10928333, -0.05107159]], dtype=float32),\n",
       " array([ 0.5454316 ,  0.28554958,  0.40638417,  1.177794  ,  0.23251544,\n",
       "         1.0706966 ,  0.5881349 ,  0.73756975,  1.0493253 ,  0.4289327 ,\n",
       "         0.6997997 ,  0.58141404,  0.5374424 ,  0.48935813,  1.1925961 ,\n",
       "         0.42582285,  1.3645506 ,  0.6367456 ,  0.35805723, -0.4125383 ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.34778467, -0.676124  , -0.01862124,  0.4654709 , -0.26900452,\n",
       "         0.03851245, -0.19524665,  0.29707468, -0.20187667, -0.06308125,\n",
       "         0.00202777,  0.01002238, -0.04576936, -0.5982975 , -0.01908175,\n",
       "        -0.06456482,  0.19258231, -0.32061496, -0.2006031 ,  0.0187201 ,\n",
       "         0.5954352 ,  0.16017967,  1.0628523 ,  0.6776288 ,  0.10892668,\n",
       "         0.9761598 , -0.1923025 ,  0.6239593 ,  1.0110875 ,  0.49295187,\n",
       "         0.7291411 ,  1.0959413 ,  0.3695808 ,  0.98426276,  1.3770982 ,\n",
       "         0.36649373,  1.2823684 ,  0.65374655,  0.3766938 , -0.37756357],\n",
       "       dtype=float32),\n",
       " array([[ 0.3713969 ,  0.23629764, -0.15179022, ..., -0.0624232 ,\n",
       "          0.19957738,  0.29300308],\n",
       "        [ 1.6277709 ,  1.8191054 ,  0.4799076 , ...,  0.33747882,\n",
       "          2.078467  ,  1.858322  ],\n",
       "        [ 0.7855366 ,  0.7050484 ,  0.7507111 , ..., -0.16205837,\n",
       "          0.7028538 ,  0.64769375],\n",
       "        ...,\n",
       "        [ 0.53515416,  0.5900962 ,  1.2902826 , ..., -0.03843456,\n",
       "          0.6997473 ,  0.97617   ],\n",
       "        [ 0.17112708,  0.4504235 ,  0.89944005, ..., -0.21613576,\n",
       "          0.73396635,  0.73148453],\n",
       "        [ 0.9454183 ,  1.2813138 ,  0.79385513, ...,  0.06926814,\n",
       "          1.4261553 ,  1.2561504 ]], dtype=float32),\n",
       " array([[ 0.00311221,  0.05533476,  0.01300392, ..., -0.10385159,\n",
       "         -0.0663966 , -0.08943792],\n",
       "        [-0.01974344, -0.06016648,  0.11880335, ...,  0.10701304,\n",
       "         -0.01709785, -0.16351628],\n",
       "        [-0.02195874,  0.04741657, -0.09600414, ...,  0.02926966,\n",
       "         -0.01123678, -0.01416242],\n",
       "        ...,\n",
       "        [-0.11510451,  0.03390775, -0.00512377, ...,  0.25055382,\n",
       "         -0.15562992,  0.03975494],\n",
       "        [ 0.04651685, -0.07025083, -0.08457635, ..., -0.07941562,\n",
       "         -0.01911857, -0.1544117 ],\n",
       "        [ 0.13249977, -0.04983875,  0.08142719, ...,  0.26016104,\n",
       "          0.25623247,  0.03508294]], dtype=float32),\n",
       " array([ 0.74338126,  0.74525017,  0.701561  , -0.16391452,  0.47345337,\n",
       "         0.83358043,  1.0156054 ,  0.51100755,  0.7006597 ,  0.7449891 ,\n",
       "         0.8086828 ,  0.84018064,  0.34716743, -0.0696771 ,  0.4546222 ,\n",
       "         0.47205663,  0.63089645,  0.6464181 ,  0.76245433,  0.7568718 ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.3594939 ,  0.26512507, -0.16785727, -0.11959606,  0.25758225,\n",
       "         0.32100034,  0.2600735 , -0.10818887,  0.27112222, -0.14595649,\n",
       "         0.1811912 ,  0.28781155, -0.0981876 , -0.00203791, -0.09274025,\n",
       "         0.24824205,  0.38090527, -0.12070718,  0.19371544,  0.22652812,\n",
       "         0.7317925 ,  0.8783711 , -0.29322293,  0.6715073 ,  0.52921224,\n",
       "         0.84553033,  0.9994223 , -0.2298165 ,  0.80972767, -0.3777241 ,\n",
       "         0.785841  ,  0.80919856,  0.34604874,  0.69066143, -0.3989177 ,\n",
       "         0.5539688 ,  0.67827886, -0.32303438,  0.8211932 ,  0.75055313],\n",
       "       dtype=float32),\n",
       " array([[-0.31718442,  0.30250344],\n",
       "        [-0.4439642 ,  0.41014504],\n",
       "        [ 0.5382883 , -0.54598624],\n",
       "        [ 0.5324978 , -0.5011728 ],\n",
       "        [ 0.5204823 , -0.540388  ],\n",
       "        [-0.43841475,  0.39302057],\n",
       "        [-0.469857  ,  0.51162785],\n",
       "        [ 0.46131444, -0.43413433],\n",
       "        [-0.41136312,  0.39434382],\n",
       "        [ 0.609403  , -0.67656255],\n",
       "        [-0.58317757,  0.46610522],\n",
       "        [-0.4299733 ,  0.4112976 ],\n",
       "        [ 0.46788213, -0.48610708],\n",
       "        [ 0.29167026, -0.3062448 ],\n",
       "        [ 0.38941574, -0.36299443],\n",
       "        [-0.53044   ,  0.54780114],\n",
       "        [ 0.18571456, -0.1774876 ],\n",
       "        [ 0.46554726, -0.4738565 ],\n",
       "        [-0.6030028 ,  0.65128   ],\n",
       "        [-0.34616604,  0.4812819 ]], dtype=float32),\n",
       " array([-0.27757105,  0.2794262 ], dtype=float32)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.optimizers.RMSprop at 0x7f0e72531d30>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9880548 , 0.0117742 ],\n",
       "       [0.34999084, 0.64975554],\n",
       "       [0.25240347, 0.7473473 ],\n",
       "       ...,\n",
       "       [0.30437657, 0.6952641 ],\n",
       "       [0.18941046, 0.81051505],\n",
       "       [0.2679293 , 0.73183596]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict(X_train_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
