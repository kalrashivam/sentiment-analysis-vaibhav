{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "import keras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_E6oV3lV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29720</td>\n",
       "      <td>29720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  tweet\n",
       "label              \n",
       "0      29720  29720\n",
       "1       2242   2242"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_class_0, count_class_1 = df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_class_0 =  df.query('label==0')\n",
    "df_class_1 =  df.query('label==1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_under = pd.concat([df_class_0_under, df_class_1],ignore_index=True ,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4484, 3)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_under.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  tweet\n",
       "label             \n",
       "0      2242   2242\n",
       "1      2242   2242"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_under.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_under['tweet']\n",
    "Y = df_under['label']\n",
    "Y_org = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "max_features = 10000\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', split=' ', lower=True, char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(X.values)\n",
    "X = tokenizer.texts_to_sequences(X.values)\n",
    "\n",
    "# add padding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X, maxlen=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' @user lmfao pathetic #soit   #growup #funny #noonethere #iknowwhoitis ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98±ð\\x9f\\x98±ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98±ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82â\\x80¦'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['tweet'], key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.conda/envs/my_root/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 35)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa70fb76668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "plt.xlabel(\"No. of components\")\n",
    "plt.ylabel(\"cummulative explained Variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(LSTM(units=40, activation='relu',return_sequences= True, input_shape=(None, 35)))\n",
    "classifier.add(Dropout(rate=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(LSTM(units=20, return_sequences= True,activation='relu'))\n",
    "classifier.add(Dropout(rate=0.2))\n",
    "classifier.add(LSTM(units=20,activation='relu'))\n",
    "classifier.add(Dropout(rate=0.2))\n",
    "classifier.add(Dense(units = 2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='rmsprop',metrics=['accuracy'],loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.conda/envs/my_root/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"\n",
      "/home/shivam/.conda/envs/my_root/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y_e = OneHotEncoder()\n",
    "Y_train_org = Y_train\n",
    "Y_test_org = Y_test\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "Y_test = Y_test.reshape(-1, 1)\n",
    "y_e.fit(Y_train)\n",
    "Y_train = y_e.transform(Y_train)\n",
    "Y_test = y_e.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3587 samples, validate on 897 samples\n",
      "Epoch 1/200\n",
      "3587/3587 [==============================] - 4s 1ms/step - loss: 0.6913 - acc: 0.5362 - val_loss: 0.6879 - val_acc: 0.6700\n",
      "Epoch 2/200\n",
      "3587/3587 [==============================] - 1s 384us/step - loss: 0.6810 - acc: 0.6694 - val_loss: 0.6718 - val_acc: 0.6767\n",
      "Epoch 3/200\n",
      "3587/3587 [==============================] - 2s 423us/step - loss: 0.6652 - acc: 0.6892 - val_loss: 0.6527 - val_acc: 0.6862\n",
      "Epoch 4/200\n",
      "3587/3587 [==============================] - 1s 388us/step - loss: 0.6393 - acc: 0.6960 - val_loss: 0.6360 - val_acc: 0.6901\n",
      "Epoch 5/200\n",
      "3587/3587 [==============================] - 1s 388us/step - loss: 0.6270 - acc: 0.6991 - val_loss: 0.6273 - val_acc: 0.6890\n",
      "Epoch 6/200\n",
      "3587/3587 [==============================] - 1s 389us/step - loss: 0.6194 - acc: 0.7021 - val_loss: 0.6228 - val_acc: 0.6912\n",
      "Epoch 7/200\n",
      "3587/3587 [==============================] - 1s 391us/step - loss: 0.6153 - acc: 0.7066 - val_loss: 0.6173 - val_acc: 0.6979\n",
      "Epoch 8/200\n",
      "3587/3587 [==============================] - 2s 441us/step - loss: 0.6067 - acc: 0.7117 - val_loss: 0.6155 - val_acc: 0.6945\n",
      "Epoch 9/200\n",
      "3587/3587 [==============================] - 2s 435us/step - loss: 0.6075 - acc: 0.7084 - val_loss: 0.6131 - val_acc: 0.7029\n",
      "Epoch 10/200\n",
      "3587/3587 [==============================] - 2s 418us/step - loss: 0.5968 - acc: 0.7147 - val_loss: 0.6114 - val_acc: 0.7090\n",
      "Epoch 11/200\n",
      "3587/3587 [==============================] - 1s 385us/step - loss: 0.6003 - acc: 0.7108 - val_loss: 0.6111 - val_acc: 0.7129\n",
      "Epoch 12/200\n",
      "3587/3587 [==============================] - 1s 387us/step - loss: 0.5974 - acc: 0.7179 - val_loss: 0.6095 - val_acc: 0.7113\n",
      "Epoch 13/200\n",
      "3587/3587 [==============================] - 1s 390us/step - loss: 0.5877 - acc: 0.7223 - val_loss: 0.6068 - val_acc: 0.7146\n",
      "Epoch 14/200\n",
      "3587/3587 [==============================] - 2s 448us/step - loss: 0.5882 - acc: 0.7237 - val_loss: 0.6064 - val_acc: 0.7168\n",
      "Epoch 15/200\n",
      "3587/3587 [==============================] - 2s 562us/step - loss: 0.5851 - acc: 0.7243 - val_loss: 0.6042 - val_acc: 0.7185\n",
      "Epoch 16/200\n",
      "3587/3587 [==============================] - 1s 398us/step - loss: 0.5839 - acc: 0.7204 - val_loss: 0.6075 - val_acc: 0.7135\n",
      "Epoch 17/200\n",
      "3587/3587 [==============================] - 1s 398us/step - loss: 0.5808 - acc: 0.7264 - val_loss: 0.6048 - val_acc: 0.7062\n",
      "Epoch 18/200\n",
      "3587/3587 [==============================] - 1s 402us/step - loss: 0.5793 - acc: 0.7254 - val_loss: 0.6041 - val_acc: 0.7146\n",
      "Epoch 19/200\n",
      "3587/3587 [==============================] - 1s 396us/step - loss: 0.5767 - acc: 0.7272 - val_loss: 0.6035 - val_acc: 0.7152\n",
      "Epoch 20/200\n",
      "3587/3587 [==============================] - 1s 400us/step - loss: 0.5735 - acc: 0.7299 - val_loss: 0.6004 - val_acc: 0.7157\n",
      "Epoch 21/200\n",
      "3587/3587 [==============================] - 1s 393us/step - loss: 0.5760 - acc: 0.7306 - val_loss: 0.6008 - val_acc: 0.7124\n",
      "Epoch 22/200\n",
      "3587/3587 [==============================] - 1s 405us/step - loss: 0.5665 - acc: 0.7361 - val_loss: 0.6024 - val_acc: 0.7157\n",
      "Epoch 23/200\n",
      "3587/3587 [==============================] - 1s 399us/step - loss: 0.5684 - acc: 0.7431 - val_loss: 0.6008 - val_acc: 0.7152\n",
      "Epoch 24/200\n",
      "3587/3587 [==============================] - 1s 398us/step - loss: 0.5634 - acc: 0.7406 - val_loss: 0.5983 - val_acc: 0.7101\n",
      "Epoch 25/200\n",
      "3587/3587 [==============================] - 2s 466us/step - loss: 0.5619 - acc: 0.7407 - val_loss: 0.5983 - val_acc: 0.7113\n",
      "Epoch 26/200\n",
      "3587/3587 [==============================] - 1s 400us/step - loss: 0.5564 - acc: 0.7442 - val_loss: 0.6015 - val_acc: 0.7179\n",
      "Epoch 27/200\n",
      "3587/3587 [==============================] - 1s 407us/step - loss: 0.5607 - acc: 0.7402 - val_loss: 0.5993 - val_acc: 0.7107\n",
      "Epoch 28/200\n",
      "3587/3587 [==============================] - 1s 416us/step - loss: 0.5560 - acc: 0.7464 - val_loss: 0.5967 - val_acc: 0.7129\n",
      "Epoch 29/200\n",
      "3587/3587 [==============================] - 1s 405us/step - loss: 0.5546 - acc: 0.7497 - val_loss: 0.5993 - val_acc: 0.7146\n",
      "Epoch 30/200\n",
      "3587/3587 [==============================] - 1s 408us/step - loss: 0.5502 - acc: 0.7573 - val_loss: 0.5996 - val_acc: 0.7168\n",
      "Epoch 31/200\n",
      "3587/3587 [==============================] - 1s 406us/step - loss: 0.5479 - acc: 0.7498 - val_loss: 0.6047 - val_acc: 0.7129\n",
      "Epoch 32/200\n",
      "3587/3587 [==============================] - 2s 424us/step - loss: 0.5520 - acc: 0.7510 - val_loss: 0.6002 - val_acc: 0.7174\n",
      "Epoch 33/200\n",
      "3587/3587 [==============================] - 1s 412us/step - loss: 0.5505 - acc: 0.7464 - val_loss: 0.6049 - val_acc: 0.7146\n",
      "Epoch 34/200\n",
      "3587/3587 [==============================] - 1s 403us/step - loss: 0.5508 - acc: 0.7501 - val_loss: 0.6043 - val_acc: 0.7140\n",
      "Epoch 35/200\n",
      "3587/3587 [==============================] - 1s 410us/step - loss: 0.5548 - acc: 0.7502 - val_loss: 0.5982 - val_acc: 0.7163\n",
      "Epoch 36/200\n",
      "3587/3587 [==============================] - 1s 416us/step - loss: 0.5390 - acc: 0.7561 - val_loss: 0.6042 - val_acc: 0.7185\n",
      "Epoch 37/200\n",
      "3587/3587 [==============================] - 2s 448us/step - loss: 0.5416 - acc: 0.7582 - val_loss: 0.6046 - val_acc: 0.7241\n",
      "Epoch 38/200\n",
      "3587/3587 [==============================] - 2s 420us/step - loss: 0.5390 - acc: 0.7555 - val_loss: 0.6104 - val_acc: 0.7168\n",
      "Epoch 39/200\n",
      "3587/3587 [==============================] - 2s 423us/step - loss: 0.5344 - acc: 0.7629 - val_loss: 0.6110 - val_acc: 0.7179\n",
      "Epoch 40/200\n",
      "3587/3587 [==============================] - 2s 425us/step - loss: 0.5354 - acc: 0.7633 - val_loss: 0.6090 - val_acc: 0.7157\n",
      "Epoch 41/200\n",
      "3587/3587 [==============================] - 2s 427us/step - loss: 0.5377 - acc: 0.7583 - val_loss: 0.6057 - val_acc: 0.7157\n",
      "Epoch 42/200\n",
      "3587/3587 [==============================] - 2s 421us/step - loss: 0.5340 - acc: 0.7570 - val_loss: 0.6173 - val_acc: 0.7179\n",
      "Epoch 43/200\n",
      "3587/3587 [==============================] - 2s 423us/step - loss: 0.5353 - acc: 0.7647 - val_loss: 0.6131 - val_acc: 0.7235\n",
      "Epoch 44/200\n",
      "3587/3587 [==============================] - 2s 423us/step - loss: 0.5308 - acc: 0.7658 - val_loss: 0.6099 - val_acc: 0.7152\n",
      "Epoch 45/200\n",
      "3587/3587 [==============================] - 2s 440us/step - loss: 0.5268 - acc: 0.7679 - val_loss: 0.6136 - val_acc: 0.7135\n",
      "Epoch 46/200\n",
      "3587/3587 [==============================] - 2s 420us/step - loss: 0.5335 - acc: 0.7590 - val_loss: 0.6106 - val_acc: 0.7140\n",
      "Epoch 47/200\n",
      "3587/3587 [==============================] - 2s 422us/step - loss: 0.5248 - acc: 0.7655 - val_loss: 0.6150 - val_acc: 0.7191\n",
      "Epoch 48/200\n",
      "3587/3587 [==============================] - 2s 420us/step - loss: 0.5250 - acc: 0.7657 - val_loss: 0.6140 - val_acc: 0.7163\n",
      "Epoch 49/200\n",
      "3587/3587 [==============================] - 2s 423us/step - loss: 0.5230 - acc: 0.7729 - val_loss: 0.6186 - val_acc: 0.7185\n",
      "Epoch 50/200\n",
      "3587/3587 [==============================] - 2s 423us/step - loss: 0.5250 - acc: 0.7697 - val_loss: 0.6183 - val_acc: 0.7113\n",
      "Epoch 51/200\n",
      "3587/3587 [==============================] - 1s 414us/step - loss: 0.5257 - acc: 0.7681 - val_loss: 0.6198 - val_acc: 0.7113\n",
      "Epoch 52/200\n",
      "3587/3587 [==============================] - 2s 418us/step - loss: 0.5194 - acc: 0.7685 - val_loss: 0.6195 - val_acc: 0.7168\n",
      "Epoch 53/200\n",
      "3587/3587 [==============================] - 2s 422us/step - loss: 0.5210 - acc: 0.7753 - val_loss: 0.6128 - val_acc: 0.7174\n",
      "Epoch 54/200\n",
      "3587/3587 [==============================] - 2s 428us/step - loss: 0.5250 - acc: 0.7710 - val_loss: 0.6093 - val_acc: 0.7202\n",
      "Epoch 55/200\n",
      "3587/3587 [==============================] - 2s 422us/step - loss: 0.5140 - acc: 0.7735 - val_loss: 0.6201 - val_acc: 0.7090\n",
      "Epoch 56/200\n",
      "3587/3587 [==============================] - 2s 504us/step - loss: 0.5082 - acc: 0.7770 - val_loss: 0.6322 - val_acc: 0.7113\n",
      "Epoch 57/200\n",
      "3587/3587 [==============================] - 2s 520us/step - loss: 0.5202 - acc: 0.7696 - val_loss: 0.6237 - val_acc: 0.7146\n",
      "Epoch 58/200\n",
      "3587/3587 [==============================] - 2s 426us/step - loss: 0.5145 - acc: 0.7761 - val_loss: 0.6249 - val_acc: 0.7168\n",
      "Epoch 59/200\n",
      "3587/3587 [==============================] - 1s 413us/step - loss: 0.5077 - acc: 0.7823 - val_loss: 0.6237 - val_acc: 0.7146\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3587/3587 [==============================] - 1s 381us/step - loss: 0.5065 - acc: 0.7774 - val_loss: 0.6255 - val_acc: 0.7207\n",
      "Epoch 61/200\n",
      "3587/3587 [==============================] - 1s 382us/step - loss: 0.5079 - acc: 0.7781 - val_loss: 0.6243 - val_acc: 0.7179\n",
      "Epoch 62/200\n",
      "3587/3587 [==============================] - 1s 380us/step - loss: 0.5071 - acc: 0.7789 - val_loss: 0.6267 - val_acc: 0.7179\n",
      "Epoch 63/200\n",
      "3587/3587 [==============================] - 1s 378us/step - loss: 0.5081 - acc: 0.7809 - val_loss: 0.6359 - val_acc: 0.7179\n",
      "Epoch 64/200\n",
      "3587/3587 [==============================] - 1s 387us/step - loss: 0.5041 - acc: 0.7810 - val_loss: 0.6315 - val_acc: 0.7174\n",
      "Epoch 65/200\n",
      "3587/3587 [==============================] - 1s 387us/step - loss: 0.5073 - acc: 0.7817 - val_loss: 0.6303 - val_acc: 0.7224\n",
      "Epoch 66/200\n",
      "3587/3587 [==============================] - 1s 381us/step - loss: 0.5126 - acc: 0.7759 - val_loss: 0.6324 - val_acc: 0.7213\n",
      "Epoch 67/200\n",
      "3587/3587 [==============================] - 1s 384us/step - loss: 0.5053 - acc: 0.7837 - val_loss: 0.6308 - val_acc: 0.7191\n",
      "Epoch 68/200\n",
      "3587/3587 [==============================] - 1s 382us/step - loss: 0.4964 - acc: 0.7888 - val_loss: 0.6382 - val_acc: 0.7230\n",
      "Epoch 69/200\n",
      "3587/3587 [==============================] - 1s 382us/step - loss: 0.5029 - acc: 0.7789 - val_loss: 0.6414 - val_acc: 0.7163\n",
      "Epoch 70/200\n",
      "3587/3587 [==============================] - 1s 382us/step - loss: 0.4990 - acc: 0.7824 - val_loss: 0.6373 - val_acc: 0.7235\n",
      "Epoch 71/200\n",
      "3587/3587 [==============================] - 1s 386us/step - loss: 0.5021 - acc: 0.7869 - val_loss: 0.6444 - val_acc: 0.7202\n",
      "Epoch 72/200\n",
      "3587/3587 [==============================] - 1s 384us/step - loss: 0.5001 - acc: 0.7732 - val_loss: 0.6379 - val_acc: 0.7285\n",
      "Epoch 73/200\n",
      "3587/3587 [==============================] - 1s 383us/step - loss: 0.4899 - acc: 0.7888 - val_loss: 0.6560 - val_acc: 0.7202\n",
      "Epoch 74/200\n",
      "3587/3587 [==============================] - 1s 384us/step - loss: 0.4946 - acc: 0.7885 - val_loss: 0.6626 - val_acc: 0.7168\n",
      "Epoch 75/200\n",
      "3587/3587 [==============================] - 1s 383us/step - loss: 0.4937 - acc: 0.7865 - val_loss: 0.6522 - val_acc: 0.7219\n",
      "Epoch 76/200\n",
      "3587/3587 [==============================] - 1s 400us/step - loss: 0.4997 - acc: 0.7866 - val_loss: 0.6384 - val_acc: 0.7224\n",
      "Epoch 77/200\n",
      "3587/3587 [==============================] - 2s 423us/step - loss: 0.4883 - acc: 0.7902 - val_loss: 0.6477 - val_acc: 0.7224\n",
      "Epoch 78/200\n",
      "3587/3587 [==============================] - 2s 425us/step - loss: 0.4934 - acc: 0.7887 - val_loss: 0.6465 - val_acc: 0.7280\n",
      "Epoch 79/200\n",
      "3587/3587 [==============================] - 1s 418us/step - loss: 0.4981 - acc: 0.7819 - val_loss: 0.6418 - val_acc: 0.7313\n",
      "Epoch 80/200\n",
      "3587/3587 [==============================] - 1s 416us/step - loss: 0.4922 - acc: 0.7866 - val_loss: 0.6459 - val_acc: 0.7274\n",
      "Epoch 81/200\n",
      "3587/3587 [==============================] - 1s 414us/step - loss: 0.4834 - acc: 0.7912 - val_loss: 0.6411 - val_acc: 0.7258\n",
      "Epoch 82/200\n",
      "3587/3587 [==============================] - 1s 409us/step - loss: 0.4860 - acc: 0.7841 - val_loss: 0.6347 - val_acc: 0.7269\n",
      "Epoch 83/200\n",
      "3587/3587 [==============================] - 1s 413us/step - loss: 0.4891 - acc: 0.7902 - val_loss: 0.6271 - val_acc: 0.7274\n",
      "Epoch 84/200\n",
      "3587/3587 [==============================] - 1s 418us/step - loss: 0.4929 - acc: 0.7820 - val_loss: 0.6282 - val_acc: 0.7263\n",
      "Epoch 85/200\n",
      "3587/3587 [==============================] - 1s 403us/step - loss: 0.4870 - acc: 0.7860 - val_loss: 0.6355 - val_acc: 0.7258\n",
      "Epoch 86/200\n",
      "3587/3587 [==============================] - 2s 418us/step - loss: 0.4700 - acc: 0.7983 - val_loss: 0.6541 - val_acc: 0.7274\n",
      "Epoch 87/200\n",
      "3587/3587 [==============================] - 1s 418us/step - loss: 0.4870 - acc: 0.7885 - val_loss: 0.6353 - val_acc: 0.7280\n",
      "Epoch 88/200\n",
      "3587/3587 [==============================] - 1s 418us/step - loss: 0.4920 - acc: 0.7876 - val_loss: 0.6341 - val_acc: 0.7297\n",
      "Epoch 89/200\n",
      "3587/3587 [==============================] - 2s 420us/step - loss: 0.4851 - acc: 0.7891 - val_loss: 0.6390 - val_acc: 0.7274\n",
      "Epoch 90/200\n",
      "3587/3587 [==============================] - 1s 411us/step - loss: 0.4767 - acc: 0.7955 - val_loss: 0.6507 - val_acc: 0.7269\n",
      "Epoch 91/200\n",
      "3587/3587 [==============================] - 1s 417us/step - loss: 0.4773 - acc: 0.7883 - val_loss: 0.6461 - val_acc: 0.7285\n",
      "Epoch 92/200\n",
      "3587/3587 [==============================] - 2s 427us/step - loss: 0.4789 - acc: 0.7916 - val_loss: 0.6498 - val_acc: 0.7269\n",
      "Epoch 93/200\n",
      "3587/3587 [==============================] - 2s 419us/step - loss: 0.4715 - acc: 0.7904 - val_loss: 0.6499 - val_acc: 0.7258\n",
      "Epoch 94/200\n",
      "3587/3587 [==============================] - 1s 417us/step - loss: 0.4774 - acc: 0.7919 - val_loss: 0.6629 - val_acc: 0.7291\n",
      "Epoch 95/200\n",
      "3587/3587 [==============================] - 2s 426us/step - loss: 0.4676 - acc: 0.8000 - val_loss: 0.6546 - val_acc: 0.7263\n",
      "Epoch 96/200\n",
      "3587/3587 [==============================] - 2s 427us/step - loss: 0.4769 - acc: 0.7912 - val_loss: 0.6372 - val_acc: 0.7246\n",
      "Epoch 97/200\n",
      "3587/3587 [==============================] - 2s 420us/step - loss: 0.4724 - acc: 0.7873 - val_loss: 0.6521 - val_acc: 0.7336\n",
      "Epoch 98/200\n",
      "3587/3587 [==============================] - 2s 552us/step - loss: 0.4727 - acc: 0.7944 - val_loss: 0.6426 - val_acc: 0.7269\n",
      "Epoch 99/200\n",
      "3587/3587 [==============================] - 2s 542us/step - loss: 0.4697 - acc: 0.7963 - val_loss: 0.6488 - val_acc: 0.7213\n",
      "Epoch 100/200\n",
      "3587/3587 [==============================] - 2s 441us/step - loss: 0.4688 - acc: 0.7984 - val_loss: 0.6468 - val_acc: 0.7258\n",
      "Epoch 101/200\n",
      "3587/3587 [==============================] - 2s 423us/step - loss: 0.4714 - acc: 0.7966 - val_loss: 0.6375 - val_acc: 0.7280\n",
      "Epoch 102/200\n",
      "3587/3587 [==============================] - 2s 424us/step - loss: 0.4650 - acc: 0.7937 - val_loss: 0.6568 - val_acc: 0.7235\n",
      "Epoch 103/200\n",
      "3587/3587 [==============================] - 2s 428us/step - loss: 0.4684 - acc: 0.7980 - val_loss: 0.6406 - val_acc: 0.7274\n",
      "Epoch 104/200\n",
      "3587/3587 [==============================] - 2s 420us/step - loss: 0.4646 - acc: 0.7986 - val_loss: 0.6466 - val_acc: 0.7269\n",
      "Epoch 105/200\n",
      "3587/3587 [==============================] - 2s 425us/step - loss: 0.4642 - acc: 0.8011 - val_loss: 0.6578 - val_acc: 0.7319\n",
      "Epoch 106/200\n",
      "3587/3587 [==============================] - 2s 421us/step - loss: 0.4705 - acc: 0.7955 - val_loss: 0.6415 - val_acc: 0.7269\n",
      "Epoch 107/200\n",
      "3587/3587 [==============================] - 2s 419us/step - loss: 0.4653 - acc: 0.7965 - val_loss: 0.6702 - val_acc: 0.7269\n",
      "Epoch 108/200\n",
      "3587/3587 [==============================] - 2s 431us/step - loss: 0.4676 - acc: 0.7883 - val_loss: 0.6582 - val_acc: 0.7269\n",
      "Epoch 109/200\n",
      "3587/3587 [==============================] - 2s 458us/step - loss: 0.4591 - acc: 0.8003 - val_loss: 0.6577 - val_acc: 0.7291\n",
      "Epoch 110/200\n",
      "3587/3587 [==============================] - 2s 454us/step - loss: 0.4634 - acc: 0.7951 - val_loss: 0.6447 - val_acc: 0.7285\n",
      "Epoch 111/200\n",
      "3587/3587 [==============================] - 2s 427us/step - loss: 0.4605 - acc: 0.8046 - val_loss: 0.6574 - val_acc: 0.7269\n",
      "Epoch 112/200\n",
      "3587/3587 [==============================] - 1s 416us/step - loss: 0.4635 - acc: 0.7951 - val_loss: 0.6586 - val_acc: 0.7269\n",
      "Epoch 113/200\n",
      "3587/3587 [==============================] - 1s 414us/step - loss: 0.4515 - acc: 0.8065 - val_loss: 0.6793 - val_acc: 0.7269\n",
      "Epoch 114/200\n",
      "3587/3587 [==============================] - 2s 421us/step - loss: 0.4499 - acc: 0.8018 - val_loss: 0.6704 - val_acc: 0.7263\n",
      "Epoch 115/200\n",
      "3587/3587 [==============================] - 2s 424us/step - loss: 0.4543 - acc: 0.8068 - val_loss: 0.6530 - val_acc: 0.7291\n",
      "Epoch 116/200\n",
      "3587/3587 [==============================] - 2s 422us/step - loss: 0.4618 - acc: 0.7986 - val_loss: 0.6498 - val_acc: 0.7263\n",
      "Epoch 117/200\n",
      "3587/3587 [==============================] - 2s 432us/step - loss: 0.4452 - acc: 0.8054 - val_loss: 0.6458 - val_acc: 0.7285\n",
      "Epoch 118/200\n",
      "3587/3587 [==============================] - 1s 418us/step - loss: 0.4642 - acc: 0.7994 - val_loss: 0.6490 - val_acc: 0.7213\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3587/3587 [==============================] - 1s 382us/step - loss: 0.4605 - acc: 0.7979 - val_loss: 0.6441 - val_acc: 0.7324\n",
      "Epoch 120/200\n",
      "3587/3587 [==============================] - 1s 380us/step - loss: 0.4524 - acc: 0.8067 - val_loss: 0.6531 - val_acc: 0.7280\n",
      "Epoch 121/200\n",
      "3587/3587 [==============================] - 1s 383us/step - loss: 0.4488 - acc: 0.8069 - val_loss: 0.6630 - val_acc: 0.7269\n",
      "Epoch 122/200\n",
      "3587/3587 [==============================] - 1s 381us/step - loss: 0.4554 - acc: 0.8018 - val_loss: 0.6427 - val_acc: 0.7313\n",
      "Epoch 123/200\n",
      "3587/3587 [==============================] - 1s 377us/step - loss: 0.4523 - acc: 0.8085 - val_loss: 0.6513 - val_acc: 0.7347\n",
      "Epoch 124/200\n",
      "3587/3587 [==============================] - 1s 383us/step - loss: 0.4484 - acc: 0.8029 - val_loss: 0.6471 - val_acc: 0.7358\n",
      "Epoch 125/200\n",
      "3587/3587 [==============================] - 1s 386us/step - loss: 0.4539 - acc: 0.8021 - val_loss: 0.6332 - val_acc: 0.7358\n",
      "Epoch 126/200\n",
      "3587/3587 [==============================] - 1s 379us/step - loss: 0.4402 - acc: 0.8046 - val_loss: 0.6537 - val_acc: 0.7347\n",
      "Epoch 127/200\n",
      "3587/3587 [==============================] - 1s 387us/step - loss: 0.4470 - acc: 0.8139 - val_loss: 0.6479 - val_acc: 0.7291\n",
      "Epoch 128/200\n",
      "3587/3587 [==============================] - 1s 383us/step - loss: 0.4406 - acc: 0.8095 - val_loss: 0.6777 - val_acc: 0.7302\n",
      "Epoch 129/200\n",
      "3587/3587 [==============================] - 1s 383us/step - loss: 0.4409 - acc: 0.8099 - val_loss: 0.6651 - val_acc: 0.7274\n",
      "Epoch 130/200\n",
      "3587/3587 [==============================] - 1s 387us/step - loss: 0.4412 - acc: 0.8132 - val_loss: 0.6468 - val_acc: 0.7341\n",
      "Epoch 131/200\n",
      "3587/3587 [==============================] - 1s 386us/step - loss: 0.4465 - acc: 0.8092 - val_loss: 0.6586 - val_acc: 0.7363\n",
      "Epoch 132/200\n",
      "3587/3587 [==============================] - 1s 387us/step - loss: 0.4410 - acc: 0.8117 - val_loss: 0.6575 - val_acc: 0.7246\n",
      "Epoch 133/200\n",
      "3587/3587 [==============================] - 1s 384us/step - loss: 0.4385 - acc: 0.8110 - val_loss: 0.6451 - val_acc: 0.7363\n",
      "Epoch 134/200\n",
      "3587/3587 [==============================] - 1s 384us/step - loss: 0.4502 - acc: 0.8047 - val_loss: 0.6496 - val_acc: 0.7358\n",
      "Epoch 135/200\n",
      "3587/3587 [==============================] - 1s 384us/step - loss: 0.4411 - acc: 0.8050 - val_loss: 0.6474 - val_acc: 0.7302\n",
      "Epoch 136/200\n",
      "3587/3587 [==============================] - 1s 381us/step - loss: 0.4416 - acc: 0.8088 - val_loss: 0.6587 - val_acc: 0.7302\n",
      "Epoch 137/200\n",
      "3587/3587 [==============================] - 1s 381us/step - loss: 0.4391 - acc: 0.8086 - val_loss: 0.6387 - val_acc: 0.7319\n",
      "Epoch 138/200\n",
      "3587/3587 [==============================] - 1s 385us/step - loss: 0.4325 - acc: 0.8088 - val_loss: 0.6546 - val_acc: 0.7369\n",
      "Epoch 139/200\n",
      "3587/3587 [==============================] - 1s 384us/step - loss: 0.4326 - acc: 0.8118 - val_loss: 0.6489 - val_acc: 0.7358\n",
      "Epoch 140/200\n",
      "3587/3587 [==============================] - 1s 384us/step - loss: 0.4303 - acc: 0.8097 - val_loss: 0.6539 - val_acc: 0.7352\n",
      "Epoch 141/200\n",
      "3587/3587 [==============================] - 2s 530us/step - loss: 0.4226 - acc: 0.8182 - val_loss: 0.6811 - val_acc: 0.7330\n",
      "Epoch 142/200\n",
      "3587/3587 [==============================] - 2s 547us/step - loss: 0.4344 - acc: 0.8156 - val_loss: 0.6640 - val_acc: 0.7302\n",
      "Epoch 143/200\n",
      "3587/3587 [==============================] - 1s 385us/step - loss: 0.4242 - acc: 0.8196 - val_loss: 0.6735 - val_acc: 0.7324\n",
      "Epoch 144/200\n",
      "3587/3587 [==============================] - 1s 379us/step - loss: 0.4258 - acc: 0.8134 - val_loss: 0.6625 - val_acc: 0.7313\n",
      "Epoch 145/200\n",
      "2528/3587 [====================>.........] - ETA: 0s - loss: 0.4144 - acc: 0.8224"
     ]
    }
   ],
   "source": [
    "X_train_lstm = np.reshape(X_train, (X_train.shape[0],1, X_train.shape[1]))\n",
    "X_test_lstm = np.reshape(X_test, (X_test.shape[0],1 ,X_test.shape[1]))\n",
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    checker = classifier.fit(X_train_lstm, Y_train, batch_size=32, epochs=200, validation_data = (X_test_lstm, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_test_label = classifier.predict(X_test_lstm)\n",
    "y_pred_test=np.argmax(Y_pred_test_label,axis =1)\n",
    "y_pred_test\n",
    "Y_pred_train_label = classifier.predict(X_train_lstm)\n",
    "y_pred_train = np.argmax(Y_pred_train_label,axis=1)\n",
    "Y_test_true = Y_test_org.astype(np.int)\n",
    "Y_train_true = Y_train_org.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test_true,y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for softmax function\n",
    "print(\"TRAIN:  \\n\",confusion_matrix(y_pred_train,Y_train_true))\n",
    "print(\"\\nTest:  \\n\",confusion_matrix(y_pred_test,Y_test_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_train_true,y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
